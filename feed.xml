<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Free Katacoda Kubernetes Tutorials Are Shutting Down</title><link>https://kubernetes.io/blog/2023/02/14/kubernetes-katacoda-tutorials-stop-from-2023-03-31/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/02/14/kubernetes-katacoda-tutorials-stop-from-2023-03-31/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Natali Vlatko, SIG Docs Co-Chair for Kubernetes&lt;/p>
&lt;p>&lt;a href="https://katacoda.com/kubernetes">Katacoda&lt;/a>, the popular learning platform from O’Reilly that has been helping people learn all about
Java, Docker, Kubernetes, Python, Go, C++, and more, &lt;a href="https://www.oreilly.com/online-learning/leveraging-katacoda-technology.html">shut down for public use in June 2022&lt;/a>.
However, tutorials specifically for Kubernetes, linked from the Kubernetes website for our project’s
users and contributors, remained available and active after this change. Unfortunately, this will no
longer be the case, and Katacoda tutorials for learning Kubernetes will cease working after March 31st, 2023.&lt;/p>
&lt;p>The Kubernetes Project wishes to thank O'Reilly Media for the many years it has supported the community
via the Katacoda learning platform. You can read more about &lt;a href="https://www.oreilly.com/online-learning/leveraging-katacoda-technology.html">the decision to shutter katacoda.com&lt;/a>
on O'Reilly's own site. With this change, we’ll be focusing on the work needed to remove links to
their various tutorials. We have a general issue tracking this topic at &lt;a href="https://github.com/kubernetes/website/issues/33936">#33936&lt;/a> and &lt;a href="https://github.com/kubernetes/website/discussions/38878">GitHub discussion&lt;/a>. We’re also
interested in researching what other learning platforms could be beneficial for the Kubernetes community,
replacing Katacoda with a link to a platform or service that has a similar user experience. However,
this research will take time, so we’re actively looking for volunteers to help with this work.
If a replacement is found, it will need to be supported by Kubernetes leadership, specifically,
SIG Contributor Experience, SIG Docs, and the Kubernetes Steering Committee.&lt;/p>
&lt;p>The Katacoda shutdown affects 25 tutorial pages, their localizations, as well as the Katacoda
Scenario repository: &lt;a href="https://github.com/katacoda-scenarios/kubernetes-bootcamp-scenarios">github.com/katacoda-scenarios/kubernetes-bootcamp-scenarios&lt;/a>. We recommend
that any links, guides, or documentation you have that points to the Katacoda learning platform be
updated immediately to reflect this change. While we have yet to find a replacement learning solution,
the Kubernetes website contains a lot of helpful documentation to support your continued learning and growth.
You can find all of our available documentation tutorials for Kubernetes at &lt;a href="https://k8s.io/docs/tutorials/">https://k8s.io/docs/tutorials/&lt;/a>.&lt;/p>
&lt;p>If you have any questions regarding the Katacoda shutdown, or subsequent link removal from Kubernetes
tutorial pages, please feel free to comment on the &lt;a href="https://github.com/kubernetes/website/issues/33936">general issue tracking the shutdown&lt;/a>,
or visit the #sig-docs channel on the Kubernetes Slack.&lt;/p></description></item><item><title>Blog: k8s.gcr.io Image Registry Will Be Frozen From the 3rd of April 2023</title><link>https://kubernetes.io/blog/2023/02/06/k8s-gcr-io-freeze-announcement/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/02/06/k8s-gcr-io-freeze-announcement/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Mahamed Ali (Rackspace Technology)&lt;/p>
&lt;p>The Kubernetes project runs a community-owned image registry called &lt;code>registry.k8s.io&lt;/code> to host its container images. On the 3rd of April 2023, the old registry &lt;code>k8s.gcr.io&lt;/code> will be frozen and no further images for Kubernetes and related subprojects will be pushed to the old registry.&lt;/p>
&lt;p>This registry &lt;code>registry.k8s.io&lt;/code> replaced the old one and has been generally available for several months. We have published a &lt;a href="https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/">blog post&lt;/a> about its benefits to the community and the Kubernetes project. This post also announced that future versions of Kubernetes will not be available in the old registry. Now that time has come.&lt;/p>
&lt;p>What does this change mean for contributors:&lt;/p>
&lt;ul>
&lt;li>If you are a maintainer of a subproject, you will need to update your manifests and Helm charts to use the new registry.&lt;/li>
&lt;/ul>
&lt;p>What does this change mean for end users:&lt;/p>
&lt;ul>
&lt;li>1.27 Kubernetes release will not be published to the old registry.&lt;/li>
&lt;li>Patch releases for 1.24, 1.25, and 1.26 will no longer be published to the old registry from April. Please read the timelines below for details of the final patch releases in the old registry.&lt;/li>
&lt;li>Starting in 1.25, the default image registry has been set to &lt;code>registry.k8s.io&lt;/code>. This value is overridable in &lt;code>kubeadm&lt;/code> and &lt;code>kubelet&lt;/code> but setting it to &lt;code>k8s.gcr.io&lt;/code> will fail for new releases after April as they won’t be present in the old registry.&lt;/li>
&lt;li>If you want to increase the reliability of your cluster and remove dependency on the community-owned registry or you are running Kubernetes in networks where external traffic is restricted, you should consider hosting local image registry mirrors. Some cloud vendors may offer hosted solutions for this.&lt;/li>
&lt;/ul>
&lt;h2 id="timeline-of-the-changes">Timeline of the changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>k8s.gcr.io&lt;/code> will be frozen on the 3rd of April 2023&lt;/li>
&lt;li>1.27 is expected to be released on the 12th of April 2023&lt;/li>
&lt;li>The last 1.23 release on &lt;code>k8s.gcr.io&lt;/code> will be 1.23.18 (1.23 goes end-of-life before the freeze)&lt;/li>
&lt;li>The last 1.24 release on &lt;code>k8s.gcr.io&lt;/code> will be 1.24.12&lt;/li>
&lt;li>The last 1.25 release on &lt;code>k8s.gcr.io&lt;/code> will be 1.25.8&lt;/li>
&lt;li>The last 1.26 release on &lt;code>k8s.gcr.io&lt;/code> will be 1.26.3&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What's next&lt;/h2>
&lt;p>Please make sure your cluster does not have dependencies on old image registry. For example, you can run this command to list the images used by pods:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl get pods --all-namespaces -o &lt;span style="color:#b8860b">jsonpath&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;{.items[*].spec.containers[*].image}&amp;#34;&lt;/span> |&lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span>tr -s &lt;span style="color:#b44">&amp;#39;[[:space:]]&amp;#39;&lt;/span> &lt;span style="color:#b44">&amp;#39;\n&amp;#39;&lt;/span> |&lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span>sort |&lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span>uniq -c
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There may be other dependencies on the old image registry. Make sure you review any potential dependencies to keep your cluster healthy and up to date.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>&lt;strong>Change is hard&lt;/strong>, and evolving our image-serving platform is needed to ensure a sustainable future for the project. We strive to make things better for everyone using Kubernetes. Many contributors from all corners of our community have been working long and hard to ensure we are making the best decisions possible, executing plans, and doing our best to communicate those plans.&lt;/p>
&lt;p>Thanks to Aaron Crickenberger, Arnaud Meukam, Benjamin Elder, Caleb Woodbine, Davanum Srinivas, Mahamed Ali, and Tim Hockin from SIG K8s Infra, Brian McQueen, and Sergey Kanzhelev from SIG Node, Lubomir Ivanov from SIG Cluster Lifecycle, Adolfo García Veytia, Jeremy Rickard, Sascha Grunert, and Stephen Augustus from SIG Release, Bob Killen and Kaslin Fields from SIG Contribex, Tim Allclair from the Security Response Committee. Also a big thank you to our friends acting as liaisons with our cloud provider partners: Jay Pipes from Amazon and Jon Johnson Jr. from Google.&lt;/p></description></item><item><title>Blog: Spotlight on SIG Instrumentation</title><link>https://kubernetes.io/blog/2023/02/03/sig-instrumentation-spotlight-2023/</link><pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/02/03/sig-instrumentation-spotlight-2023/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Imran Noor Mohamed (Delivery Hero)&lt;/p>
&lt;p>Observability requires the right data at the right time for the right consumer
(human or piece of software) to make the right decision. In the context of Kubernetes,
having best practices for cluster observability across all Kubernetes components is crucial.&lt;/p>
&lt;p>SIG Instrumentation helps to address this issue by providing best practices and tools
that all other SIGs use to instrument Kubernetes components-like the &lt;em>API server&lt;/em>,
&lt;em>scheduler&lt;/em>, &lt;em>kubelet&lt;/em> and &lt;em>kube-controller-manager&lt;/em>.&lt;/p>
&lt;p>In this SIG Instrumentation spotlight, &lt;a href="https://www.linkedin.com/in/imrannoormohamed/">Imran Noor Mohamed&lt;/a>,
SIG ContribEx-Comms tech lead talked with &lt;a href="https://twitter.com/ehashdn">Elana Hashman&lt;/a>,
and &lt;a href="https://www.linkedin.com/in/hankang">Han Kang&lt;/a>, chairs of SIG Instrumentation,
on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.&lt;/p>
&lt;h2 id="about-sig-instrumentation">About SIG Instrumentation&lt;/h2>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: Hello, thank you for the opportunity of learning more about SIG Instrumentation.
Could you tell us a bit about yourself, your role, and how you got involved in SIG Instrumentation?&lt;/p>
&lt;p>&lt;strong>Han (HK)&lt;/strong>: I started in SIG Instrumentation in 2018, and became a chair in 2020.
I primarily got involved with SIG instrumentation due to a number of upstream issues
with metrics which ended up affecting GKE in bad ways. As a result, we ended up
launching an initiative to stabilize our metrics and make metrics a proper API.&lt;/p>
&lt;p>&lt;strong>Elana (EH)&lt;/strong>: I also joined SIG Instrumentation in 2018 and became a chair at the
same time as Han. I was working as a site reliability engineer (SRE) on bare metal
Kubernetes clusters and was working to build out our observability stack.
I encountered some issues with label joins where Kubernetes metrics didn’t match
kube-state-metrics (&lt;a href="https://github.com/kubernetes/kube-state-metrics">KSM&lt;/a>) and
started participating in SIG meetings to improve things. I helped test performance
improvements to kube-state-metrics and ultimately coauthored a KEP for overhauling
metrics in the 1.14 release to improve usability.&lt;/p>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: Interesting! Does that mean SIG Instrumentation involves a lot of plumbing?&lt;/p>
&lt;p>&lt;strong>Han (HK)&lt;/strong>: I wouldn’t say it involves a ton of plumbing, though it does touch
basically every code base. We have our own dedicated directories for our metrics,
logs, and tracing frameworks which we tend to work out of primarily. We do have to
interact with other SIGs in order to propagate our changes which makes us more of
a horizontal SIG.&lt;/p>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: Speaking about interaction and coordination with other SIG could
you describe how the SIGs is organized?&lt;/p>
&lt;p>&lt;strong>Elana (EH)&lt;/strong>: In SIG Instrumentation, we have two chairs, Han and myself, as well
as two tech leads, David Ashpole and Damien Grisonnet. We all work together as the
SIG’s leads in order to run meetings, triage issues and PRs, review and approve KEPs,
plan for each release, present at KubeCon and community meetings, and write our annual
report. Within the SIG we also have a number of important subprojects, each of which is
stewarded by its subproject owners. For example, Marek Siarkowicz is a subproject owner
of &lt;a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server&lt;/a>.&lt;/p>
&lt;p>Because we’re a horizontal SIG, some of our projects have a wide scope and require
coordination from a dedicated group of contributors. For example, in order to guide
the Kubernetes migration to structured logging, we chartered the
&lt;a href="https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md">Structured Logging&lt;/a>
Working Group (WG), organized by Marek and Patrick Ohly. The WG doesn’t own any code,
but helps with various components such as the &lt;em>kubelet&lt;/em>, &lt;em>scheduler&lt;/em>, etc. in migrating
their code to use structured logs.&lt;/p>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: Walking through the
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-instrumentation/charter.md">charter&lt;/a>
alone it’s clear that SIG Instrumentation has a lot of sub-projects.
Could you highlight some important ones?&lt;/p>
&lt;p>&lt;strong>Han (HK)&lt;/strong>: We have many different sub-projects and we are in dire need of
people who can come and help shepherd them. Our most important projects in-tree
(that is, within the kubernetes/kubernetes repo) are metrics, tracing, and,
structured logging. Our most important projects out-of-tree are
(a) KSM (kube-state-metrics) and (b) metrics-server.&lt;/p>
&lt;p>&lt;strong>Elana (EH)&lt;/strong>: Echoing this, we would love to bring on more maintainers for
kube-state-metrics and metrics-server. Our friends at WG Structured Logging are
also looking for contributors. Other subprojects include klog, prometheus-adapter,
and a new subproject that we just launched for collecting high-fidelity, scalable
utilization metrics called &lt;a href="https://github.com/kubernetes-sigs/usage-metrics-collector">usage-metrics-collector&lt;/a>.
All are seeking new contributors!&lt;/p>
&lt;h2 id="current-status-and-ongoing-challenges">Current status and ongoing challenges&lt;/h2>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: For release &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.26">1.26&lt;/a>
we can see that there are a relevant number of metrics, logs, and tracing
&lt;a href="https://www.k8s.dev/resources/keps/">KEPs&lt;/a> in the pipeline. Would you like to
point out important things for last release (maybe alpha &amp;amp; stable milestone candidates?)&lt;/p>
&lt;p>&lt;strong>Han (HK)&lt;/strong>: We can now generate &lt;a href="https://kubernetes.io/docs/reference/instrumentation/metrics/">documentation&lt;/a>
for every single metric in the main Kubernetes code base! We have a pretty fancy
static analysis pipeline that enables this functionality. We’ve also added feature
metrics so that you can look at your metrics to determine which features are enabled
in your cluster at a given time. Lastly, we added a component-sli endpoint, which
should make it easy for people to create availability SLOs for &lt;em>control-plane&lt;/em> components.&lt;/p>
&lt;p>&lt;strong>Elana (EH)&lt;/strong>: We’ve also been working on tracing KEPs for both the &lt;em>API server&lt;/em>
and &lt;em>kubelet&lt;/em>, though neither graduated in 1.26. I’m also really excited about the
work Han is doing with WG Reliability to extend and improve our metrics stability framework.&lt;/p>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: What do you think are the Kubernetes-specific challenges tackled by
the SIG Instrumentation? What are the future efforts to solve them?&lt;/p>
&lt;p>&lt;strong>Han (HK)&lt;/strong>: SIG instrumentation suffered a bit in the past from being a horizontal SIG.
We did not have an obvious location to put our code and did not have a good mechanism to
audit metrics that people would randomly add. We’ve fixed this over the years and now we
have dedicated spots for our code and a reliable mechanism for auditing new metrics.
We also now offer stability guarantees for metrics. We hope to have full-blown tracing
up and down the kubernetes stack, and metric support via exemplars.&lt;/p>
&lt;p>&lt;strong>Elana (EH)&lt;/strong>: I think SIG Instrumentation is a really interesting SIG because it
poses different kinds of opportunities to get involved than in other SIGs. You don’t
have to be a software developer to contribute to our SIG! All of our components and
subprojects are focused on better understanding Kubernetes and its performance in
production, which allowed me to get involved as one of the few SIG Chairs working as
an SRE at that time. I like that we provide opportunities for newcomers to contribute
through using, testing, and providing feedback on our subprojects, which is a lower
barrier to entry. Because many of these projects are out-of-tree, I think one of our
challenges is to figure out what’s in scope for core Kubernetes SIGs instrumentation
subprojects, what’s missing, and then fill in the gaps.&lt;/p>
&lt;h2 id="community-and-contribution">Community and contribution&lt;/h2>
&lt;p>&lt;strong>Imran (INM)&lt;/strong>: Kubernetes values community over products. Any recommendation
for anyone looking into getting involved in SIG Instrumentation work? Where
should they start (new contributor-friendly areas within SIG?)&lt;/p>
&lt;p>&lt;strong>Han(HK) and Elana (EH)&lt;/strong>: Come to our bi-weekly triage
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-instrumentation#meetings">meetings&lt;/a>!
They aren’t recorded and are a great place to ask questions and learn about our ongoing work.
We strive to be a friendly community and one of the easiest SIGs to get started with.
You can check out our latest KubeCon NA 2022 &lt;a href="https://youtu.be/JIzrlWtAA8Y">SIG Instrumentation Deep Dive&lt;/a>
to get more insight into our work. We also invite you to join our Slack channel #sig-instrumentation
and feel free to reach out to any of our SIG leads or subproject owners directly.&lt;/p>
&lt;p>Thank you so much for your time and insights into the workings of SIG Instrumentation!&lt;/p></description></item><item><title>Blog: Consider All Microservices Vulnerable — And Monitor Their Behavior</title><link>https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/</link><pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong>
David Hadas (IBM Research Labs)&lt;/p>
&lt;p>&lt;em>This post warns Devops from a false sense of security. Following security best practices when developing and configuring microservices do not result in non-vulnerable microservices. The post shows that although all deployed microservices are vulnerable, there is much that can be done to ensure microservices are not exploited. It explains how analyzing the behavior of clients and services from a security standpoint, named here &lt;strong>&amp;quot;Security-Behavior Analytics&amp;quot;&lt;/strong>, can protect the deployed vulnerable microservices. It points to &lt;a href="http://knative.dev/security-guard">Guard&lt;/a>, an open source project offering security-behavior monitoring and control of Kubernetes microservices presumed vulnerable.&lt;/em>&lt;/p>
&lt;p>As cyber attacks continue to intensify in sophistication, organizations deploying cloud services continue to grow their cyber investments aiming to produce safe and non-vulnerable services. However, the year-by-year growth in cyber investments does not result in a parallel reduction in cyber incidents. Instead, the number of cyber incidents continues to grow annually. Evidently, organizations are doomed to fail in this struggle - no matter how much effort is made to detect and remove cyber weaknesses from deployed services, it seems offenders always have the upper hand.&lt;/p>
&lt;p>Considering the current spread of offensive tools, sophistication of offensive players, and ever-growing cyber financial gains to offenders, any cyber strategy that relies on constructing a non-vulnerable, weakness-free service in 2023 is clearly too naïve. It seems the only viable strategy is to:&lt;/p>
&lt;p>➥ &lt;strong>Admit that your services are vulnerable!&lt;/strong>&lt;/p>
&lt;p>In other words, consciously accept that you will never create completely invulnerable services. If your opponents find even a single weakness as an entry-point, you lose! Admitting that in spite of your best efforts, all your services are still vulnerable is an important first step. Next, this post discusses what you can do about it...&lt;/p>
&lt;h2 id="how-to-protect-microservices-from-being-exploited">How to protect microservices from being exploited&lt;/h2>
&lt;p>Being vulnerable does not necessarily mean that your service will be exploited. Though your services are vulnerable in some ways unknown to you, offenders still need to identify these vulnerabilities and then exploit them. If offenders fail to exploit your service vulnerabilities, you win! In other words, having a vulnerability that can’t be exploited, represents a risk that can’t be realized.&lt;/p>
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/security_behavior_figure_1.svg"
alt="Image of an example of offender gaining foothold in a service"/> &lt;figcaption>
&lt;p>Figure 1. An Offender gaining foothold in a vulnerable service&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>The above diagram shows an example in which the offender does not yet have a foothold in the service; that is, it is assumed that your service does not run code controlled by the offender on day 1. In our example the service has vulnerabilities in the API exposed to clients. To gain an initial foothold the offender uses a malicious client to try and exploit one of the service API vulnerabilities. The malicious client sends an exploit that triggers some unplanned behavior of the service.&lt;/p>
&lt;p>More specifically, let’s assume the service is vulnerable to an SQL injection. The developer failed to sanitize the user input properly, thereby allowing clients to send values that would change the intended behavior. In our example, if a client sends a query string with key “username” and value of &lt;em>“tom or 1=1”&lt;/em>, the client will receive the data of all users. Exploiting this vulnerability requires the client to send an irregular string as the value. Note that benign users will not be sending a string with spaces or with the equal sign character as a username, instead they will normally send legal usernames which for example may be defined as a short sequence of characters a-z. No legal username can trigger service unplanned behavior.&lt;/p>
&lt;p>In this simple example, one can already identify several opportunities to detect and block an attempt to exploit the vulnerability (un)intentionally left behind by the developer, making the vulnerability unexploitable. First, the malicious client behavior differs from the behavior of benign clients, as it sends irregular requests. If such a change in behavior is detected and blocked, the exploit will never reach the service. Second, the service behavior in response to the exploit differs from the service behavior in response to a regular request. Such behavior may include making subsequent irregular calls to other services such as a data store, taking irregular time to respond, and/or responding to the malicious client with an irregular response (for example, containing much more data than normally sent in case of benign clients making regular requests). Service behavioral changes, if detected, will also allow blocking the exploit in different stages of the exploitation attempt.&lt;/p>
&lt;p>More generally:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Monitoring the behavior of clients can help detect and block exploits against service API vulnerabilities. In fact, deploying efficient client behavior monitoring makes many vulnerabilities unexploitable and others very hard to achieve. To succeed, the offender needs to create an exploit undetectable from regular requests.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Monitoring the behavior of services can help detect services as they are being exploited regardless of the attack vector used. Efficient service behavior monitoring limits what an attacker may be able to achieve as the offender needs to ensure the service behavior is undetectable from regular service behavior.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Combining both approaches may add a protection layer to the deployed vulnerable services, drastically decreasing the probability for anyone to successfully exploit any of the deployed vulnerable services. Next, let us identify four use cases where you need to use security-behavior monitoring.&lt;/p>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>One can identify the following four different stages in the life of any service from a security standpoint. In each stage, security-behavior monitoring is required to meet different challenges:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Service State&lt;/th>
&lt;th>Use case&lt;/th>
&lt;th>What do you need in order to cope with this use case?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Normal&lt;/strong>&lt;/td>
&lt;td>&lt;strong>No known vulnerabilities:&lt;/strong> The service owner is normally not aware of any known vulnerabilities in the service image or configuration. Yet, it is reasonable to assume that the service has weaknesses.&lt;/td>
&lt;td>&lt;strong>Provide generic protection against any unknown, zero-day, service vulnerabilities&lt;/strong> - Detect/block irregular patterns sent as part of incoming client requests that may be used as exploits.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Vulnerable&lt;/strong>&lt;/td>
&lt;td>&lt;strong>An applicable CVE is published:&lt;/strong> The service owner is required to release a new non-vulnerable revision of the service. Research shows that in practice this process of removing a known vulnerability may take many weeks to accomplish (2 months on average).&lt;/td>
&lt;td>&lt;strong>Add protection based on the CVE analysis&lt;/strong> - Detect/block incoming requests that include specific patterns that may be used to exploit the discovered vulnerability. Continue to offer services, although the service has a known vulnerability.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Exploitable&lt;/strong>&lt;/td>
&lt;td>&lt;strong>A known exploit is published:&lt;/strong> The service owner needs a way to filter incoming requests that contain the known exploit.&lt;/td>
&lt;td>&lt;strong>Add protection based on a known exploit signature&lt;/strong> - Detect/block incoming client requests that carry signatures identifying the exploit. Continue to offer services, although the presence of an exploit.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Misused&lt;/strong>&lt;/td>
&lt;td>&lt;strong>An offender misuses pods backing the service:&lt;/strong> The offender can follow an attack pattern enabling him/her to misuse pods. The service owner needs to restart any compromised pods while using non compromised pods to continue offering the service. Note that once a pod is restarted, the offender needs to repeat the attack pattern before he/she may again misuse it.&lt;/td>
&lt;td>&lt;strong>Identify and restart instances of the component that is being misused&lt;/strong> - At any given time, some backing pods may be compromised and misused, while others behave as designed. Detect/remove the misused pods while allowing other pods to continue servicing client requests.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Fortunately, microservice architecture is well suited to security-behavior monitoring as discussed next.&lt;/p>
&lt;h2 id="microservices-vs-monoliths">Security-Behavior of microservices versus monoliths&lt;/h2>
&lt;p>Kubernetes is often used to support workloads designed with microservice architecture. By design, microservices aim to follow the UNIX philosophy of &amp;quot;Do One Thing And Do It Well&amp;quot;. Each microservice has a bounded context and a clear interface. In other words, you can expect the microservice clients to send relatively regular requests and the microservice to present a relatively regular behavior as a response to these requests. Consequently, a microservice architecture is an excellent candidate for security-behavior monitoring.&lt;/p>
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/security_behavior_figure_2.svg"
alt="Image showing why microservices are well suited for security-behavior monitoring"/> &lt;figcaption>
&lt;p>Figure 2. Microservices are well suited for security-behavior monitoring&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>The diagram above clarifies how dividing a monolithic service to a set of microservices improves our ability to perform security-behavior monitoring and control. In a monolithic service approach, different client requests are intertwined, resulting in a diminished ability to identify irregular client behaviors. Without prior knowledge, an observer of the intertwined client requests will find it hard to distinguish between types of requests and their related characteristics. Further, internal client requests are not exposed to the observer. Lastly, the aggregated behavior of the monolithic service is a compound of the many different internal behaviors of its components, making it hard to identify irregular service behavior.&lt;/p>
&lt;p>In a microservice environment, each microservice is expected by design to offer a more well-defined service and serve better defined type of requests. This makes it easier for an observer to identify irregular client behavior and irregular service behavior. Further, a microservice design exposes the internal requests and internal services which offer more security-behavior data to identify irregularities by an observer. Overall, this makes the microservice design pattern better suited for security-behavior monitoring and control.&lt;/p>
&lt;h2 id="security-behavior-monitoring-on-kubernetes">Security-Behavior monitoring on Kubernetes&lt;/h2>
&lt;p>Kubernetes deployments seeking to add Security-Behavior may use &lt;a href="http://knative.dev/security-guard">Guard&lt;/a>, developed under the CNCF project Knative. Guard is integrated into the full Knative automation suite that runs on top of Kubernetes. Alternatively, &lt;strong>you can deploy Guard as a standalone tool&lt;/strong> to protect any HTTP-based workload on Kubernetes.&lt;/p>
&lt;p>See:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/knative-sandbox/security-guard">Guard&lt;/a> on Github, for using Guard as a standalone tool.&lt;/li>
&lt;li>The Knative automation suite - Read about Knative, in the blog post &lt;a href="https://davidhadas.wordpress.com/2022/08/29/knative-an-opinionated-kubernetes">Opinionated Kubernetes&lt;/a> which describes how Knative simplifies and unifies the way web services are deployed on Kubernetes.&lt;/li>
&lt;li>You may contact Guard maintainers on the &lt;a href="https://kubernetes.slack.com/archives/C019LFTGNQ3">SIG Security&lt;/a> Slack channel or on the Knative community &lt;a href="https://knative.slack.com/archives/CBYV1E0TG">security&lt;/a> Slack channel. The Knative community channel will move soon to the &lt;a href="https://communityinviter.com/apps/cloud-native/cncf">CNCF Slack&lt;/a> under the name &lt;code>#knative-security&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>The goal of this post is to invite the Kubernetes community to action and introduce Security-Behavior monitoring and control to help secure Kubernetes based deployments. Hopefully, the community as a follow up will:&lt;/p>
&lt;ol>
&lt;li>Analyze the cyber challenges presented for different Kubernetes use cases&lt;/li>
&lt;li>Add appropriate security documentation for users on how to introduce Security-Behavior monitoring and control.&lt;/li>
&lt;li>Consider how to integrate with tools that can help users monitor and control their vulnerable services.&lt;/li>
&lt;/ol>
&lt;h2 id="getting-involved">Getting involved&lt;/h2>
&lt;p>You are welcome to get involved and join the effort to develop security behavior monitoring
and control for Kubernetes; to share feedback and contribute to code or documentation;
and to make or suggest improvements of any kind.&lt;/p></description></item><item><title>Blog: Protect Your Mission-Critical Pods From Eviction With PriorityClass</title><link>https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/</link><pubDate>Thu, 12 Jan 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sunny Bhambhani (InfraCloud Technologies)&lt;/p>
&lt;p>Kubernetes has been widely adopted, and many organizations use it as their de-facto orchestration engine for running workloads that need to be created and deleted frequently.&lt;/p>
&lt;p>Therefore, proper scheduling of the pods is key to ensuring that application pods are up and running within the Kubernetes cluster without any issues. This article delves into the use cases around resource management by leveraging the &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClass&lt;/a> object to protect mission-critical or high-priority pods from getting evicted and making sure that the application pods are up, running, and serving traffic.&lt;/p>
&lt;h2 id="resource-management-in-kubernetes">Resource management in Kubernetes&lt;/h2>
&lt;p>The control plane consists of multiple components, out of which the scheduler (usually the built-in &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">kube-scheduler&lt;/a>) is one of the components which is responsible for assigning a node to a pod.&lt;/p>
&lt;p>Whenever a pod is created, it enters a &amp;quot;pending&amp;quot; state, after which the scheduler determines which node is best suited for the placement of the new pod.&lt;/p>
&lt;p>In the background, the scheduler runs as an infinite loop looking for pods without a &lt;code>nodeName&lt;/code> set that are &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">ready for scheduling&lt;/a>. For each Pod that needs scheduling, the scheduler tries to decide which node should run that Pod.&lt;/p>
&lt;p>If the scheduler cannot find any node, the pod remains in the pending state, which is not ideal.&lt;/p>
&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> To name a few, &lt;code>nodeSelector&lt;/code> , &lt;code>taints and tolerations&lt;/code> , &lt;code>nodeAffinity&lt;/code> , the rank of nodes based on available resources (for example, CPU and memory), and several other criteria are used to determine the pod's placement.
&lt;/div>
&lt;p>The below diagram, from point number 1 through 4, explains the request flow:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/kube-scheduler.svg"
alt="A diagram showing the scheduling of three Pods that a client has directly created."/> &lt;figcaption>
&lt;h4>Scheduling in Kubernetes&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="typical-use-cases">Typical use cases&lt;/h2>
&lt;p>Below are some real-life scenarios where control over the scheduling and eviction of pods may be required.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Let's say the pod you plan to deploy is critical, and you have some resource constraints. An example would be the DaemonSet of an infrastructure component like Grafana Loki. The Loki pods must run before other pods can on every node. In such cases, you could ensure resource availability by manually identifying and deleting the pods that are not required or by adding a new node to the cluster. Both these approaches are unsuitable since the former would be tedious to execute, and the latter could involve an expenditure of time and money.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Another use case could be a single cluster that holds the pods for the below environments with associated priorities:&lt;/p>
&lt;ul>
&lt;li>Production (&lt;code>prod&lt;/code>): top priority&lt;/li>
&lt;li>Preproduction (&lt;code>preprod&lt;/code>): intermediate priority&lt;/li>
&lt;li>Development (&lt;code>dev&lt;/code>): least priority&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>In the event of high resource consumption in the cluster, there is competition for CPU and memory resources on the nodes. While cluster-level autoscaling &lt;em>may&lt;/em> add more nodes, it takes time. In the interim, if there are no further nodes to scale the cluster, some Pods could remain in a Pending state, or the service could be degraded as they compete for resources. If the kubelet does evict a Pod from the node, that eviction would be random because the kubelet doesn’t have any special information about which Pods to evict and which to keep.&lt;/p>
&lt;ol start="3">
&lt;li>A third example could be a microservice backed by a queuing application or a database running into a resource crunch and the queue or database getting evicted. In such a case, all the other services would be rendered useless until the database can serve traffic again.&lt;/li>
&lt;/ol>
&lt;p>There can also be other scenarios where you want to control the order of scheduling or order of eviction of pods.&lt;/p>
&lt;h2 id="priorityclasses-in-kubernetes">PriorityClasses in Kubernetes&lt;/h2>
&lt;p>PriorityClass is a cluster-wide API object in Kubernetes and part of the &lt;code>scheduling.k8s.io/v1&lt;/code> API group. It contains a mapping of the PriorityClass name (defined in &lt;code>.metadata.name&lt;/code>) and an integer value (defined in &lt;code>.value&lt;/code>). This represents the value that the scheduler uses to determine Pod's relative priority.&lt;/p>
&lt;p>Additionally, when you create a cluster using kubeadm or a managed Kubernetes service (for example, Azure Kubernetes Service), Kubernetes uses PriorityClasses to safeguard the pods that are hosted on the control plane nodes. This ensures that critical cluster components such as CoreDNS and kube-proxy can run even if resources are constrained.&lt;/p>
&lt;p>This availability of pods is achieved through the use of a special PriorityClass that ensures the pods are up and running and that the overall cluster is not affected.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get priorityclass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME VALUE GLOBAL-DEFAULT AGE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">system-cluster-critical 2000000000 false 82m
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">system-node-critical 2000001000 false 82m
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The diagram below shows exactly how it works with the help of an example, which will be detailed in the upcoming section.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/decision-tree.svg"
alt="A flow chart that illustrates how the kube-scheduler prioritizes new Pods and potentially preempts existing Pods"/> &lt;figcaption>
&lt;h4>Pod scheduling and preemption&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="pod-priority-and-preemption">Pod priority and preemption&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption">Pod preemption&lt;/a> is a Kubernetes feature that allows the cluster to preempt pods (removing an existing Pod in favor of a new Pod) on the basis of priority. &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority">Pod priority&lt;/a> indicates the importance of a pod relative to other pods while scheduling. If there aren't enough resources to run all the current pods, the scheduler tries to evict lower-priority pods over high-priority ones.&lt;/p>
&lt;p>Also, when a healthy cluster experiences a node failure, typically, lower-priority pods get preempted to create room for higher-priority pods on the available node. This happens even if the cluster can bring up a new node automatically since pod creation is usually much faster than bringing up a new node.&lt;/p>
&lt;h3 id="priorityclass-requirements">PriorityClass requirements&lt;/h3>
&lt;p>Before you set up PriorityClasses, there are a few things to consider.&lt;/p>
&lt;ol>
&lt;li>Decide which PriorityClasses are needed. For instance, based on environment, type of pods, type of applications, etc.&lt;/li>
&lt;li>The default PriorityClass resource for your cluster. The pods without a &lt;code>priorityClassName&lt;/code> will be treated as priority 0.&lt;/li>
&lt;li>Use a consistent naming convention for all PriorityClasses.&lt;/li>
&lt;li>Make sure that the pods for your workloads are running with the right PriorityClass.&lt;/li>
&lt;/ol>
&lt;h2 id="priorityclass-hands-on-example">PriorityClass hands-on example&lt;/h2>
&lt;p>Let’s say there are 3 application pods: one for prod, one for preprod, and one for development. Below are three sample YAML manifest files for each of those.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># development&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dev-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dev&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dev-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;256Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;0.2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;.5Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;0.5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># preproduction&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>preprod-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>preprod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>preprod-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;1.5Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;1.5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># production&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can create these pods with the &lt;code>kubectl create -f &amp;lt;FILE.yaml&amp;gt;&lt;/code> command, and then check their status
using the &lt;code>kubectl get pods&lt;/code> command. You can see if they are up and look ready to serve traffic:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get pods --show-labels
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE LABELS
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">dev-nginx 1/1 Running 0 55s env=dev
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">preprod-nginx 1/1 Running 0 55s env=preprod
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">prod-nginx 0/1 Pending 0 55s env=prod
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Bad news. The pod for the Production environment is still Pending and isn't serving any traffic.&lt;/p>
&lt;p>Let's see why this is happening:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get events
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">5s Warning FailedScheduling pod/prod-nginx 0/2 nodes are available: 1 Insufficient cpu, 2 Insufficient memory.
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this example, there is only one worker node, and that node has a resource crunch.&lt;/p>
&lt;p>Now, let's look at how PriorityClass can help in this situation since prod should be given higher priority than the other environments.&lt;/p>
&lt;h2 id="priorityclass-api">PriorityClass API&lt;/h2>
&lt;p>Before creating PriorityClasses based on these requirements, let's see what a basic manifest for a PriorityClass looks like and outline some prerequisites:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>scheduling.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PriorityClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PRIORITYCLASS_NAME&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># any integer value between -1000000000 to 1000000000 &lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">description&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&amp;gt;-&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44;font-style:italic"> &lt;/span>&lt;span style="color:#bbb"> &lt;/span>(Optional) description goes here!&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">globalDefault&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># or true. Only one PriorityClass can be the global default.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Below are some prerequisites for PriorityClasses:&lt;/p>
&lt;ul>
&lt;li>The name of a PriorityClass must be a valid DNS subdomain name.&lt;/li>
&lt;li>When you make your own PriorityClass, the name should not start with &lt;code>system-&lt;/code>, as those names are
reserved by Kubernetes itself (for example, they are used for two built-in PriorityClasses).&lt;/li>
&lt;li>Its absolute value should be between -1000000000 to 1000000000 (1 billion).&lt;/li>
&lt;li>Larger numbers are reserved by PriorityClasses such as &lt;code>system-cluster-critical&lt;/code>
(this Pod is critically important to the cluster) and &lt;code>system-node-critical&lt;/code> (the node
critically relies on this Pod).
&lt;code>system-node-critical&lt;/code> is a higher priority than &lt;code>system-cluster-critical&lt;/code>, because a
cluster-critical Pod can only work well if the node where it is running has all its node-level
critical requirements met.&lt;/li>
&lt;li>There are two optional fields:
&lt;ul>
&lt;li>&lt;code>globalDefault&lt;/code>: When true, this PriorityClass is used for pods where a &lt;code>priorityClassName&lt;/code> is not specified.
Only one PriorityClass with &lt;code>globalDefault&lt;/code> set to true can exist in a cluster.&lt;br>
If there is no PriorityClass defined with globalDefault set to true, all the pods with no priorityClassName defined will be treated with 0 priority (i.e. the least priority).&lt;/li>
&lt;li>&lt;code>description&lt;/code>: A string with a meaningful value so that people know when to use this PriorityClass.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> Adding a PriorityClass with &lt;code>globalDefault&lt;/code> set to &lt;code>true&lt;/code> does not mean it will apply the same to the existing pods that are already running. This will be applicable only to the pods that came into existence after the PriorityClass was created.
&lt;/div>
&lt;h3 id="priorityclass-in-action">PriorityClass in action&lt;/h3>
&lt;p>Here's an example. Next, create some environment-specific PriorityClasses:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>scheduling.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PriorityClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dev-pc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">globalDefault&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">description&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&amp;gt;-&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44;font-style:italic"> &lt;/span>&lt;span style="color:#bbb"> &lt;/span>(Optional) This priority class should only be used for all development pods.&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>scheduling.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PriorityClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>preprod-pc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">globalDefault&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">description&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&amp;gt;-&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44;font-style:italic"> &lt;/span>&lt;span style="color:#bbb"> &lt;/span>(Optional) This priority class should only be used for all preprod pods.&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>scheduling.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PriorityClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod-pc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">4000000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">globalDefault&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">description&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&amp;gt;-&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44;font-style:italic"> &lt;/span>&lt;span style="color:#bbb"> &lt;/span>(Optional) This priority class should only be used for all prod pods.&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Use &lt;code>kubectl create -f &amp;lt;FILE.YAML&amp;gt;&lt;/code> command to create a pc and &lt;code>kubectl get pc&lt;/code> to check its status.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get pc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME VALUE GLOBAL-DEFAULT AGE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">dev-pc 1000000 false 3m13s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">preprod-pc 2000000 false 2m3s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">prod-pc 4000000 false 7s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">system-cluster-critical 2000000000 false 82m
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">system-node-critical 2000001000 false 82m
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The new PriorityClasses are in place now. A small change is needed in the pod manifest or pod template (in a ReplicaSet or Deployment). In other words, you need to specify the priority class name at &lt;code>.spec.priorityClassName&lt;/code> (which is a string value).&lt;/p>
&lt;p>First update the previous production pod manifest file to have a PriorityClass assigned, then delete the Production pod and recreate it. You can't edit the priority class for a Pod that already exists.&lt;/p>
&lt;p>In my cluster, when I tried this, here's what happened.
First, that change seems successful; the status of pods has been updated:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get pods --show-labels
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE LABELS
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">dev-nginx 1/1 Terminating 0 55s env=dev
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">preprod-nginx 1/1 Running 0 55s env=preprod
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">prod-nginx 0/1 Pending 0 55s env=prod
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The dev-nginx pod is getting terminated. Once that is successfully terminated and there are enough resources for the prod pod, the control plane can schedule the prod pod:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Warning FailedScheduling pod/prod-nginx 0/2 nodes are available: 1 Insufficient cpu, 2 Insufficient memory.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Preempted pod/dev-nginx by default/prod-nginx on node node01
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Killing pod/dev-nginx Stopping container dev-nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Scheduled pod/prod-nginx Successfully assigned default/prod-nginx to node01
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Pulling pod/prod-nginx Pulling image &amp;#34;nginx&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Pulled pod/prod-nginx Successfully pulled image &amp;#34;nginx&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Created pod/prod-nginx Created container prod-nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Normal Started pod/prod-nginx Started container prod-nginx
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="enforcement">Enforcement&lt;/h2>
&lt;p>When you set up PriorityClasses, they exist just how you defined them. However, people
(and tools) that make changes to your cluster are free to set any PriorityClass, or to not
set any PriorityClass at all.
However, you can use other Kubernetes features to make sure that the priorities you wanted
are actually applied.&lt;/p>
&lt;p>As an alpha feature, you can define a &lt;a href="https://kubernetes.io/blog/2022/12/20/validating-admission-policies-alpha/">ValidatingAdmissionPolicy&lt;/a> and a ValidatingAdmissionPolicyBinding so that, for example,
Pods that go into the &lt;code>prod&lt;/code> namespace must use the &lt;code>prod-pc&lt;/code> PriorityClass.
With another ValidatingAdmissionPolicyBinding you ensure that the &lt;code>preprod&lt;/code> namespace
uses the &lt;code>preprod-pc&lt;/code> PriorityClass, and so on.
In &lt;em>any&lt;/em> cluster, you can enforce similar controls using external projects such as
&lt;a href="https://kyverno.io/">Kyverno&lt;/a> or &lt;a href="https://open-policy-agent.github.io/gatekeeper/">Gatekeeper&lt;/a>,
through validating admission webhooks.&lt;/p>
&lt;p>However you do it, Kubernetes gives you options to make sure that the PriorityClasses are
used how you wanted them to be, or perhaps just to
&lt;a href="https://open-policy-agent.github.io/gatekeeper/website/docs/violations/#warn-enforcement-action">warn&lt;/a>
users when they pick an unsuitable option.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>The above example and its events show you what this feature of Kubernetes brings to the table, along with several scenarios where you can use this feature. To reiterate, this helps ensure that mission-critical pods are up and available to serve the traffic and, in the case of a resource crunch, determines cluster behavior.&lt;/p>
&lt;p>It gives you some power to decide the order of scheduling and order of &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption">preemption&lt;/a> for Pods. Therefore, you need to define the PriorityClasses sensibly.
For example, if you have a cluster autoscaler to add nodes on demand,
make sure to run it with the &lt;code>system-cluster-critical&lt;/code> PriorityClass. You don't want to
get in a situation where the autoscaler has been preempted and there are no new nodes
coming online.&lt;/p>
&lt;p>If you have any queries or feedback, feel free to reach out to me on &lt;a href="http://www.linkedin.com/in/sunnybhambhani">LinkedIn&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: Eviction policy for unhealthy pods guarded by PodDisruptionBudgets</title><link>https://kubernetes.io/blog/2023/01/06/unhealthy-pod-eviction-policy-for-pdbs/</link><pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/01/06/unhealthy-pod-eviction-policy-for-pdbs/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Filip Křepinský (Red Hat), Morten Torkildsen (Google), Ravi Gudimetla (Apple)&lt;/p>
&lt;p>Ensuring the disruptions to your applications do not affect its availability isn't a simple
task. Last month's release of Kubernetes v1.26 lets you specify an &lt;em>unhealthy pod eviction policy&lt;/em>
for &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets">PodDisruptionBudgets&lt;/a> (PDBs)
to help you maintain that availability during node management operations.
In this article, we will dive deeper into what modifications were introduced for PDBs to
give application owners greater flexibility in managing disruptions.&lt;/p>
&lt;h2 id="what-problems-does-this-solve">What problems does this solve?&lt;/h2>
&lt;p>API-initiated eviction of pods respects PodDisruptionBudgets (PDBs). This means that a requested &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/#pod-disruption">voluntary disruption&lt;/a>
via an eviction to a Pod, should not disrupt a guarded application and &lt;code>.status.currentHealthy&lt;/code> of a PDB should not fall
below &lt;code>.status.desiredHealthy&lt;/code>. Running pods that are &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">Unhealthy&lt;/a>
do not count towards the PDB status, but eviction of these is only possible in case the application
is not disrupted. This helps disrupted or not yet started application to achieve availability
as soon as possible without additional downtime that would be caused by evictions.&lt;/p>
&lt;p>Unfortunately, this poses a problem for cluster administrators that would like to drain nodes
without any manual interventions. Misbehaving applications with pods in &lt;code>CrashLoopBackOff&lt;/code>
state (due to a bug or misconfiguration) or pods that are simply failing to become ready
make this task much harder. Any eviction request will fail due to violation of a PDB,
when all pods of an application are unhealthy. Draining of a node cannot make any progress
in that case.&lt;/p>
&lt;p>On the other hand there are users that depend on the existing behavior, in order to:&lt;/p>
&lt;ul>
&lt;li>prevent data-loss that would be caused by deleting pods that are guarding an underlying resource or storage&lt;/li>
&lt;li>achieve the best availability possible for their application&lt;/li>
&lt;/ul>
&lt;p>Kubernetes 1.26 introduced a new experimental field to the PodDisruptionBudget API: &lt;code>.spec.unhealthyPodEvictionPolicy&lt;/code>.
When enabled, this field lets you support both of those requirements.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>API-initiated eviction is the process that triggers graceful pod termination.
The process can be initiated either by calling the API directly,
by using a &lt;code>kubectl drain&lt;/code> command, or other actors in the cluster.
During this process every pod removal is consulted with appropriate PDBs,
to ensure that a sufficient number of pods is always running in the cluster.&lt;/p>
&lt;p>The following policies allow PDB authors to have a greater control how the process deals with unhealthy pods.&lt;/p>
&lt;p>There are two policies &lt;code>IfHealthyBudget&lt;/code> and &lt;code>AlwaysAllow&lt;/code> to choose from.&lt;/p>
&lt;p>The former, &lt;code>IfHealthyBudget&lt;/code>, follows the existing behavior to achieve the best availability
that you get by default. Unhealthy pods can be disrupted only if their application
has a minimum available &lt;code>.status.desiredHealthy&lt;/code> number of pods.&lt;/p>
&lt;p>By setting the &lt;code>spec.unhealthyPodEvictionPolicy&lt;/code> field of your PDB to &lt;code>AlwaysAllow&lt;/code>,
you are choosing the best effort availability for your application.
With this policy it is always possible to evict unhealthy pods.
This will make it easier to maintain and upgrade your clusters.&lt;/p>
&lt;p>We think that &lt;code>AlwaysAllow&lt;/code> will often be a better choice, but for some critical workloads you may
still prefer to protect even unhealthy Pods from node drains or other forms of API-initiated
eviction.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>This is an alpha feature, which means you have to enable the &lt;code>PDBUnhealthyPodEvictionPolicy&lt;/code>
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>,
with the command line argument &lt;code>--feature-gates=PDBUnhealthyPodEvictionPolicy=true&lt;/code>
to the kube-apiserver.&lt;/p>
&lt;p>Here's an example. Assume that you've enabled the feature gate in your cluster, and that you
already defined a Deployment that runs a plain webserver. You labelled the Pods for that
Deployment with &lt;code>app: nginx&lt;/code>.
You want to limit avoidable disruption, and you know that best effort availability is
sufficient for this app.
You decide to allow evictions even if those webserver pods are unhealthy.
You create a PDB to guard this application, with the &lt;code>AlwaysAllow&lt;/code> policy for evicting
unhealthy pods:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>policy/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PodDisruptionBudget&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx-pdb&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxUnavailable&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">unhealthyPodEvictionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>AlwaysAllow&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>Read the KEP: &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3017-pod-healthy-policy-for-pdb">Unhealthy Pod Eviction Policy for PDBs&lt;/a>&lt;/li>
&lt;li>Read the documentation: &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy&lt;/a> for PodDisruptionBudgets&lt;/li>
&lt;li>Review the Kubernetes documentation for &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets">PodDisruptionBudgets&lt;/a>, &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">draining of Nodes&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/">evictions&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>If you have any feedback, please reach out to us in the &lt;a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps&lt;/a> channel on Slack (visit &lt;a href="https://slack.k8s.io/">https://slack.k8s.io/&lt;/a> for an invitation if you need one), or on the SIG Apps mailing list: &lt;a href="mailto:kubernetes-sig-apps@googlegroups.com">kubernetes-sig-apps@googlegroups.com&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: Retroactive Default StorageClass</title><link>https://kubernetes.io/blog/2023/01/05/retroactive-default-storage-class/</link><pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/01/05/retroactive-default-storage-class/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Roman Bednář (Red Hat)&lt;/p>
&lt;p>The v1.25 release of Kubernetes introduced an alpha feature to change how a default StorageClass was assigned to a PersistentVolumeClaim (PVC).
With the feature enabled, you no longer need to create a default StorageClass first and PVC second to assign the class. Additionally, any PVCs without a StorageClass assigned can be updated later.
This feature was graduated to beta in Kubernetes 1.26.&lt;/p>
&lt;p>You can read &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retroactive-default-storageclass-assignment">retroactive default StorageClass assignment&lt;/a> in the Kubernetes documentation for more details about how to use that,
or you can read on to learn about why the Kubernetes project is making this change.&lt;/p>
&lt;h2 id="why-did-storageclass-assignment-need-improvements">Why did StorageClass assignment need improvements&lt;/h2>
&lt;p>Users might already be familiar with a similar feature that assigns default StorageClasses to &lt;strong>new&lt;/strong> PVCs at the time of creation. This is currently handled by the &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass">admission controller&lt;/a>.&lt;/p>
&lt;p>But what if there wasn't a default StorageClass defined at the time of PVC creation?
Users would end up with a PVC that would never be assigned a class.
As a result, no storage would be provisioned, and the PVC would be somewhat &amp;quot;stuck&amp;quot; at this point.
Generally, two main scenarios could result in &amp;quot;stuck&amp;quot; PVCs and cause problems later down the road.
Let's take a closer look at each of them.&lt;/p>
&lt;h3 id="changing-default-storageclass">Changing default StorageClass&lt;/h3>
&lt;p>With the alpha feature enabled, there were two options admins had when they wanted to change the default StorageClass:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Creating a new StorageClass as default before removing the old one associated with the PVC.
This would result in having two defaults for a short period.
At this point, if a user were to create a PersistentVolumeClaim with storageClassName set to &lt;code>null&lt;/code> (implying default StorageClass), the newest default StorageClass would be chosen and assigned to this PVC.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Removing the old default first and creating a new default StorageClass.
This would result in having no default for a short time.
Subsequently, if a user were to create a PersistentVolumeClaim with storageClassName set to &lt;code>null&lt;/code> (implying default StorageClass), the PVC would be in &lt;code>Pending&lt;/code> state forever.
The user would have to fix this by deleting the PVC and recreating it once the default StorageClass was available.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="resource-ordering-during-cluster-installation">Resource ordering during cluster installation&lt;/h3>
&lt;p>If a cluster installation tool needed to create resources that required storage, for example, an image registry, it was difficult to get the ordering right.
This is because any Pods that required storage would rely on the presence of a default StorageClass and would fail to be created if it wasn't defined.&lt;/p>
&lt;h2 id="what-changed">What changed&lt;/h2>
&lt;p>We've changed the PersistentVolume (PV) controller to assign a default StorageClass to any unbound PersistentVolumeClaim that has the storageClassName set to &lt;code>null&lt;/code>.
We've also modified the PersistentVolumeClaim admission within the API server to allow the change of values from an unset value to an actual StorageClass name.&lt;/p>
&lt;h3 id="null-vs-empty-string">Null &lt;code>storageClassName&lt;/code> versus &lt;code>storageClassName: &amp;quot;&amp;quot;&lt;/code> - does it matter?&lt;/h3>
&lt;p>Before this feature was introduced, those values were equal in terms of behavior. Any PersistentVolumeClaim with the storageClassName set to &lt;code>null&lt;/code> or &lt;code>&amp;quot;&amp;quot;&lt;/code> would bind to an existing PersistentVolume resource with storageClassName also set to &lt;code>null&lt;/code> or &lt;code>&amp;quot;&amp;quot;&lt;/code>.&lt;/p>
&lt;p>With this new feature enabled we wanted to maintain this behavior but also be able to update the StorageClass name.
With these constraints in mind, the feature changes the semantics of &lt;code>null&lt;/code>. If a default StorageClass is present, &lt;code>null&lt;/code> would translate to &amp;quot;Give me a default&amp;quot; and &lt;code>&amp;quot;&amp;quot;&lt;/code> would mean &amp;quot;Give me PersistentVolume that also has &lt;code>&amp;quot;&amp;quot;&lt;/code> StorageClass name.&amp;quot; In the absence of a StorageClass, the behavior would remain unchanged.&lt;/p>
&lt;p>Summarizing the above, we've changed the semantics of &lt;code>null&lt;/code> so that its behavior depends on the presence or absence of a definition of default StorageClass.&lt;/p>
&lt;p>The tables below show all these cases to better describe when PVC binds and when its StorageClass gets updated.&lt;/p>
&lt;table>
&lt;caption>PVC binding behavior with Retroactive default StorageClass&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th colspan="2">&lt;/th>
&lt;th>PVC &lt;tt>storageClassName&lt;/tt> = &lt;code>""&lt;/code>&lt;/th>
&lt;th>PVC &lt;tt>storageClassName&lt;/tt> = &lt;code>null&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td rowspan="2">Without default class&lt;/td>
&lt;td>PV &lt;tt>storageClassName&lt;/tt> = &lt;code>""&lt;/code>&lt;/td>
&lt;td>binds&lt;/td>
&lt;td>binds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PV without &lt;tt>storageClassName&lt;/tt>&lt;/td>
&lt;td>binds&lt;/td>
&lt;td>binds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2">With default class&lt;/td>
&lt;td>PV &lt;tt>storageClassName&lt;/tt> = &lt;code>""&lt;/code>&lt;/td>
&lt;td>binds&lt;/td>
&lt;td>class updates&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PV without &lt;tt>storageClassName&lt;/tt>&lt;/td>
&lt;td>binds&lt;/td>
&lt;td>class updates&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="how-to-use-it">How to use it&lt;/h2>
&lt;p>If you want to test the feature whilst it's alpha, you need to enable the relevant feature gate in the kube-controller-manager and the kube-apiserver. Use the &lt;code>--feature-gates&lt;/code> command line argument:&lt;/p>
&lt;pre tabindex="0">&lt;code>--feature-gates=&amp;#34;...,RetroactiveDefaultStorageClass=true&amp;#34;
&lt;/code>&lt;/pre>&lt;h3 id="test-drive">Test drive&lt;/h3>
&lt;p>If you would like to see the feature in action and verify it works fine in your cluster here's what you can try:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Define a basic PersistentVolumeClaim:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Create the PersistentVolumeClaim when there is no default StorageClass. The PVC won't provision or bind (unless there is an existing, suitable PV already present) and will remain in &lt;code>Pending&lt;/code> state.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kc get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
pvc-1 Pending
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Configure one StorageClass as default.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kc patch sc -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;annotations&amp;#34;:{&amp;#34;storageclass.kubernetes.io/is-default-class&amp;#34;:&amp;#34;true&amp;#34;}}}&amp;#39;
storageclass.storage.k8s.io/my-storageclass patched
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Verify that PersistentVolumeClaims is now provisioned correctly and was updated retroactively with new default StorageClass.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kc get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
pvc-1 Bound pvc-06a964ca-f997-4780-8627-b5c3bf5a87d8 1Gi RWO my-storageclass 87m
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="new-metrics">New metrics&lt;/h3>
&lt;p>To help you see that the feature is working as expected we also introduced a new &lt;code>retroactive_storageclass_total&lt;/code> metric to show how many times that the PV controller attempted to update PersistentVolumeClaim, and &lt;code>retroactive_storageclass_errors_total&lt;/code> to show how many of those attempts failed.&lt;/p>
&lt;h2 id="getting-involved">Getting involved&lt;/h2>
&lt;p>We always welcome new contributors so if you would like to get involved you can join our &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special-Interest-Group&lt;/a> (SIG).&lt;/p>
&lt;p>If you would like to share feedback, you can do so on our &lt;a href="https://app.slack.com/client/T09NY5SBT/C09QZFCE5">public Slack channel&lt;/a>.&lt;/p>
&lt;p>Special thanks to all the contributors that provided great reviews, shared valuable insight and helped implement this feature (alphabetical order):&lt;/p>
&lt;ul>
&lt;li>Deep Debroy (&lt;a href="https://github.com/ddebroy">ddebroy&lt;/a>)&lt;/li>
&lt;li>Divya Mohan (&lt;a href="https://github.com/divya-mohan0209">divya-mohan0209&lt;/a>)&lt;/li>
&lt;li>Jan Šafránek (&lt;a href="https://github.com/jsafrane/">jsafrane&lt;/a>)&lt;/li>
&lt;li>Joe Betz (&lt;a href="https://github.com/jpbetz">jpbetz&lt;/a>)&lt;/li>
&lt;li>Jordan Liggitt (&lt;a href="https://github.com/liggitt">liggitt&lt;/a>)&lt;/li>
&lt;li>Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)&lt;/li>
&lt;li>Seokho Son (&lt;a href="https://github.com/seokho-son">seokho-son&lt;/a>)&lt;/li>
&lt;li>Shannon Kularathna (&lt;a href="https://github.com/shannonxtreme">shannonxtreme&lt;/a>)&lt;/li>
&lt;li>Tim Bannister (&lt;a href="https://github.com/sftim">sftim&lt;/a>)&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>)&lt;/li>
&lt;li>Wojciech Tyczynski (&lt;a href="https://github.com/wojtek-t">wojtek-t&lt;/a>)&lt;/li>
&lt;li>Xing Yang (&lt;a href="https://github.com/xing-yang">xing-yang&lt;/a>)&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes v1.26: Alpha support for cross-namespace storage data sources</title><link>https://kubernetes.io/blog/2023/01/02/cross-namespace-data-sources-alpha/</link><pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2023/01/02/cross-namespace-data-sources-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Takafumi Takahashi (Hitachi Vantara)&lt;/p>
&lt;p>Kubernetes v1.26, released last month, introduced an alpha feature that
lets you specify a data source for a PersistentVolumeClaim, even where the source
data belong to a different namespace.
With the new feature enabled, you specify a namespace in the &lt;code>dataSourceRef&lt;/code> field of
a new PersistentVolumeClaim. Once Kubernetes checks that access is OK, the new
PersistentVolume can populate its data from the storage source specified in that other
namespace.
Before Kubernetes v1.26, provided your cluster had the &lt;code>AnyVolumeDataSource&lt;/code> feature enabled,
you could already provision new volumes from a data source in the &lt;strong>same&lt;/strong>
namespace.
However, that only worked for the data source in the same namespace,
therefore users couldn't provision a PersistentVolume with a claim
in one namespace from a data source in other namespace.
To solve this problem, Kubernetes v1.26 added a new alpha &lt;code>namespace&lt;/code> field
to &lt;code>dataSourceRef&lt;/code> field in PersistentVolumeClaim the API.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>Once the csi-provisioner finds that a data source is specified with a &lt;code>dataSourceRef&lt;/code> that
has a non-empty namespace name,
it checks all reference grants within the namespace that's specified by the&lt;code>.spec.dataSourceRef.namespace&lt;/code>
field of the PersistentVolumeClaim, in order to see if access to the data source is allowed.
If any ReferenceGrant allows access, the csi-provisioner provisions a volume from the data source.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;p>The following things are required to use cross namespace volume provisioning:&lt;/p>
&lt;ul>
&lt;li>Enable the &lt;code>AnyVolumeDataSource&lt;/code> and &lt;code>CrossNamespaceVolumeDataSource&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gates&lt;/a> for the kube-apiserver and kube-controller-manager&lt;/li>
&lt;li>Install a CRD for the specific &lt;code>VolumeSnapShot&lt;/code> controller&lt;/li>
&lt;li>Install the CSI Provisioner controller and enable the &lt;code>CrossNamespaceVolumeDataSource&lt;/code> feature gate&lt;/li>
&lt;li>Install the CSI driver&lt;/li>
&lt;li>Install a CRD for ReferenceGrants&lt;/li>
&lt;/ul>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>To see how this works, you can install the sample and try it out.
This sample do to create PVC in dev namespace from VolumeSnapshot in prod namespace.
That is a simple example. For real world use, you might want to use a more complex approach.&lt;/p>
&lt;h3 id="example-assumptions">Assumptions for this example&lt;/h3>
&lt;ul>
&lt;li>Your Kubernetes cluster was deployed with &lt;code>AnyVolumeDataSource&lt;/code> and &lt;code>CrossNamespaceVolumeDataSource&lt;/code> feature gates enabled&lt;/li>
&lt;li>There are two namespaces, dev and prod&lt;/li>
&lt;li>CSI driver is being deployed&lt;/li>
&lt;li>There is an existing VolumeSnapshot named &lt;code>new-snapshot-demo&lt;/code> in the &lt;em>prod&lt;/em> namespace&lt;/li>
&lt;li>The ReferenceGrant CRD (from the Gateway API project) is already deployed&lt;/li>
&lt;/ul>
&lt;h3 id="grant-referencegrants-read-permission-to-the-csi-provisioner">Grant ReferenceGrants read permission to the CSI Provisioner&lt;/h3>
&lt;p>Access to ReferenceGrants is only needed when the CSI driver
has the &lt;code>CrossNamespaceVolumeDataSource&lt;/code> controller capability.
For this example, the external-provisioner needs &lt;strong>get&lt;/strong>, &lt;strong>list&lt;/strong>, and &lt;strong>watch&lt;/strong>
permissions for &lt;code>referencegrants&lt;/code> (API group &lt;code>gateway.networking.k8s.io&lt;/code>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;gateway.networking.k8s.io&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;referencegrants&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">verbs&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;get&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;list&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;watch&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="enable-the-crossnamespacevolumedatasource-feature-gate-for-the-csi-provisioner">Enable the CrossNamespaceVolumeDataSource feature gate for the CSI Provisioner&lt;/h3>
&lt;p>Add &lt;code>--feature-gates=CrossNamespaceVolumeDataSource=true&lt;/code> to the csi-provisioner command line.
For example, use this manifest snippet to redefine the container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- -v=5&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- --csi-address=/csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- --feature-gates=Topology=true&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- --feature-gates=CrossNamespaceVolumeDataSource=true&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-provisioner:latest&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>IfNotPresent&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-provisioner&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="create-a-referencegrant">Create a ReferenceGrant&lt;/h3>
&lt;p>Here's a manifest for an example ReferenceGrant.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.networking.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ReferenceGrant&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-prod-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dev&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">to&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>new-snapshot-demo&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="create-a-persistentvolumeclaim-by-using-cross-namespace-data-source">Create a PersistentVolumeClaim by using cross namespace data source&lt;/h3>
&lt;p>Kubernetes creates a PersistentVolumeClaim on dev and the CSI driver populates
the PersistentVolume used on dev from snapshots on prod.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dev&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSourceRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>new-snapshot-demo&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>The enhancement proposal,
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/3294-provision-volumes-from-cross-namespace-snapshots">Provision volumes from cross-namespace snapshots&lt;/a>, includes lots of detail about the history and technical implementation of this feature.&lt;/p>
&lt;p>Please get involved by joining the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>
to help us enhance this feature.
There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>It takes a wonderful group to make wonderful software.
Special thanks to the following people for the insightful reviews,
thorough consideration and valuable contribution to the CrossNamespaceVolumeDataSouce feature:&lt;/p>
&lt;ul>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;li>Masaki Kimura (mkimuram)&lt;/li>
&lt;li>Tim Hockin (thockin)&lt;/li>
&lt;li>Ben Swartzlander (bswartz)&lt;/li>
&lt;li>Rob Scott (robscott)&lt;/li>
&lt;li>John Griffith (j-griffith)&lt;/li>
&lt;li>Michael Henriksen (mhenriks)&lt;/li>
&lt;li>Mustafa Elbehery (Elbehery)&lt;/li>
&lt;/ul>
&lt;p>It’s been a joy to work with y'all on this.&lt;/p></description></item><item><title>Blog: Kubernetes v1.26: Advancements in Kubernetes Traffic Engineering</title><link>https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/</link><pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Andrew Sy Kim (Google)&lt;/p>
&lt;p>Kubernetes v1.26 includes significant advancements in network traffic engineering with the graduation of
two features (Service internal traffic policy support, and EndpointSlice terminating conditions) to GA,
and a third feature (Proxy terminating endpoints) to beta. The combination of these enhancements aims
to address short-comings in traffic engineering that people face today, and unlock new capabilities for the future.&lt;/p>
&lt;h2 id="traffic-loss-from-load-balancers-during-rolling-updates">Traffic Loss from Load Balancers During Rolling Updates&lt;/h2>
&lt;p>Prior to Kubernetes v1.26, clusters could experience &lt;a href="https://github.com/kubernetes/kubernetes/issues/85643">loss of traffic&lt;/a>
from Service load balancers during rolling updates when setting the &lt;code>externalTrafficPolicy&lt;/code> field to &lt;code>Local&lt;/code>.
There are a lot of moving parts at play here so a quick overview of how Kubernetes manages load balancers might help!&lt;/p>
&lt;p>In Kubernetes, you can create a Service with &lt;code>type: LoadBalancer&lt;/code> to expose an application externally with a load balancer.
The load balancer implementation varies between clusters and platforms, but the Service provides a generic abstraction
representing the load balancer that is consistent across all Kubernetes installations.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-service&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app.kubernetes.io/name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-app&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">9376&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LoadBalancer&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Under the hood, Kubernetes allocates a NodePort for the Service, which is then used by kube-proxy to provide a
network data path from the NodePort to the Pod. A controller will then add all available Nodes in the cluster
to the load balancer’s backend pool, using the designated NodePort for the Service as the backend target port.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-service-load-balancer.png"
alt="Figure 1: Overview of Service load balancers"/> &lt;figcaption>
&lt;p>Figure 1: Overview of Service load balancers&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Oftentimes it is beneficial to set &lt;code>externalTrafficPolicy: Local&lt;/code> for Services, to avoid extra hops between
Nodes that are not running healthy Pods backing that Service. When using &lt;code>externalTrafficPolicy: Local&lt;/code>,
an additional NodePort is allocated for health checking purposes, such that Nodes that do not contain healthy
Pods are excluded from the backend pool for a load balancer.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-lb-healthy.png"
alt="Figure 2: Load balancer traffic to a healthy Node, when externalTrafficPolicy is Local"/> &lt;figcaption>
&lt;p>Figure 2: Load balancer traffic to a healthy Node, when externalTrafficPolicy is Local&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>One such scenario where traffic can be lost is when a Node loses all Pods for a Service,
but the external load balancer has not probed the health check NodePort yet. The likelihood of this situation
is largely dependent on the health checking interval configured on the load balancer. The larger the interval,
the more likely this will happen, since the load balancer will continue to send traffic to a node
even after kube-proxy has removed forwarding rules for that Service. This also occurrs when Pods start terminating
during rolling updates. Since Kubernetes does not consider terminating Pods as “Ready”, traffic can be loss
when there are only terminating Pods on any given Node during a rolling update.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-lb-without-proxy-terminating-endpoints.png"
alt="Figure 3: Load balancer traffic to terminating endpoints, when externalTrafficPolicy is Local"/> &lt;figcaption>
&lt;p>Figure 3: Load balancer traffic to terminating endpoints, when externalTrafficPolicy is Local&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Starting in Kubernetes v1.26, kube-proxy enables the &lt;code>ProxyTerminatingEndpoints&lt;/code> feature by default, which
adds automatic failover and routing to terminating endpoints in scenarios where the traffic would otherwise
be dropped. More specifically, when there is a rolling update and a Node only contains terminating Pods,
kube-proxy will route traffic to the terminating Pods based on their readiness. In addition, kube-proxy will
actively fail the health check NodePort if there are only terminating Pods available. By doing so,
kube-proxy alerts the external load balancer that new connections should not be sent to that Node but will
gracefully handle requests for existing connections.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-lb-with-proxy-terminating-endpoints.png"
alt="Figure 4: Load Balancer traffic to terminating endpoints with ProxyTerminatingEndpoints enabled, when externalTrafficPolicy is Local"/> &lt;figcaption>
&lt;p>Figure 4: Load Balancer traffic to terminating endpoints with ProxyTerminatingEndpoints enabled, when externalTrafficPolicy is Local&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="endpointslice-conditions">EndpointSlice Conditions&lt;/h3>
&lt;p>In order to support this new capability in kube-proxy, the EndpointSlice API introduced new conditions for endpoints:
&lt;code>serving&lt;/code> and &lt;code>terminating&lt;/code>.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/endpointslice-overview.png"
alt="Figure 5: Overview of EndpointSlice conditions"/> &lt;figcaption>
&lt;p>Figure 5: Overview of EndpointSlice conditions&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>The &lt;code>serving&lt;/code> condition is semantically identical to &lt;code>ready&lt;/code>, except that it can be &lt;code>true&lt;/code> or &lt;code>false&lt;/code>
while a Pod is terminating, unlike &lt;code>ready&lt;/code> which will always be &lt;code>false&lt;/code> for terminating Pods for compatibility reasons.
The &lt;code>terminating&lt;/code> condition is true for Pods undergoing termination (non-empty deletionTimestamp), false otherwise.&lt;/p>
&lt;p>The addition of these two conditions enables consumers of this API to understand Pod states that were previously not possible.
For example, we can now track &amp;quot;ready&amp;quot; and &amp;quot;not ready&amp;quot; Pods that are also terminating.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/endpointslice-with-terminating-pod.png"
alt="Figure 6: EndpointSlice conditions with a terminating Pod"/> &lt;figcaption>
&lt;p>Figure 6: EndpointSlice conditions with a terminating Pod&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Consumers of the EndpointSlice API, such as Kube-proxy and Ingress Controllers, can now use these conditions to coordinate connection draining
events, by continuing to forward traffic for existing connections but rerouting new connections to other non-terminating endpoints.&lt;/p>
&lt;h2 id="optimizing-internal-node-local-traffic">Optimizing Internal Node-Local Traffic&lt;/h2>
&lt;p>Similar to how Services can set &lt;code>externalTrafficPolicy: Local&lt;/code> to avoid extra hops for externally sourced traffic, Kubernetes
now supports &lt;code>internalTrafficPolicy: Local&lt;/code>, to enable the same optimization for traffic originating within the cluster, specifically
for traffic using the Service Cluster IP as the destination address. This feature graduated to Beta in Kubernetes v1.24 and is graduating to GA in v1.26.&lt;/p>
&lt;p>Services default the &lt;code>internalTrafficPolicy&lt;/code> field to &lt;code>Cluster&lt;/code>, where traffic is randomly distributed to all endpoints.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/service-internal-traffic-policy-cluster.png"
alt="Figure 7: Service routing when internalTrafficPolicy is Cluster"/> &lt;figcaption>
&lt;p>Figure 7: Service routing when internalTrafficPolicy is Cluster&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>When &lt;code>internalTrafficPolicy&lt;/code> is set to &lt;code>Local&lt;/code>, kube-proxy will forward internal traffic for a Service only if there is an available endpoint
that is local to the same Node.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/service-internal-traffic-policy-local.png"
alt="Figure 8: Service routing when internalTrafficPolicy is Local"/> &lt;figcaption>
&lt;p>Figure 8: Service routing when internalTrafficPolicy is Local&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;div class="alert alert-warning caution callout" role="alert">
&lt;strong>Caution:&lt;/strong> When using &lt;code>internalTrafficPoliy: Local&lt;/code>, traffic will be dropped by kube-proxy when no local endpoints are available.
&lt;/div>
&lt;h2 id="getting-involved">Getting Involved&lt;/h2>
&lt;p>If you're interested in future discussions on Kubernetes traffic engineering, you can get involved in SIG Network through the following ways:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-network">#sig-network&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-network">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnetwork">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network#meetings">Biweekly meetings&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.26: Job Tracking, to Support Massively Parallel Batch Workloads, Is Generally Available</title><link>https://kubernetes.io/blog/2022/12/29/scalable-job-tracking-ga/</link><pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/29/scalable-job-tracking-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Aldo Culquicondor (Google)&lt;/p>
&lt;p>The Kubernetes 1.26 release includes a stable implementation of the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job&lt;/a>
controller that can reliably track a large amount of Jobs with high levels of
parallelism. &lt;a href="https://github.com/kubernetes/community/tree/master/sig-apps">SIG Apps&lt;/a>
and &lt;a href="https://github.com/kubernetes/community/tree/master/wg-batch">WG Batch&lt;/a>
have worked on this foundational improvement since Kubernetes 1.22. After
multiple iterations and scale verifications, this is now the default
implementation of the Job controller.&lt;/p>
&lt;p>Paired with the Indexed &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode">completion mode&lt;/a>,
the Job controller can handle massively parallel batch Jobs, supporting up to
100k concurrent Pods.&lt;/p>
&lt;p>The new implementation also made possible the development of &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy">Pod failure policy&lt;/a>,
which is in beta in the 1.26 release.&lt;/p>
&lt;h2 id="how-do-i-use-this-feature">How do I use this feature?&lt;/h2>
&lt;p>To use Job tracking with finalizers, upgrade to Kubernetes 1.25 or newer and
create new Jobs. You can also use this feature in v1.23 and v1.24, if you have the
ability to enable the &lt;code>JobTrackingWithFinalizers&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>.&lt;/p>
&lt;p>If your cluster runs Kubernetes 1.26, Job tracking with finalizers is a stable
feature. For v1.25, it's behind that feature gate, and your cluster administrators may have
explicitly disabled it - for example, if you have a policy of not using
beta features.&lt;/p>
&lt;p>Jobs created before the upgrade will still be tracked using the legacy behavior.
This is to avoid retroactively adding finalizers to running Pods, which might
introduce race conditions.&lt;/p>
&lt;p>For maximum performance on large Jobs, the Kubernetes project recommends
using the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode">Indexed completion mode&lt;/a>.
In this mode, the control plane is able to track Job progress with less API
calls.&lt;/p>
&lt;p>If you are a developer of operator(s) for batch, &lt;a href="https://en.wikipedia.org/wiki/High-performance_computing">HPC&lt;/a>,
&lt;a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Machine_learning">ML&lt;/a>
or related workloads, we encourage you to use the Job API to delegate accurate
progress tracking to Kubernetes. If there is something missing in the Job API
that forces you to manage plain Pods, the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-batch">Working Group Batch&lt;/a>
welcomes your feedback and contributions.&lt;/p>
&lt;h3 id="deprecation-notices">Deprecation notices&lt;/h3>
&lt;p>During the development of the feature, the control plane added the annotation
&lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#batch-kubernetes-io-job-tracking">&lt;code>batch.kubernetes.io/job-tracking&lt;/code>&lt;/a>
to the Jobs that were created when the feature was enabled.
This allowed a safe transition for older Jobs, but it was never meant to stay.&lt;/p>
&lt;p>In the 1.26 release, we deprecated the annotation &lt;code>batch.kubernetes.io/job-tracking&lt;/code>
and the control plane will stop adding it in Kubernetes 1.27.
Along with that change, we will remove the legacy Job tracking implementation.
As a result, the Job controller will track all Jobs using finalizers and it will
ignore Pods that don't have the aforementioned finalizer.&lt;/p>
&lt;p>Before you upgrade your cluster to 1.27, we recommend that you verify that there
are no running Jobs that don't have the annotation, or you wait for those jobs
to complete.
Otherwise, you might observe the control plane recreating some Pods.
We expect that this shouldn't affect any users, as the feature is enabled by
default since Kubernetes 1.25, giving enough buffer for old jobs to complete.&lt;/p>
&lt;h2 id="what-problem-does-the-new-implementation-solve">What problem does the new implementation solve?&lt;/h2>
&lt;p>Generally, Kubernetes workload controllers, such as ReplicaSet or StatefulSet,
rely on the existence of Pods or other objects in the API to determine the
status of the workload and whether replacements are needed.
For example, if a Pod that belonged to a ReplicaSet terminates or ceases to
exist, the ReplicaSet controller needs to create a replacement Pod to satisfy
the desired number of replicas (&lt;code>.spec.replicas&lt;/code>).&lt;/p>
&lt;p>Since its inception, the Job controller also relied on the existence of Pods in
the API to track Job status. A Job has &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode">completion&lt;/a>
and &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#handling-pod-and-container-failures">failure handling&lt;/a>
policies, requiring the end state of a finished Pod to determine whether to
create a replacement Pod or mark the Job as completed or failed. As a result,
the Job controller depended on Pods, even terminated ones, to remain in the API
in order to keep track of the status.&lt;/p>
&lt;p>This dependency made the tracking of Job status unreliable, because Pods can be
deleted from the API for a number of reasons, including:&lt;/p>
&lt;ul>
&lt;li>The garbage collector removing orphan Pods when a Node goes down.&lt;/li>
&lt;li>The garbage collector removing terminated Pods when they reach a threshold.&lt;/li>
&lt;li>The Kubernetes scheduler preempting a Pod to accommodate higher priority Pods.&lt;/li>
&lt;li>The taint manager evicting a Pod that doesn't tolerate a &lt;code>NoExecute&lt;/code> taint.&lt;/li>
&lt;li>External controllers, not included as part of Kubernetes, or humans deleting
Pods.&lt;/li>
&lt;/ul>
&lt;h3 id="the-new-implementation">The new implementation&lt;/h3>
&lt;p>When a controller needs to take an action on objects before they are removed, it
should add a &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/">finalizer&lt;/a>
to the objects that it manages.
A finalizer prevents the objects from being deleted from the API until the
finalizers are removed. Once the controller is done with the cleanup and
accounting for the deleted object, it can remove the finalizer from the object and the
control plane removes the object from the API.&lt;/p>
&lt;p>This is what the new Job controller is doing: adding a finalizer during Pod
creation, and removing the finalizer after the Pod has terminated and has been
accounted for in the Job status. However, it wasn't that simple.&lt;/p>
&lt;p>The main challenge is that there are at least two objects involved: the Pod
and the Job. While the finalizer lives in the Pod object, the accounting lives
in the Job object. There is no mechanism to atomically remove the finalizer in
the Pod and update the counters in the Job status. Additionally, there could be
more than one terminated Pod at a given time.&lt;/p>
&lt;p>To solve this problem, we implemented a three staged approach, each translating
to an API call.&lt;/p>
&lt;ol>
&lt;li>For each terminated Pod, add the unique ID (UID) of the Pod into short-lived
lists stored in the &lt;code>.status&lt;/code> of the owning Job
(&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/job-v1/#JobStatus">.status.uncountedTerminatedPods&lt;/a>).&lt;/li>
&lt;li>Remove the finalizer from the Pods(s).&lt;/li>
&lt;li>Atomically do the following operations:
&lt;ul>
&lt;li>remove UIDs from the short-lived lists&lt;/li>
&lt;li>increment the overall &lt;code>succeeded&lt;/code> and &lt;code>failed&lt;/code> counters in the &lt;code>status&lt;/code> of
the Job.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Additional complications come from the fact that the Job controller might
receive the results of the API changes in steps 1 and 2 out of order. We solved
this by adding an in-memory cache for removed finalizers.&lt;/p>
&lt;p>Still, we faced some issues during the beta stage, leaving some pods stuck
with finalizers in some conditions (&lt;a href="https://github.com/kubernetes/kubernetes/issues/108645">#108645&lt;/a>,
&lt;a href="https://github.com/kubernetes/kubernetes/issues/109485">#109485&lt;/a>, and
&lt;a href="https://github.com/kubernetes/kubernetes/pull/111646">#111646&lt;/a>). As a result,
we decided to switch that feature gate to be disabled by default for the 1.23
and 1.24 releases.&lt;/p>
&lt;p>Once resolved, we re-enabled the feature for the 1.25 release. Since then, we
have received reports from our customers running tens of thousands of Pods at a
time in their clusters through the Job API. Seeing this success, we decided to
graduate the feature to stable in 1.26, as part of our long term commitment to
make the Job API the best way to run large batch Jobs in a Kubernetes cluster.&lt;/p>
&lt;p>To learn more about the feature, you can read the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2307-job-tracking-without-lingering-pods">KEP&lt;/a>.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>As with any Kubernetes feature, multiple people contributed to getting this
done, from testing and filing bugs to reviewing code.&lt;/p>
&lt;p>On behalf of SIG Apps, I would like to especially thank Jordan Liggitt (Google)
for helping me debug and brainstorm solutions for more than one race condition
and Maciej Szulik (Red Hat) for his thorough reviews.&lt;/p></description></item><item><title>Blog: Kubernetes v1.26: CPUManager goes GA</title><link>https://kubernetes.io/blog/2022/12/27/cpumanager-ga/</link><pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/27/cpumanager-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong>
Francesco Romani (Red Hat)&lt;/p>
&lt;p>The CPU Manager is a part of the kubelet, the Kubernetes node agent, which enables the user to allocate exclusive CPUs to containers.
Since Kubernetes v1.10, where it &lt;a href="https://kubernetes.io/blog/2018/07/24/feature-highlight-cpu-manager/">graduated to Beta&lt;/a>, the CPU Manager proved itself reliable and
fulfilled its role of allocating exclusive CPUs to containers, so adoption has steadily grown making it a staple component of performance-critical
and low-latency setups. Over time, most changes were about bugfixes or internal refactoring, with the following noteworthy user-visible changes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Kubernetes/Kubernetes/pull/83592">support explicit reservation of CPUs&lt;/a>: it was already possible to request to reserve a given
number of CPUs for system resources, including the kubelet itself, which will not be used for exclusive CPU allocation. Now it is possible to also
explicitly select which CPUs to reserve instead of letting the kubelet pick them up automatically.&lt;/li>
&lt;li>&lt;a href="https://github.com/Kubernetes/Kubernetes/pull/97415">report the exclusively allocated CPUs&lt;/a> to containers, much like is already done for devices,
using the kubelet-local &lt;a href="https://kubernetes.io/docs/concepts/extend-Kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources">PodResources API&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/Kubernetes/Kubernetes/pull/101771">optimize the usage of system resources&lt;/a>, eliminating unnecessary sysfs changes.&lt;/li>
&lt;/ul>
&lt;p>The CPU Manager reached the point on which it &amp;quot;just works&amp;quot;, so in Kubernetes v1.26 it has graduated to generally available (GA).&lt;/p>
&lt;h2 id="cpu-managed-customization">Customization options for CPU Manager&lt;/h2>
&lt;p>The CPU Manager supports two operation modes, configured using its &lt;em>policies&lt;/em>. With the &lt;code>none&lt;/code> policy, the CPU Manager allocates CPUs to containers
without any specific constraint except the (optional) quota set in the Pod spec.
With the &lt;code>static&lt;/code> policy, then provided that the pod is in the Guaranteed QoS class and every container in that Pod requests an integer amount of vCPU cores,
then the CPU Manager allocates CPUs exclusively. Exclusive assignment means that other containers (whether from the same Pod, or from a different Pod) do not
get scheduled onto that CPU.&lt;/p>
&lt;p>This simple operational model served the user base pretty well, but as the CPU Manager matured more and more, users started to look at more elaborate use
cases and how to better support them.&lt;/p>
&lt;p>Rather than add more policies, the community realized that pretty much all the novel use cases are some variation of the behavior enabled by the &lt;code>static&lt;/code>
CPU Manager policy. Hence, it was decided to add &lt;a href="https://github.com/Kubernetes/enhancements/tree/master/keps/sig-node/2625-cpumanager-policies-thread-placement#proposed-change">options to tune the behavior of the static policy&lt;/a>.
The options have a varying degree of maturity, like any other Kubernetes feature, and in order to be accepted, each new option provides a backward
compatible behavior when disabled, and to document how to interact with each other, should they interact at all.&lt;/p>
&lt;p>This enabled the Kubernetes project to graduate to GA the CPU Manager core component and core CPU allocation algorithms to GA,
while also enabling a new age of experimentation in this area.
In Kubernetes v1.26, the CPU Manager supports &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies.md#static-policy-options">three different policy options&lt;/a>:&lt;/p>
&lt;dl>
&lt;dt>&lt;code>full-pcpus-only&lt;/code>&lt;/dt>
&lt;dd>restrict the CPU Manager core allocation algorithm to full physical cores only, reducing noisy neighbor issues from hardware technologies that allow sharing cores.&lt;/dd>
&lt;dt>&lt;code>distribute-cpus-across-numa&lt;/code>&lt;/dt>
&lt;dd>drive the CPU Manager to evenly distribute CPUs across NUMA nodes, for cases where more than one NUMA node is required to satisfy the allocation.&lt;/dd>
&lt;dt>&lt;code>align-by-socket&lt;/code>&lt;/dt>
&lt;dd>change how the CPU Manager allocates CPUs to a container: consider CPUs to be aligned at the socket boundary, instead of NUMA node boundary.&lt;/dd>
&lt;/dl>
&lt;h2 id="further-development">Further development&lt;/h2>
&lt;p>After graduating the main CPU Manager feature, each existing policy option will follow their graduation process, independent from CPU Manager and from each other option.
There is room for new options to be added, but there's also a growing demand for even more flexibility than what the CPU Manager, and its policy options, currently grant.&lt;/p>
&lt;p>Conversations are in progress in the community about splitting the CPU Manager and the other resource managers currently part of the kubelet executable
into pluggable, independent kubelet plugins. If you are interested in this effort, please join the conversation on SIG Node communication channels (Slack, mailing list, weekly meeting).&lt;/p>
&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>Please check out the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/">Control CPU Management Policies on the Node&lt;/a>
task page to learn more about the CPU Manager, and how it fits in relation to the other node-level resource managers.&lt;/p>
&lt;h2 id="getting-involved">Getting involved&lt;/h2>
&lt;p>This feature is driven by the &lt;a href="https://github.com/Kubernetes/community/blob/master/sig-node/README.md">SIG Node&lt;/a> community.
Please join us to connect with the community and share your ideas and feedback around the above feature and
beyond. We look forward to hearing from you!&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: Pod Scheduling Readiness</title><link>https://kubernetes.io/blog/2022/12/26/pod-scheduling-readiness-alpha/</link><pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/26/pod-scheduling-readiness-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Wei Huang (Apple), Abdullah Gharaibeh (Google)&lt;/p>
&lt;p>Kubernetes 1.26 introduced a new Pod feature: &lt;em>scheduling gates&lt;/em>. In Kubernetes, scheduling gates
are keys that tell the scheduler when a Pod is ready to be considered for scheduling.&lt;/p>
&lt;h2 id="what-problem-does-it-solve">What problem does it solve?&lt;/h2>
&lt;p>When a Pod is created, the scheduler will continuously attempt to find a node that fits it. This
infinite loop continues until the scheduler either finds a node for the Pod, or the Pod gets deleted.&lt;/p>
&lt;p>Pods that remain unschedulable for long periods of time (e.g., ones that are blocked on some external event)
waste scheduling cycles. A scheduling cycle may take ≅20ms or more depending on the complexity of
the Pod's scheduling constraints. Therefore, at scale, those wasted cycles significantly impact the
scheduler's performance. See the arrows in the &amp;quot;scheduler&amp;quot; box below.&lt;/p>
&lt;figure>
&lt;div class="mermaid">
graph LR;
pod((New Pod))-->queue
subgraph Scheduler
queue(scheduler queue)
sched_cycle[/scheduling cycle/]
schedulable{schedulable?}
queue==>|Pop out|sched_cycle
sched_cycle==>schedulable
schedulable==>|No|queue
subgraph note [Cycles wasted on keep rescheduling 'unready' Pods]
end
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:1px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:1px,color:#fff;
classDef Scheduler fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
classDef note fill:#edf2ae,stroke:#fff,stroke-width:1px;
class queue,sched_cycle,schedulable k8s;
class pod plain;
class note note;
class Scheduler Scheduler;
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;p>Scheduling gates helps address this problem. It allows declaring that newly created Pods are not
ready for scheduling. When scheduling gates are present on a Pod, the scheduler ignores the Pod
and therefore saves unnecessary scheduling attempts. Those Pods will also be ignored by Cluster
Autoscaler if you have it installed in the cluster.&lt;/p>
&lt;p>Clearing the gates is the responsibility of external controllers with knowledge of when the Pod
should be considered for scheduling (e.g., a quota manager).&lt;/p>
&lt;figure>
&lt;div class="mermaid">
graph LR;
pod((New Pod))-->queue
subgraph Scheduler
queue(scheduler queue)
sched_cycle[/scheduling cycle/]
schedulable{schedulable?}
popout{Pop out?}
queue==>|PreEnqueue check|popout
popout-->|Yes|sched_cycle
popout==>|No|queue
sched_cycle-->schedulable
schedulable-->|No|queue
subgraph note [A knob to gate Pod's scheduling]
end
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:1px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:1px,color:#fff;
classDef Scheduler fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
classDef note fill:#edf2ae,stroke:#fff,stroke-width:1px;
classDef popout fill:#f96,stroke:#fff,stroke-width:1px;
class queue,sched_cycle,schedulable k8s;
class pod plain;
class note note;
class popout popout;
class Scheduler Scheduler;
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>Scheduling gates in general works very similar to Finalizers. Pods with a non-empty
&lt;code>spec.schedulingGates&lt;/code> field will show as status &lt;code>SchedulingGated&lt;/code> and be blocked from
scheduling. Note that more than one gate can be added, but they all should be added upon Pod
creation (e.g., you can add them as part of the spec or via a mutating webhook).&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME READY STATUS RESTARTS AGE
test-pod 0/1 SchedulingGated 0 10s
&lt;/code>&lt;/pre>&lt;p>To clear the gates, you update the Pod by removing all of the items from the Pod's &lt;code>schedulingGates&lt;/code>
field. The gates do not need to be removed all at once, but only when all the gates are removed the
scheduler will start to consider the Pod for scheduling.&lt;/p>
&lt;p>Under the hood, scheduling gates are implemented as a PreEnqueue scheduler plugin, a new scheduler
framework extension point that is invoked at the beginning of each scheduling cycle.&lt;/p>
&lt;h2 id="use-cases">Use Cases&lt;/h2>
&lt;p>An important use case this feature enables is dynamic quota management. Kubernetes supports
&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuota&lt;/a>, however the API Server enforces quota at
the time you attempt Pod creation. For example, if a new Pod exceeds the CPU quota, it gets rejected.
The API Server doesn't queue the Pod; therefore, whoever created the Pod needs to continuously attempt
to recreate it again. This either means a delay between resources becoming available and the Pod
actually running, or it means load on the API server and Scheduler due to constant attempts.&lt;/p>
&lt;p>Scheduling gates allows an external quota manager to address the above limitation of ResourceQuota.
Specifically, the manager could add a &lt;code>example.com/quota-check&lt;/code> scheduling gate to all Pods created in the
cluster (using a mutating webhook). The manager would then remove the gate when there is quota to
start the Pod.&lt;/p>
&lt;h2 id="whats-next">Whats next?&lt;/h2>
&lt;p>To use this feature, the &lt;code>PodSchedulingReadiness&lt;/code> feature gate must be enabled in the API Server
and scheduler. You're more than welcome to test it out and tell us (SIG Scheduling) what you think!&lt;/p>
&lt;h2 id="additional-resources">Additional resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">Pod Scheduling Readiness&lt;/a>
in the Kubernetes documentation&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/3521-pod-scheduling-readiness/README.md">Kubernetes Enhancement Proposal&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.26: Support for Passing Pod fsGroup to CSI Drivers At Mount Time</title><link>https://kubernetes.io/blog/2022/12/23/kubernetes-12-06-fsgroup-on-mount/</link><pubDate>Fri, 23 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/23/kubernetes-12-06-fsgroup-on-mount/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Fabio Bertinatto (Red Hat), Hemant Kumar (Red Hat)&lt;/p>
&lt;p>Delegation of &lt;code>fsGroup&lt;/code> to CSI drivers was first introduced as alpha in Kubernetes 1.22,
and graduated to beta in Kubernetes 1.25.
For Kubernetes 1.26, we are happy to announce that this feature has graduated to
General Availability (GA).&lt;/p>
&lt;p>In this release, if you specify a &lt;code>fsGroup&lt;/code> in the
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod">security context&lt;/a>,
for a (Linux) Pod, all processes in the pod's containers are part of the additional group
that you specified.&lt;/p>
&lt;p>In previous Kubernetes releases, the kubelet would &lt;em>always&lt;/em> apply the
&lt;code>fsGroup&lt;/code> ownership and permission changes to files in the volume according to the policy
you specified in the Pod's &lt;code>.spec.securityContext.fsGroupChangePolicy&lt;/code> field.&lt;/p>
&lt;p>Starting with Kubernetes 1.26, CSI drivers have the option to apply the &lt;code>fsGroup&lt;/code> settings during
volume mount time, which frees the kubelet from changing the permissions of files and directories
in those volumes.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>CSI drivers that support this feature should advertise the
&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetcapabilities">&lt;code>VOLUME_MOUNT_GROUP&lt;/code>&lt;/a> node capability.&lt;/p>
&lt;p>After recognizing this information, the kubelet passes the &lt;code>fsGroup&lt;/code> information to
the CSI driver during pod startup. This is done through the
&lt;a href="https://github.com/container-storage-interface/spec/blob/v1.7.0/spec.md#nodestagevolume">&lt;code>NodeStageVolumeRequest&lt;/code>&lt;/a> and
&lt;a href="https://github.com/container-storage-interface/spec/blob/v1.7.0/spec.md#nodepublishvolume">&lt;code>NodePublishVolumeRequest&lt;/code>&lt;/a>
CSI calls.&lt;/p>
&lt;p>Consequently, the CSI driver is expected to apply the &lt;code>fsGroup&lt;/code> to the files in the volume using a
&lt;em>mount option&lt;/em>. As an example, &lt;a href="https://github.com/kubernetes-sigs/azurefile-csi-driver">Azure File CSIDriver&lt;/a> utilizes the &lt;code>gid&lt;/code> mount option to map
the &lt;code>fsGroup&lt;/code> information to all the files in the volume.&lt;/p>
&lt;p>It should be noted that in the example above the kubelet refrains from directly
applying the permission changes into the files and directories in that volume files.
Additionally, two policy definitions no longer have an effect: neither
&lt;code>.spec.fsGroupPolicy&lt;/code> for the CSIDriver object, nor
&lt;code>.spec.securityContext.fsGroupChangePolicy&lt;/code> for the Pod.&lt;/p>
&lt;p>For more details about the inner workings of this feature, check out the
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2317-fsgroup-on-mount/">enhancement proposal&lt;/a>
and the &lt;a href="https://kubernetes-csi.github.io/docs/support-fsgroup.html">CSI Driver &lt;code>fsGroup&lt;/code> Support&lt;/a>
in the CSI developer documentation.&lt;/p>
&lt;h2 id="why-is-it-important">Why is it important?&lt;/h2>
&lt;p>Without this feature, applying the fsGroup information to files is not possible in certain storage environments.&lt;/p>
&lt;p>For instance, Azure File does not support a concept of POSIX-style ownership and permissions
of files. The CSI driver is only able to set the file permissions at the volume level.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>This feature should be mostly transparent to users. If you maintain a CSI driver that should
support this feature, read
&lt;a href="https://kubernetes-csi.github.io/docs/support-fsgroup.html">CSI Driver &lt;code>fsGroup&lt;/code> Support&lt;/a>
for more information on how to support this feature in your CSI driver.&lt;/p>
&lt;p>Existing CSI drivers that do not support this feature will continue to work as usual:
they will not receive any &lt;code>fsGroup&lt;/code> information from the kubelet. In addition to that,
the kubelet will continue to perform the ownership and permissions changes to files
for those volumes, according to the policies specified in &lt;code>.spec.fsGroupPolicy&lt;/code> for the
CSIDriver and &lt;code>.spec.securityContext.fsGroupChangePolicy&lt;/code> for the relevant Pod.&lt;/p></description></item><item><title>Blog: Kubernetes v1.26: GA Support for Kubelet Credential Providers</title><link>https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/</link><pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Andrew Sy Kim (Google), Dixita Narang (Google)&lt;/p>
&lt;p>Kubernetes v1.26 introduced generally available (GA) support for &lt;a href="https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/">&lt;em>kubelet credential
provider plugins&lt;/em>&lt;/a>,
offering an extensible plugin framework to dynamically fetch credentials
for any container image registry.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Kubernetes supports the ability to dynamically fetch credentials for a container registry service.
Prior to Kubernetes v1.20, this capability was compiled into the kubelet and only available for
Amazon Elastic Container Registry, Azure Container Registry, and Google Cloud Container Registry.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-in-tree.png"
alt="Figure 1: Kubelet built-in credential provider support for Amazon Elastic Container Registry, Azure Container Registry, and Google Cloud Container Registry."/> &lt;figcaption>
&lt;p>Figure 1: Kubelet built-in credential provider support for Amazon Elastic Container Registry, Azure Container Registry, and Google Cloud Container Registry.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Kubernetes v1.20 introduced alpha support for kubelet credential providers plugins,
which provides a mechanism for the kubelet to dynamically authenticate and pull images
for arbitrary container registries - whether these are public registries, managed services,
or even a self-hosted registry.
In Kubernetes v1.26, this feature is now GA&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-plugin.png"
alt="Figure 2: Kubelet credential provider overview"/> &lt;figcaption>
&lt;p>Figure 2: Kubelet credential provider overview&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="why-is-it-important">Why is it important?&lt;/h2>
&lt;p>Prior to Kubernetes v1.20, if you wanted to dynamically fetch credentials for image registries
other than ACR (Azure Container Registry), ECR (Elastic Container Registry), or GCR
(Google Container Registry), you needed to modify the kubelet code.
The new plugin mechanism can be used in any cluster, and lets you authenticate to new registries without
any changes to Kubernetes itself. Any cloud provider or vendor can publish a plugin that lets you authenticate with their image registry.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>The kubelet and the exec plugin binary communicate through stdio (stdin, stdout, and stderr) by sending and receiving
json-serialized api-versioned types. If the exec plugin is enabled and the kubelet requires authentication information for an image
that matches against a plugin, the kubelet will execute the plugin binary, passing the &lt;code>CredentialProviderRequest&lt;/code> API via stdin. Then
the exec plugin communicates with the container registry to dynamically fetch the credentials and returns the credentials in an
encoded response of the &lt;code>CredentialProviderResponse&lt;/code> API to the kubelet via stdout.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-how-it-works.png"
alt="Figure 3: Kubelet credential provider plugin flow"/> &lt;figcaption>
&lt;p>Figure 3: Kubelet credential provider plugin flow&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>On receiving credentials from the kubelet, the plugin can also indicate how long credentials can be cached for, to prevent unnecessary
execution of the plugin by the kubelet for subsequent image pull requests to the same registry. In cases where the cache duration
is not specified by the plugin, a default cache duration can be specified by the kubelet (more details below).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kubelet.k8s.io/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;CredentialProviderResponse&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;auth&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;cacheDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;6h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;private-registry.io/my-app&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;username&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;exampleuser&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;password&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;token12345&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In addition, the plugin can specify the scope in which cached credentials are valid for. This is specified through the &lt;code>cacheKeyType&lt;/code> field
in &lt;code>CredentialProviderResponse&lt;/code>. When the value is &lt;code>Image&lt;/code>, the kubelet will only use cached credentials for future image pulls that exactly
match the image of the first request. When the value is &lt;code>Registry&lt;/code>, the kubelet will use cached credentials for any subsequent image pulls
destined for the same registry host but using different paths (for example, &lt;code>gcr.io/foo/bar&lt;/code> and &lt;code>gcr.io/bar/foo&lt;/code> refer to different images
from the same registry). Lastly, when the value is &lt;code>Global&lt;/code>, the kubelet will use returned credentials for all images that match against
the plugin, including images that can map to different registry hosts (for example, gcr.io vs k8s.gcr.io). The &lt;code>cacheKeyType&lt;/code> field is required by plugin
implementations.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kubelet.k8s.io/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;CredentialProviderResponse&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;auth&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;cacheKeyType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Registry&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;private-registry.io/my-app&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;username&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;exampleuser&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;password&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;token12345&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="using-kubelet-credential-providers">Using kubelet credential providers&lt;/h2>
&lt;p>You can configure credential providers by installing the exec plugin(s) into
a local directory accessible by the kubelet on every node. Then you set two command line arguments for the kubelet:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--image-credential-provider-config&lt;/code>: the path to the credential provider plugin config file.&lt;/li>
&lt;li>&lt;code>--image-credential-provider-bin-dir&lt;/code>: the path to the directory where credential provider plugin binaries are located.&lt;/li>
&lt;/ul>
&lt;p>The configuration file passed into &lt;code>--image-credential-provider-config&lt;/code> is read by the kubelet to determine which exec plugins should be invoked for a container image used by a Pod.
Note that the name of each &lt;em>provider&lt;/em> must match the name of the binary located in the local directory specified in &lt;code>--image-credential-provider-bin-dir&lt;/code>, otherwise the kubelet
cannot locate the path of the plugin to invoke.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CredentialProviderConfig&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet.config.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">providers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>auth-provider-gcp&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>credentialprovider.kubelet.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchImages&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;container.cloud.google.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;gcr.io&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*.gcr.io&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*.pkg.dev&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- get-credentials&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- --v=3&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">defaultCacheDuration&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Below is an overview of how the Kubernetes project is using kubelet credential providers for end-to-end testing.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-enabling.png"
alt="Figure 4: Kubelet credential provider configuration used for Kubernetes e2e testing"/> &lt;figcaption>
&lt;p>Figure 4: Kubelet credential provider configuration used for Kubernetes e2e testing&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>For more configuration details, see &lt;a href="https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/">Kubelet Credential Providers&lt;/a>.&lt;/p>
&lt;h2 id="getting-involved">Getting Involved&lt;/h2>
&lt;p>Come join SIG Node if you want to report bugs or have feature requests for the Kubelet Credential Provider. You can reach us through the following ways:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#meetings">Biweekly meetings&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.26: Introducing Validating Admission Policies</title><link>https://kubernetes.io/blog/2022/12/20/validating-admission-policies-alpha/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/20/validating-admission-policies-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Joe Betz (Google), Cici Huang (Google)&lt;/p>
&lt;p>In Kubernetes 1.26, the 1st alpha release of validating admission policies is
available!&lt;/p>
&lt;p>Validating admission policies use the &lt;a href="https://github.com/google/cel-spec">Common Expression
Language&lt;/a> (CEL) to offer a declarative,
in-process alternative to &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">validating admission
webhooks&lt;/a>.&lt;/p>
&lt;p>CEL was first introduced to Kubernetes for the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation rules for
CustomResourceDefinitions&lt;/a>.
This enhancement expands the use of CEL in Kubernetes to support a far wider
range of admission use cases.&lt;/p>
&lt;p>Admission webhooks can be burdensome to develop and operate. Webhook developers
must implement and maintain a webhook binary to handle admission requests. Also,
admission webhooks are complex to operate. Each webhook must be deployed,
monitored and have a well defined upgrade and rollback plan. To make matters
worse, if a webhook times out or becomes unavailable, the Kubernetes control
plane can become unavailable. This enhancement avoids much of this complexity of
admission webhooks by embedding CEL expressions into Kubernetes resources
instead of calling out to a remote webhook binary.&lt;/p>
&lt;p>For example, to set a limit on how many replicas a Deployment can have.
Start by defining a validation policy:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admissionregistration.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ValidatingAdmissionPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-policy.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchConstraints&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceRules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;apps&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operations&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;CREATE&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;UPDATE&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;deployments&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">expression&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;object.spec.replicas &amp;lt;= 5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>expression&lt;/code> field contains the CEL expression that is used to validate
admission requests. &lt;code>matchConstraints&lt;/code> declares what types of requests this
&lt;code>ValidatingAdmissionPolicy&lt;/code> is may validate.&lt;/p>
&lt;p>Next bind the policy to the appropriate resources:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admissionregistration.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ValidatingAdmissionPolicyBinding&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-binding-test.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">policyName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-policy.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchResources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>environment&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>In&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- test&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This &lt;code>ValidatingAdmissionPolicyBinding&lt;/code> resource binds the above policy only to
namespaces where the &lt;code>environment&lt;/code> label is set to &lt;code>test&lt;/code>. Once this binding
is created, the kube-apiserver will begin enforcing this admission policy.&lt;/p>
&lt;p>To emphasize how much simpler this approach is than admission webhooks, if this example
were instead implemented with a webhook, an entire binary would need to be
developed and maintained just to perform a &lt;code>&amp;lt;=&lt;/code> check. In our review of a wide
range of admission webhooks used in production, the vast majority performed
relatively simple checks, all of which can easily be expressed using CEL.&lt;/p>
&lt;p>Validation admission policies are highly configurable, enabling policy authors
to define policies that can be parameterized and scoped to resources as needed
by cluster administrators.&lt;/p>
&lt;p>For example, the above admission policy can be modified to make it configurable:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admissionregistration.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ValidatingAdmissionPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-policy.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">paramKind&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>rules.example.com/v1&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># You also need a CustomResourceDefinition for this API&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ReplicaLimit&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchConstraints&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceRules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;apps&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operations&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;CREATE&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;UPDATE&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;deployments&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">expression&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;object.spec.replicas &amp;lt;= params.maxReplicas&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, &lt;code>paramKind&lt;/code> defines the resources used to configure the policy and the
&lt;code>expression&lt;/code> uses the &lt;code>params&lt;/code> variable to access the parameter resource.&lt;/p>
&lt;p>This allows multiple bindings to be defined, each configured differently. For
example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admissionregistration.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ValidatingAdmissionPolicyBinding&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-binding-production.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">policyName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-policy.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">paramRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-params-production.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchResources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>environment&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>In&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- production&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>rules.example.com/v1&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># defined via a CustomResourceDefinition&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ReplicaLimit&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;demo-params-production.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">maxReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This binding and parameter resource pair limit deployments in namespaces with the
&lt;code>environment&lt;/code> label set to &lt;code>production&lt;/code> to a max of 1000 replicas.&lt;/p>
&lt;p>You can then use a separate binding and parameter pair to set a different limit
for namespaces in the &lt;code>test&lt;/code> environment.&lt;/p>
&lt;p>I hope this has given you a glimpse of what is possible with validating
admission policies! There are many features that we have not yet touched on.&lt;/p>
&lt;p>To learn more, read
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/">Validating Admission Policy&lt;/a>.&lt;/p>
&lt;p>We are working hard to add more features to admission policies and make the
enhancement easier to use. Try it out, send us your feedback and help us build
a simpler alternative to admission webhooks!&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>If you want to get involved in development of admission policies, discuss enhancement
roadmaps, or report a bug, you can get in touch with developers at
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: Device Manager graduates to GA</title><link>https://kubernetes.io/blog/2022/12/19/devicemanager-ga/</link><pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/19/devicemanager-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Swati Sehgal (Red Hat)&lt;/p>
&lt;p>The Device Plugin framework was introduced in the Kubernetes v1.8 release as a vendor
independent framework to enable discovery, advertisement and allocation of external
devices without modifying core Kubernetes. The feature graduated to Beta in v1.10.
With the recent release of Kubernetes v1.26, Device Manager is now generally
available (GA).&lt;/p>
&lt;p>Within the kubelet, the Device Manager facilitates communication with device plugins
using gRPC through Unix sockets. Device Manager and Device plugins both act as gRPC
servers and clients by serving and connecting to the exposed gRPC services respectively.
Device plugins serve a gRPC service that kubelet connects to for device discovery,
advertisement (as extended resources) and allocation. Device Manager connects to
the &lt;code>Registration&lt;/code> gRPC service served by kubelet to register itself with kubelet.&lt;/p>
&lt;p>Please refer to the documentation for an &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#example-pod">example&lt;/a> on how a pod can request a device exposed to the cluster by a device plugin.&lt;/p>
&lt;p>Here are some example implementations of device plugins:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/RadeonOpenCompute/k8s-device-plugin">AMD GPU device plugin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/intel/intel-device-plugins-for-kubernetes">Collection of Intel device plugins for Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA device plugin for Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">SRIOV network device plugin for Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="noteworthy-developments-since-device-plugin-framework-introduction">Noteworthy developments since Device Plugin framework introduction&lt;/h2>
&lt;h3 id="kubelet-apis-moved-to-kubelet-staging-repo">Kubelet APIs moved to kubelet staging repo&lt;/h3>
&lt;p>External facing &lt;code>deviceplugin&lt;/code> API packages moved from &lt;code>k8s.io/kubernetes/pkg/kubelet/apis/&lt;/code>
to &lt;code>k8s.io/kubelet/pkg/apis/&lt;/code> in v1.17. Refer to &lt;a href="https://github.com/kubernetes/kubernetes/pull/83551">Move external facing kubelet apis to staging&lt;/a> for more details on the rationale behind this change.&lt;/p>
&lt;h3 id="device-plugin-api-updates">Device Plugin API updates&lt;/h3>
&lt;p>Additional gRPC endpoints introduced:&lt;/p>
&lt;ol>
&lt;li>&lt;code>GetDevicePluginOptions&lt;/code> is used by device plugins to communicate
options to the &lt;code>DeviceManager&lt;/code> in order to indicate if &lt;code>PreStartContainer&lt;/code>,
&lt;code>GetPreferredAllocation&lt;/code> or other future optional calls are supported and
can be called before making devices available to the container.&lt;/li>
&lt;li>&lt;code>GetPreferredAllocation&lt;/code> allows a device plugin to forward allocation
preferrence to the &lt;code>DeviceManager&lt;/code> so it can incorporate this information
into its allocation decisions. The &lt;code>DeviceManager&lt;/code> will call out to a
plugin at pod admission time asking for a preferred device allocation
of a given size from a list of available devices to make a more informed
decision. E.g. Specifying inter-device constraints to indicate preferrence
on best-connected set of devices when allocating devices to a container.&lt;/li>
&lt;li>&lt;code>PreStartContainer&lt;/code> is called before each container start if indicated by
device plugins during registration phase. It allows Device Plugins to run device
specific operations on the Devices requested. E.g. reconfiguring or
reprogramming FPGAs before the container starts running.&lt;/li>
&lt;/ol>
&lt;p>Pull Requests that introduced these changes are here:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/58282">Invoke preStart RPC call before container start, if desired by plugin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/92665">Add GetPreferredAllocation() call to the v1beta1 device plugin API&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>With introduction of the above endpoints the interaction between Device Manager in
kubelet and Device Manager can be shown as below:&lt;/p>
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/blog/2022/12/19/devicemanager-ga/deviceplugin-framework-overview.svg"
alt="Representation of the Device Plugin framework showing the relationship between the kubelet and a device plugin"/> &lt;figcaption>
&lt;p>Device Plugin framework Overview&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="change-in-semantics-of-device-plugin-registration-process">Change in semantics of device plugin registration process&lt;/h3>
&lt;p>Device plugin code was refactored to separate 'plugin' package under the &lt;code>devicemanager&lt;/code>
package to lay the groundwork for introducing a &lt;code>v1beta2&lt;/code> device plugin API. This would
allow adding support in &lt;code>devicemanager&lt;/code> to service multiple device plugin APIs at the
same time.&lt;/p>
&lt;p>With this refactoring work, it is now mandatory for a device plugin to start serving its gRPC
service before registering itself with kubelet. Previously, these two operations were asynchronous
and device plugin could register itself before starting its gRPC server which is no longer the
case. For more details, refer to &lt;a href="https://github.com/kubernetes/kubernetes/pull/109016">PR #109016&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/issues/112395">Issue #112395&lt;/a>.&lt;/p>
&lt;h3 id="dynamic-resource-allocation">Dynamic resource allocation&lt;/h3>
&lt;p>In Kubernetes 1.26, inspired by how &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">Persistent Volumes&lt;/a>
are handled in Kubernetes, &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation&lt;/a>
has been introduced to cater to devices that have more sophisticated resource requirements like:&lt;/p>
&lt;ol>
&lt;li>Decouple device initialization and allocation from the pod lifecycle.&lt;/li>
&lt;li>Facilitate dynamic sharing of devices between containers and pods.&lt;/li>
&lt;li>Support custom resource-specific parameters&lt;/li>
&lt;li>Enable resource-specific setup and cleanup actions&lt;/li>
&lt;li>Enable support for Network-attached resources, not just node-local resources&lt;/li>
&lt;/ol>
&lt;h2 id="is-the-device-plugin-api-stable-now">Is the Device Plugin API stable now?&lt;/h2>
&lt;p>No, the Device Plugin API is still not stable; the latest Device Plugin API version
available is &lt;code>v1beta1&lt;/code>. There are plans in the community to introduce &lt;code>v1beta2&lt;/code> API
to service multiple plugin APIs at once. A per-API call with request/response types
would allow adding support for newer API versions without explicitly bumping the API.&lt;/p>
&lt;p>In addition to that, there are existing proposals in the community to introduce additional
endpoints &lt;a href="https://github.com/kubernetes/enhancements/issues/3162">KEP-3162: Add Deallocate and PostStopContainer to Device Manager API&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: Non-Graceful Node Shutdown Moves to Beta</title><link>https://kubernetes.io/blog/2022/12/16/kubernetes-1-26-non-graceful-node-shutdown-beta/</link><pubDate>Fri, 16 Dec 2022 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2022/12/16/kubernetes-1-26-non-graceful-node-shutdown-beta/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Xing Yang (VMware), Ashutosh Kumar (VMware)&lt;/p>
&lt;p>Kubernetes v1.24 &lt;a href="https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/">introduced&lt;/a> an alpha quality implementation of improvements
for handling a &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown">non-graceful node shutdown&lt;/a>.
In Kubernetes v1.26, this feature moves to beta. This feature allows stateful workloads to failover to a different node after the original node is shut down or in a non-recoverable state, such as the hardware failure or broken OS.&lt;/p>
&lt;h2 id="what-is-a-node-shutdown-in-kubernetes">What is a node shutdown in Kubernetes?&lt;/h2>
&lt;p>In a Kubernetes cluster, it is possible for a node to shut down. This could happen either in a planned way or it could happen unexpectedly. You may plan for a security patch, or a kernel upgrade and need to reboot the node, or it may shut down due to preemption of VM instances. A node may also shut down due to a hardware failure or a software problem.&lt;/p>
&lt;p>To trigger a node shutdown, you could run a &lt;code>shutdown&lt;/code> or &lt;code>poweroff&lt;/code> command in a shell,
or physically press a button to power off a machine.&lt;/p>
&lt;p>A node shutdown could lead to workload failure if the node is not drained before the shutdown.&lt;/p>
&lt;p>In the following, we will describe what is a graceful node shutdown and what is a non-graceful node shutdown.&lt;/p>
&lt;h2 id="what-is-a-graceful-node-shutdown">What is a &lt;em>graceful&lt;/em> node shutdown?&lt;/h2>
&lt;p>The kubelet's handling for a &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">graceful node shutdown&lt;/a>
allows the kubelet to detect a node shutdown event, properly terminate the pods on that node,
and release resources before the actual shutdown.
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">Critical pods&lt;/a>
are terminated after all the regular pods are terminated, to ensure that the
essential functions of an application can continue to work as long as possible.&lt;/p>
&lt;h2 id="what-is-a-non-graceful-node-shutdown">What is a &lt;em>non-graceful&lt;/em> node shutdown?&lt;/h2>
&lt;p>A Node shutdown can be graceful only if the kubelet's &lt;em>node shutdown manager&lt;/em> can
detect the upcoming node shutdown action. However, there are cases where a kubelet
does not detect a node shutdown action. This could happen because the &lt;code>shutdown&lt;/code>
command does not trigger the &lt;a href="https://www.freedesktop.org/wiki/Software/systemd/inhibit">Inhibitor Locks&lt;/a> mechanism used by the kubelet on Linux, or because of a user error. For example, if
the &lt;code>shutdownGracePeriod&lt;/code> and &lt;code>shutdownGracePeriodCriticalPods&lt;/code> details are not
configured correctly for that node.&lt;/p>
&lt;p>When a node is shut down (or crashes), and that shutdown was &lt;strong>not&lt;/strong> detected by the kubelet
node shutdown manager, it becomes a non-graceful node shutdown. Non-graceful node shutdown
is a problem for stateful apps.
If a node containing a pod that is part of a StatefulSet is shut down in a non-graceful way, the Pod
will be stuck in &lt;code>Terminating&lt;/code> status indefinitely, and the control plane cannot create a replacement
Pod for that StatefulSet on a healthy node.
You can delete the failed Pods manually, but this is not ideal for a self-healing cluster.
Similarly, pods that ReplicaSets created as part of a Deployment will be stuck in &lt;code>Terminating&lt;/code> status, and
that were bound to the now-shutdown node, stay as &lt;code>Terminating&lt;/code> indefinitely.
If you have set a horizontal scaling limit, even those terminating Pods count against the limit,
so your workload may struggle to self-heal if it was already at maximum scale.
(By the way: if the node that had done a non-graceful shutdown comes back up, the kubelet does delete
the old Pod, and the control plane can make a replacement.)&lt;/p>
&lt;h2 id="what-s-new-for-the-beta">What's new for the beta?&lt;/h2>
&lt;p>For Kubernetes v1.26, the non-graceful node shutdown feature is beta and enabled by default.
The &lt;code>NodeOutOfServiceVolumeDetach&lt;/code>
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> is enabled by default
on &lt;code>kube-controller-manager&lt;/code> instead of being opt-in; you can still disable it if needed
(please also file an issue to explain the problem).&lt;/p>
&lt;p>On the instrumentation side, the kube-controller-manager reports two new metrics.&lt;/p>
&lt;dl>
&lt;dt>&lt;code>force_delete_pods_total&lt;/code>&lt;/dt>
&lt;dd>number of pods that are being forcibly deleted (resets on Pod garbage collection controller restart)&lt;/dd>
&lt;dt>&lt;code>force_delete_pod_errors_total&lt;/code>&lt;/dt>
&lt;dd>number of errors encountered when attempting forcible Pod deletion (also resets on Pod garbage collection controller restart)&lt;/dd>
&lt;/dl>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>In the case of a node shutdown, if a graceful shutdown is not working or the node is in a
non-recoverable state due to hardware failure or broken OS, you can manually add an &lt;code>out-of-service&lt;/code>
taint on the Node. For example, this can be &lt;code>node.kubernetes.io/out-of-service=nodeshutdown:NoExecute&lt;/code>
or &lt;code>node.kubernetes.io/out-of-service=nodeshutdown:NoSchedule&lt;/code>. This taint trigger pods on the node to
be forcefully deleted if there are no matching tolerations on the pods. Persistent volumes attached to the shutdown node will be detached, and new pods will be created successfully on a different running node.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl taint nodes &amp;lt;node-name&amp;gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note:&lt;/strong> Before applying the out-of-service taint, you must verify that a node is already in shutdown
or power-off state (not in the middle of restarting), either because the user intentionally shut it down
or the node is down due to hardware failures, OS issues, etc.&lt;/p>
&lt;p>Once all the workload pods that are linked to the out-of-service node are moved to a new running node, and the shutdown node has been recovered, you should remove that taint on the affected node after the node is recovered.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the Non-Graceful Node Shutdown implementation to GA in either 1.27 or 1.28.&lt;/p>
&lt;p>This feature requires a user to manually add a taint to the node to trigger the failover of workloads and remove the taint after the node is recovered.&lt;/p>
&lt;p>The cluster operator can automate this process by automatically applying the &lt;code>out-of-service&lt;/code> taint
if there is a programmatic way to determine that the node is really shut down and there isn’t IO between
the node and storage. The cluster operator can then automatically remove the taint after the workload
fails over successfully to another running node and that the shutdown node has been recovered.&lt;/p>
&lt;p>In the future, we plan to find ways to automatically detect and fence nodes that are shut down or in a non-recoverable state and fail their workloads over to another node.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>To learn more, read &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown">Non Graceful node shutdown&lt;/a> in the Kubernetes documentation.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>We offer a huge thank you to all the contributors who helped with design, implementation, and review of this feature:&lt;/p>
&lt;ul>
&lt;li>Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)&lt;/li>
&lt;li>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">derekwaynecarr&lt;/a>)&lt;/li>
&lt;li>Danielle Endocrimes (&lt;a href="https://github.com/endocrimes">endocrimes&lt;/a>)&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>)&lt;/li>
&lt;li>Ashutosh Kumar (&lt;a href="https://github.com/sonasingh46">sonasingh46&lt;/a>)&lt;/li>
&lt;li>Hemant Kumar (&lt;a href="https://github.com/gnufied">gnufied&lt;/a>)&lt;/li>
&lt;li>Yuiko Mouri(&lt;a href="https://github.com/YuikoTakada">YuikoTakada&lt;/a>)&lt;/li>
&lt;li>Mrunal Patel (&lt;a href="https://github.com/mrunalp">mrunalp&lt;/a>)&lt;/li>
&lt;li>David Porter (&lt;a href="https://github.com/bobbypage">bobbypage&lt;/a>)&lt;/li>
&lt;li>Yassine Tijani (&lt;a href="https://github.com/yastij">yastij&lt;/a>)&lt;/li>
&lt;li>Jing Xu (&lt;a href="https://github.com/jingxu97">jingxu97&lt;/a>)&lt;/li>
&lt;li>Xing Yang (&lt;a href="https://github.com/xing-yang">xing-yang&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>There are many people who have helped review the design and implementation along the way. We want to thank everyone who has contributed to this effort including the about 30 people who have reviewed the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown">KEP&lt;/a> and implementation over the last couple of years.&lt;/p>
&lt;p>This feature is a collaboration between SIG Storage and SIG Node. For those interested in getting involved with the design and development of any part of the Kubernetes Storage system, join the Kubernetes Storage Special Interest Group (SIG). For those interested in getting involved with the design and development of the components that support the controlled interactions between pods and host resources, join the Kubernetes Node SIG.&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: Alpha API For Dynamic Resource Allocation</title><link>https://kubernetes.io/blog/2022/12/15/dynamic-resource-allocation/</link><pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/15/dynamic-resource-allocation/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Patrick Ohly (Intel), Kevin Klues (NVIDIA)&lt;/p>
&lt;p>Dynamic resource allocation is a new API for requesting resources. It is a
generalization of the persistent volumes API for generic resources, making it possible to:&lt;/p>
&lt;ul>
&lt;li>access the same resource instance in different pods and containers,&lt;/li>
&lt;li>attach arbitrary constraints to a resource request to get the exact resource
you are looking for,&lt;/li>
&lt;li>initialize a resource according to parameters provided by the user.&lt;/li>
&lt;/ul>
&lt;p>Third-party resource drivers are responsible for interpreting these parameters
as well as tracking and allocating resources as requests come in.&lt;/p>
&lt;p>Dynamic resource allocation is an &lt;em>alpha feature&lt;/em> and only enabled when the
&lt;code>DynamicResourceAllocation&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature
gate&lt;/a> and the
&lt;code>resource.k8s.io/v1alpha1&lt;/code> &lt;a class='glossary-tooltip' title='A set of related paths in the Kubernetes API.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning' target='_blank' aria-label='API group'>API group&lt;/a> are enabled. For details, see the
&lt;code>--feature-gates&lt;/code> and &lt;code>--runtime-config&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver
parameters&lt;/a>.
The kube-scheduler, kube-controller-manager and kubelet components all need
the feature gate enabled as well.&lt;/p>
&lt;p>The default configuration of kube-scheduler enables the &lt;code>DynamicResources&lt;/code>
plugin if and only if the feature gate is enabled. Custom configurations may
have to be modified to include it.&lt;/p>
&lt;p>Once dynamic resource allocation is enabled, resource drivers can be installed
to manage certain kinds of hardware. Kubernetes has a test driver that is used
for end-to-end testing, but also can be run manually. See
&lt;a href="#running-the-test-driver">below&lt;/a> for step-by-step instructions.&lt;/p>
&lt;h2 id="api">API&lt;/h2>
&lt;p>The new &lt;code>resource.k8s.io/v1alpha1&lt;/code> &lt;a class='glossary-tooltip' title='A set of related paths in the Kubernetes API.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning' target='_blank' aria-label='API group'>API group&lt;/a> provides four new types:&lt;/p>
&lt;dl>
&lt;dt>ResourceClass&lt;/dt>
&lt;dd>Defines which resource driver handles a certain kind of
resource and provides common parameters for it. ResourceClasses
are created by a cluster administrator when installing a resource
driver.&lt;/dd>
&lt;dt>ResourceClaim&lt;/dt>
&lt;dd>Defines a particular resource instances that is required by a
workload. Created by a user (lifecycle managed manually, can be shared
between different Pods) or for individual Pods by the control plane based on
a ResourceClaimTemplate (automatic lifecycle, typically used by just one
Pod).&lt;/dd>
&lt;dt>ResourceClaimTemplate&lt;/dt>
&lt;dd>Defines the spec and some meta data for creating
ResourceClaims. Created by a user when deploying a workload.&lt;/dd>
&lt;dt>PodScheduling&lt;/dt>
&lt;dd>Used internally by the control plane and resource drivers
to coordinate pod scheduling when ResourceClaims need to be allocated
for a Pod.&lt;/dd>
&lt;/dl>
&lt;p>Parameters for ResourceClass and ResourceClaim are stored in separate objects,
typically using the type defined by a &lt;a class='glossary-tooltip' title='Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/' target='_blank' aria-label='CRD'>CRD&lt;/a> that was created when
installing a resource driver.&lt;/p>
&lt;p>With this alpha feature enabled, the &lt;code>spec&lt;/code> of Pod defines ResourceClaims that are needed for a Pod
to run: this information goes into a new
&lt;code>resourceClaims&lt;/code> field. Entries in that list reference either a ResourceClaim
or a ResourceClaimTemplate. When referencing a ResourceClaim, all Pods using
this &lt;code>.spec&lt;/code> (for example, inside a Deployment or StatefulSet) share the same
ResourceClaim instance. When referencing a ResourceClaimTemplate, each Pod gets
its own ResourceClaim instance.&lt;/p>
&lt;p>For a container defined within a Pod, the &lt;code>resources.claims&lt;/code> list
defines whether that container gets
access to these resource instances, which makes it possible to share resources
between one or more containers inside the same Pod. For example, an init container could
set up the resource before the application uses it.&lt;/p>
&lt;p>Here is an example of a fictional resource driver. Two ResourceClaim objects
will get created for this Pod and each container gets access to one of them.&lt;/p>
&lt;p>Assuming a resource driver called &lt;code>resource-driver.example.com&lt;/code> was installed
together with the following resource class:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: resource.k8s.io/v1alpha1
kind: ResourceClass
name: resource.example.com
driverName: resource-driver.example.com
&lt;/code>&lt;/pre>&lt;p>An end-user could then allocate two specific resources of type
&lt;code>resource.example.com&lt;/code> as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cats.resource.example.com/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClaimParameters&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>large-black-cats&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">color&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>black&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">size&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>large&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>resource.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceClaimTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>large-black-cats&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>resource.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parametersRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cats.resource.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClaimParameters&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>large-black-cats&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>–--&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pod-with-cats&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># two example containers; each container claims one cat resource&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>first-example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ubuntu:22.04&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;9999&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claims&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cat-0&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>second-example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ubuntu:22.04&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;9999&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claims&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cat-1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceClaims&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cat-0&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceClaimTemplateName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>large-black-cats&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cat-1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceClaimTemplateName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>large-black-cats&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="scheduling">Scheduling&lt;/h2>
&lt;p>In contrast to native resources (such as CPU or RAM) and
&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources">extended resources&lt;/a>
(managed by a
device plugin, advertised by kubelet), the scheduler has no knowledge of what
dynamic resources are available in a cluster or how they could be split up to
satisfy the requirements of a specific ResourceClaim. Resource drivers are
responsible for that. Drivers mark ResourceClaims as &lt;em>allocated&lt;/em> once resources
for it are reserved. This also then tells the scheduler where in the cluster a
claimed resource is actually available.&lt;/p>
&lt;p>ResourceClaims can get resources allocated as soon as the ResourceClaim
is created (&lt;em>immediate allocation&lt;/em>), without considering which Pods will use
the resource. The default (&lt;em>wait for first consumer&lt;/em>) is to delay allocation until
a Pod that relies on the ResourceClaim becomes eligible for scheduling.
This design with two allocation options is similar to how Kubernetes handles
storage provisioning with PersistentVolumes and PersistentVolumeClaims.&lt;/p>
&lt;p>In the wait for first consumer mode, the scheduler checks all ResourceClaims needed
by a Pod. If the Pods has any ResourceClaims, the scheduler creates a PodScheduling
(a special object that requests scheduling details on behalf of the Pod). The PodScheduling
has the same name and namespace as the Pod and the Pod as its as owner.
Using its PodScheduling, the scheduler informs the resource drivers
responsible for those ResourceClaims about nodes that the scheduler considers
suitable for the Pod. The resource drivers respond by excluding nodes that
don't have enough of the driver's resources left.&lt;/p>
&lt;p>Once the scheduler has that resource
information, it selects one node and stores that choice in the PodScheduling
object. The resource drivers then allocate resources based on the relevant
ResourceClaims so that the resources will be available on that selected node.
Once that resource allocation is complete, the scheduler attempts to schedule the Pod
to a suitable node. Scheduling can still fail at this point; for example, a different Pod could
be scheduled to the same node in the meantime. If this happens, already allocated
ResourceClaims may get deallocated to enable scheduling onto a different node.&lt;/p>
&lt;p>As part of this process, ResourceClaims also get reserved for the
Pod. Currently ResourceClaims can either be used exclusively by a single Pod or
an unlimited number of Pods.&lt;/p>
&lt;p>One key feature is that Pods do not get scheduled to a node unless all of
their resources are allocated and reserved. This avoids the scenario where
a Pod gets scheduled onto one node and then cannot run there, which is bad
because such a pending Pod also blocks all other resources like RAM or CPU that were
set aside for it.&lt;/p>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>The scheduler plugin must be involved in scheduling Pods which use
ResourceClaims. Bypassing the scheduler by setting the &lt;code>nodeName&lt;/code> field leads
to Pods that the kubelet refuses to start because the ResourceClaims are not
reserved or not even allocated. It may be possible to remove this
&lt;a href="https://github.com/kubernetes/kubernetes/issues/114005">limitation&lt;/a> in the
future.&lt;/p>
&lt;h2 id="writing-a-resource-driver">Writing a resource driver&lt;/h2>
&lt;p>A dynamic resource allocation driver typically consists of two separate-but-coordinating
components: a centralized controller, and a DaemonSet of node-local kubelet
plugins. Most of the work required by the centralized controller to coordinate
with the scheduler can be handled by boilerplate code. Only the business logic
required to actually allocate ResourceClaims against the ResourceClasses owned
by the plugin needs to be customized. As such, Kubernetes provides
the following package, including APIs for invoking this boilerplate code as
well as a &lt;code>Driver&lt;/code> interface that you can implement to provide their custom
business logic:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/dynamic-resource-allocation/tree/release-1.26/controller">k8s.io/dynamic-resource-allocation/controller&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Likewise, boilerplate code can be used to register the node-local plugin with
the kubelet, as well as start a gRPC server to implement the kubelet plugin
API. For drivers written in Go, the following package is recommended:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/dynamic-resource-allocation/tree/release-1.26/kubeletplugin">k8s.io/dynamic-resource-allocation/kubeletplugin&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>It is up to the driver developer to decide how these two components
communicate. The &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md">KEP&lt;/a> outlines an &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation#implementing-a-plugin-for-node-resources">approach using
CRDs&lt;/a>.&lt;/p>
&lt;p>Within SIG Node, we also plan to provide a complete &lt;a href="https://github.com/kubernetes-sigs/dra-example-driver">example
driver&lt;/a> that can serve
as a template for other drivers.&lt;/p>
&lt;h2 id="running-the-test-driver">Running the test driver&lt;/h2>
&lt;p>The following steps bring up a local, one-node cluster directly from the
Kubernetes source code. As a prerequisite, your cluster must have nodes with a container
runtime that supports the
&lt;a href="https://github.com/container-orchestrated-devices/container-device-interface">Container Device Interface&lt;/a>
(CDI). For example, you can run CRI-O &lt;a href="https://github.com/cri-o/cri-o/releases/tag/v1.23.2">v1.23.2&lt;/a> or later.
Once containerd v1.7.0 is released, we expect that you can run that or any later version.
In the example below, we use CRI-O.&lt;/p>
&lt;p>First, clone the Kubernetes source code. Inside that directory, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> hack/install-etcd.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> &lt;span style="color:#b8860b">RUNTIME_CONFIG&lt;/span>&lt;span style="color:#666">=&lt;/span>resource.k8s.io/v1alpha1 &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span>&lt;span style="color:#888"> FEATURE_GATES=DynamicResourceAllocation=true \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> DNS_ADDON=&amp;#34;coredns&amp;#34; \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> CGROUP_DRIVER=systemd \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> CONTAINER_RUNTIME_ENDPOINT=unix:///var/run/crio/crio.sock \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> LOG_LEVEL=6 \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> ENABLE_CSI_SNAPSHOTTER=false \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> API_SECURE_PORT=6444 \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> ALLOW_PRIVILEGED=1 \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> PATH=$(pwd)/third_party/etcd:$PATH \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> ./hack/local-up-cluster.sh -O
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">To start using your cluster, you can open up another terminal/tab and run:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="color:#888"> export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the cluster is up, in another
terminal run the test driver controller. &lt;code>KUBECONFIG&lt;/code> must be set for all of
the following commands.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> go run ./test/e2e/dra/test-driver --feature-gates &lt;span style="color:#b8860b">ContextualLogging&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span> -v&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span> controller
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In another terminal, run the kubelet plugin:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> sudo mkdir -p /var/run/cdi &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span>&lt;span style="color:#888"> sudo chmod a+rwx /var/run/cdi /var/lib/kubelet/plugins_registry /var/lib/kubelet/plugins/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> go run ./test/e2e/dra/test-driver --feature-gates &lt;span style="color:#b8860b">ContextualLogging&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span> -v&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">6&lt;/span> kubelet-plugin
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Changing the permissions of the directories makes it possible to run and (when
using delve) debug the kubelet plugin as a normal user, which is convenient
because it uses the already populated Go cache. Remember to restore permissions
with &lt;code>sudo chmod go-w&lt;/code> when done. Alternatively, you can also build the binary
and run that as root.&lt;/p>
&lt;p>Now the cluster is ready to create objects:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl create -f test/e2e/dra/test-driver/deploy/example/resourceclass.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">resourceclass.resource.k8s.io/example created
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl create -f test/e2e/dra/test-driver/deploy/example/pod-inline.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">configmap/test-inline-claim-parameters created
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">resourceclaimtemplate.resource.k8s.io/test-inline-claim-template created
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">pod/test-inline-claim created
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get resourceclaims
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME RESOURCECLASSNAME ALLOCATIONMODE STATE AGE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">test-inline-claim-resource example WaitForFirstConsumer allocated,reserved 8s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">test-inline-claim 0/2 Completed 0 21s
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The test driver doesn't do much, it only sets environment variables as defined
in the ConfigMap. The test pod dumps the environment, so the log can be checked
to verify that everything worked:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> kubectl logs test-inline-claim with-resource | grep user_a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">user_a=&amp;#39;b&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;ul>
&lt;li>See the
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md">Dynamic Resource Allocation&lt;/a>
KEP for more information on the design.&lt;/li>
&lt;li>Read &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation&lt;/a>
in the official Kubernetes documentation.&lt;/li>
&lt;li>You can participate in
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-node/README.md">SIG Node&lt;/a>
and / or the &lt;a href="https://github.com/cncf/tag-runtime/blob/master/wg/COD.md">CNCF Container Orchestrated Device Working Group&lt;/a>.&lt;/li>
&lt;li>You can view or comment on the &lt;a href="https://github.com/orgs/kubernetes/projects/95/views/1">project board&lt;/a>
for dynamic resource allocation.&lt;/li>
&lt;li>In order to move this feature towards beta, we need feedback from hardware
vendors, so here's a call to action: try out this feature, consider how it can help
with problems that your users are having, and write resource drivers…&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.26: Windows HostProcess Containers Are Generally Available</title><link>https://kubernetes.io/blog/2022/12/13/windows-host-process-containers-ga/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/13/windows-host-process-containers-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Brandon Smith (Microsoft) and Mark Rossetti (Microsoft)&lt;/p>
&lt;p>The long-awaited day has arrived: HostProcess containers, the Windows equivalent to Linux privileged
containers, has finally made it to &lt;strong>GA in Kubernetes 1.26&lt;/strong>!&lt;/p>
&lt;p>What are HostProcess containers and why are they useful?&lt;/p>
&lt;p>Cluster operators are often faced with the need to configure their nodes upon provisioning such as
installing Windows services, configuring registry keys, managing TLS certificates,
making network configuration changes, or even deploying monitoring tools such as a Prometheus's node-exporter.
Previously, performing these actions on Windows nodes was usually done by running PowerShell scripts
over SSH or WinRM sessions and/or working with your cloud provider's virtual machine management tooling.
HostProcess containers now enable you to do all of this and more with minimal effort using Kubernetes native APIs.&lt;/p>
&lt;p>With HostProcess containers you can now package any payload
into the container image, map volumes into containers at runtime, and manage them like any other Kubernetes workload.
You get all the benefits of containerized packaging and deployment methods combined with a reduction in
both administrative and development cost.
Gone are the days where cluster operators would need to manually log onto
Windows nodes to perform administrative duties.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess containers&lt;/a> differ
quite significantly from regular Windows Server containers.
They are run directly as processes on the host with the access policies of
a user you specify. HostProcess containers run as either the built-in Windows system accounts or
ephemeral users within a user group defined by you. HostProcess containers also share
the host's network namespace and access/configure storage mounts visible to the host.
On the other hand, Windows Server containers are highly isolated and exist in a separate
execution namespace. Direct access to the host from a Windows Server container is explicitly disallowed
by default.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>Windows HostProcess containers are implemented with Windows &lt;a href="https://learn.microsoft.com/en-us/windows/win32/procthread/job-objects">&lt;em>Job Objects&lt;/em>&lt;/a>,
a break from the previous container model which use server silos.
Job Objects are components of the Windows OS which offer the ability to
manage a group of processes as a group (also known as a &lt;em>job&lt;/em>) and assign resource constraints to the
group as a whole. Job objects are specific to the Windows OS and are not associated with
the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job API&lt;/a>. They have no process
or file system isolation,
enabling the privileged payload to view and edit the host file system with the
desired permissions, among other host resources. The init process, and any processes
it launches (including processes explicitly launched by the user) are all assigned to the
job object of that container. When the init process exits or is signaled to exit,
all the processes in the job will be signaled to exit, the job handle will be
closed and the storage will be unmounted.&lt;/p>
&lt;p>HostProcess and Linux privileged containers enable similar scenarios but differ
greatly in their implementation (hence the naming difference). HostProcess containers
have their own &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#windowssecuritycontextoptions-v1-core">PodSecurityContext&lt;/a> fields.
Those used to configure Linux privileged containers &lt;strong>do not&lt;/strong> apply. Enabling privileged access to a Windows host is a
fundamentally different process than with Linux so the configuration and
capabilities of each differ significantly. Below is a diagram detailing the
overall architecture of Windows HostProcess containers:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2022/12/13/windows-host-process-containers-ga/hpc_architecture.svg"
alt="HostProcess Architecture"/>
&lt;/figure>
&lt;p>Two major features were added prior to moving to stable: the ability to run as local user accounts, and
a simplified method of accessing volume mounts. To learn more, read
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod&lt;/a>.&lt;/p>
&lt;h2 id="hostprocess-containers-in-action">HostProcess containers in action&lt;/h2>
&lt;p>Kubernetes SIG Windows has been busy putting HostProcess containers to use - even before GA!
They've been very excited to use HostProcess containers for a number of important activities
that were a pain to perform in the past.&lt;/p>
&lt;p>Here are just a few of the many use use cases with example deployments:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/hostprocess/calico#calico-example">CNI solutions and kube-proxy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus-community/windows_exporter/blob/master/kubernetes/windows-exporter-daemonset.yaml">windows-exporter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/hostprocess/csi-proxy">csi-proxy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jsturtevant/windows-debug">Windows-debug container&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/hostprocess/eventflow-logger">ETW event streaming&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>A HostProcess container can be built using any base image of your choosing, however, for convenience we have
created &lt;a href="https://github.com/microsoft/windows-host-process-containers-base-image">a HostProcess container base image&lt;/a>.
This image is only a few KB in size and does not inherit any of the same compatibility requirements as regular Windows
server containers which allows it to run on any Windows server version.&lt;/p>
&lt;p>To use that Microsoft image, put this in your &lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-dockerfile" data-lang="dockerfile">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">FROM&lt;/span>&lt;span style="color:#b44"> mcr.microsoft.com/oss/kubernetes/windows-host-process-containers-base-image:v1.0.0&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can run HostProcess containers from within a
&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/#privileged-mode-for-containers">HostProcess Pod&lt;/a>.&lt;/p>
&lt;p>To get started with running Windows containers,
see the general guidance for &lt;a href="https://kubernetes.io/docs/setup/production-environment/windows/">deploying Windows nodes&lt;/a>.
If you have a compatible node (for example: Windows as the operating system
with containerd v1.7 or later as the container runtime), you can deploy a Pod with one
or more HostProcess containers.
See the &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/#before-you-begin">Create a Windows HostProcess Pod - Prerequisites&lt;/a>
for more information.&lt;/p>
&lt;p>Please note that within a Pod, you can't mix HostProcess containers with normal Windows containers.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Work through &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Read about Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Read the enhancement proposal &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-windows/1981-windows-privileged-container-support">Windows Privileged Containers and Host Networking Mode&lt;/a> (KEP-1981)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Watch the &lt;a href="https://www.youtube.com/watch?v=LcXT9pVkwvo">Windows HostProcess for Configuration and Beyond&lt;/a> KubeCon NA 2022 talk&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Get involved with &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">SIG Windows&lt;/a>
to contribute!&lt;/p></description></item><item><title>Blog: Kubernetes 1.26: We're now signing our binary release artifacts!</title><link>https://kubernetes.io/blog/2022/12/12/kubernetes-release-artifact-signing/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/12/kubernetes-release-artifact-signing/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sascha Grunert&lt;/p>
&lt;p>The Kubernetes Special Interest Group (SIG) Release is proud to announce that we
are digitally signing all release artifacts, and that this aspect of Kubernetes
has now reached &lt;em>beta&lt;/em>.&lt;/p>
&lt;p>Signing artifacts provides end users a chance to verify the integrity of the
downloaded resource. It allows to mitigate man-in-the-middle attacks directly on
the client side and therefore ensures the trustfulness of the remote serving the
artifacts. The overall goal of out past work was to define the used tooling for
signing all Kubernetes related artifacts as well as providing a standard signing
process for related projects (for example for those in &lt;a href="https://github.com/kubernetes-sigs">kubernetes-sigs&lt;/a>).&lt;/p>
&lt;p>We already signed all officially released container images (from Kubernetes v1.24 onwards).
Image signing was alpha for v1.24 and v1.25. For v1.26, we've added all
&lt;strong>binary artifacts&lt;/strong> to the signing process as well! This means that now all
&lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.26/CHANGELOG/CHANGELOG-1.26.md#downloads-for-v1260">client, server and source tarballs&lt;/a>, &lt;a href="https://gcsweb.k8s.io/gcs/kubernetes-release/release/v1.26.0/bin">binary artifacts&lt;/a>,
&lt;a href="https://storage.googleapis.com/kubernetes-release/release/v1.26.0/kubernetes-release.spdx">Software Bills of Material (SBOMs)&lt;/a> as well as the &lt;a href="https://storage.googleapis.com/kubernetes-release/release/v1.26.0/provenance.json">build
provenance&lt;/a> will be signed using &lt;a href="https://github.com/sigstore/cosign">cosign&lt;/a>. Technically
speaking, we now ship additional &lt;code>*.sig&lt;/code> (signature) and &lt;code>*.cert&lt;/code> (certificate)
files side by side to the artifacts for verifying their integrity.&lt;/p>
&lt;p>To verify an artifact, for example &lt;code>kubectl&lt;/code>, you can download the
signature and certificate alongside with the binary. I use the release candidate
&lt;code>rc.1&lt;/code> of v1.26 for demonstration purposes because the final has not been released yet:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>curl -sSfL https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl -o kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -sSfL https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.sig -o kubectl.sig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -sSfL https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.cert -o kubectl.cert
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then you can verify &lt;code>kubectl&lt;/code> using &lt;a href="https://github.com/sigstore/cosign">&lt;code>cosign&lt;/code>&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b8860b">COSIGN_EXPERIMENTAL&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">1&lt;/span> cosign verify-blob kubectl --signature kubectl.sig --certificate kubectl.cert
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code>tlog entry verified with uuid: 5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657 index: 8173886
Verified OK
&lt;/code>&lt;/pre>&lt;p>The UUID can be used to query the &lt;a href="https://github.com/sigstore/rekor">rekor&lt;/a> transparency log:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>rekor-cli get --uuid 5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code>LogID: c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d
Index: 8173886
IntegratedTime: 2022-11-30T18:59:07Z
UUID: 24296fb24b8ad77a5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657
Body: {
&amp;#34;HashedRekordObj&amp;#34;: {
&amp;#34;data&amp;#34;: {
&amp;#34;hash&amp;#34;: {
&amp;#34;algorithm&amp;#34;: &amp;#34;sha256&amp;#34;,
&amp;#34;value&amp;#34;: &amp;#34;982dfe7eb5c27120de6262d30fa3e8029bc1da9e632ce70570e9c921d2851fc2&amp;#34;
}
},
&amp;#34;signature&amp;#34;: {
&amp;#34;content&amp;#34;: &amp;#34;MEQCIH0e1/0svxMoLzjeyhAaLFSHy5ZaYy0/2iQl2t3E0Pj4AiBsWmwjfLzrVyp9/v1sy70Q+FHE8miauOOVkAW2lTYVug==&amp;#34;,
&amp;#34;publicKey&amp;#34;: {
&amp;#34;content&amp;#34;: &amp;#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWthZ0F3SUJBZ0lVRldab0pLSUlFWkp3LzdsRkFrSVE2SHBQdi93d0NnWUlLb1pJemowRUF3TXcKTnpFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUjR3SEFZRFZRUURFeFZ6YVdkemRHOXlaUzFwYm5SbApjbTFsWkdsaGRHVXdIaGNOTWpJeE1UTXdNVGcxT1RBMldoY05Nakl4TVRNd01Ua3dPVEEyV2pBQU1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVDT3h5OXBwTFZzcVFPdHJ6RFgveTRtTHZSeU1scW9sTzBrS0EKTlJDM3U3bjMreHorYkhvWVkvMUNNRHpJQjBhRTA3NkR4ZWVaSkhVaWFjUXU4a0dDNktPQ0FXVXdnZ0ZoTUE0RwpBMVVkRHdFQi93UUVBd0lIZ0RBVEJnTlZIU1VFRERBS0JnZ3JCZ0VGQlFjREF6QWRCZ05WSFE0RUZnUVV5SmwxCkNlLzIzNGJmREJZQ2NzbXkreG5qdnpjd0h3WURWUjBqQkJnd0ZvQVUzOVBwejFZa0VaYjVxTmpwS0ZXaXhpNFkKWkQ4d1FnWURWUjBSQVFIL0JEZ3dOb0UwYTNKbGJDMXpkR0ZuYVc1blFHczRjeTF5Wld4bGJtY3RjSEp2WkM1cApZVzB1WjNObGNuWnBZMlZoWTJOdmRXNTBMbU52YlRBcEJnb3JCZ0VFQVlPL01BRUJCQnRvZEhSd2N6b3ZMMkZqClkyOTFiblJ6TG1kdmIyZHNaUzVqYjIwd2dZb0dDaXNHQVFRQjFua0NCQUlFZkFSNkFIZ0FkZ0RkUFRCcXhzY1IKTW1NWkhoeVpaemNDb2twZXVONDhyZitIaW5LQUx5bnVqZ0FBQVlUSjZDdlJBQUFFQXdCSE1FVUNJRXI4T1NIUQp5a25jRFZpNEJySklXMFJHS0pqNkQyTXFGdkFMb0I5SmNycXlBaUVBNW4xZ283cmQ2U3ZVeXNxeldhMUdudGZKCllTQnVTZHF1akVySFlMQTUrZTR3Q2dZSUtvWkl6ajBFQXdNRFpnQXdZd0l2Tlhub3pyS0pWVWFESTFiNUlqa1oKUWJJbDhvcmlMQ1M4MFJhcUlBSlJhRHNCNTFUeU9iYTdWcGVYdThuTHNjVUNNREU4ZmpPZzBBc3ZzSXp2azNRUQo0c3RCTkQrdTRVV1UrcjhYY0VxS0YwNGJjTFQwWEcyOHZGQjRCT2x6R204K093PT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=&amp;#34;
}
}
}
}
&lt;/code>&lt;/pre>&lt;p>The &lt;code>HashedRekordObj.signature.content&lt;/code> should match the content of the file
&lt;code>kubectl.sig&lt;/code> and &lt;code>HashedRekordObj.signature.publicKey.content&lt;/code> should be
identical with the contents of &lt;code>kubectl.cert&lt;/code>. It is also possible to specify
the remote certificate and signature locations without downloading them:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b8860b">COSIGN_EXPERIMENTAL&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">1&lt;/span> cosign verify-blob kubectl &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --signature https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.sig &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --certificate https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.cert
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code>tlog entry verified with uuid: 5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657 index: 8173886
Verified OK
&lt;/code>&lt;/pre>&lt;p>All of the mentioned steps as well as how to verify container images are
outlined in the official documentation about how to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts">Verify Signed Kubernetes
Artifacts&lt;/a>. In one of the next upcoming Kubernetes releases we will
working making the global story more mature by ensuring that truly all
Kubernetes artifacts are signed. Beside that, we are considering using Kubernetes
owned infrastructure for the signing (root trust) and verification (transparency
log) process.&lt;/p>
&lt;h2 id="getting-involved">Getting involved&lt;/h2>
&lt;p>If you're interested in contributing to SIG Release, then consider applying for
the upcoming v1.27 shadowing program (watch for the announcement on
&lt;a href="https://groups.google.com/a/kubernetes.io/g/dev">k-dev&lt;/a>) or join our &lt;a href="http://bit.ly/k8s-sig-release-meeting">weekly meeting&lt;/a> to say &lt;em>hi&lt;/em>.&lt;/p>
&lt;p>We're looking forward to making even more of those awesome changes for future
Kubernetes releases. For example, we're working on the &lt;a href="https://github.com/kubernetes/enhancements/issues/3027">SLSA Level 3 Compliance
in the Kubernetes Release Process&lt;/a> or the &lt;a href="https://github.com/kubernetes/enhancements/issues/2853">Renaming of the kubernetes/kubernetes
default branch name to &lt;code>main&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Thank you for reading this blog post! I'd like to use this opportunity to give
all involved SIG Release folks a special shout-out for shipping this feature in
time!&lt;/p>
&lt;p>Feel free to reach out to us by using the &lt;a href="https://groups.google.com/g/kubernetes-sig-release">SIG Release mailing list&lt;/a> or
the &lt;a href="http://slack.k8s.io">#sig-release&lt;/a> Slack channel.&lt;/p>
&lt;h2 id="additional-resources">Additional resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3031">Signing Release Artifacts Enhancement Proposal&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes v1.26: Electrifying</title><link>https://kubernetes.io/blog/2022/12/09/kubernetes-v1-26-release/</link><pubDate>Fri, 09 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/09/kubernetes-v1-26-release/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.26/release-team.md">Kubernetes 1.26 Release Team&lt;/a>&lt;/p>
&lt;p>It's with immense joy that we announce the release of Kubernetes v1.26!&lt;/p>
&lt;p>This release includes a total of 37 enhancements: eleven of them are graduating to Stable, ten are
graduating to Beta, and sixteen of them are entering Alpha. We also have twelve features being
deprecated or removed, three of which we better detail in this announcement.&lt;/p>
&lt;h2 id="release-theme-and-logo">Release theme and logo&lt;/h2>
&lt;p>&lt;strong>Kubernetes 1.26: Electrifying&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-12-08-kubernetes-1.26-release/kubernetes-1.26.png"
alt="Kubernetes 1.26 Electrifying logo"/>
&lt;/figure>
&lt;p>The theme for Kubernetes v1.26 is &lt;em>Electrifying&lt;/em>.&lt;/p>
&lt;p>Each Kubernetes release is the result of the coordinated effort of dedicated volunteers, and only
made possible due to the use of a diverse and complex set of computing resources, spread out through
multiple datacenters and regions worldwide. The end result of a release - the binaries, the image
containers, the documentation - are then deployed on a growing number of personal, on-premises, and
cloud computing resources.&lt;/p>
&lt;p>In this release we want to recognise the importance of all these building blocks on which Kubernetes
is developed and used, while at the same time raising awareness on the importance of taking the
energy consumption footprint into account: environmental sustainability is an inescapable concern of
creators and users of any software solution, and the environmental footprint of software, like
Kubernetes, an area which we believe will play a significant role in future releases.&lt;/p>
&lt;p>As a community, we always work to make each new release process better than before (in this release,
we have &lt;a href="https://github.com/orgs/kubernetes/projects/98/views/1">started to use Projects for tracking
enhancements&lt;/a>, for example). If &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/">v1.24
&amp;quot;Stargazer&amp;quot;&lt;/a> &lt;em>had us looking upwards, to
what is possible when our community comes together&lt;/em>, and &lt;a href="https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/">v1.25
&amp;quot;Combiner&amp;quot;&lt;/a> &lt;em>what the combined efforts of our community
are capable of&lt;/em>, this v1.26 &amp;quot;Electrifying&amp;quot; is also dedicated to all of those whose individual
motion, integrated into the release flow, made all of this possible.&lt;/p>
&lt;h2 id="major-themes">Major themes&lt;/h2>
&lt;p>Kubernetes v1.26 is composed of many changes, brought to you by a worldwide team of volunteers. For
this release, we have identified several major themes.&lt;/p>
&lt;h3 id="change-in-container-image-registry">Change in container image registry&lt;/h3>
&lt;p>In the previous release, &lt;a href="https://github.com/kubernetes/kubernetes/pull/109938">Kubernetes changed the container
registry&lt;/a>, allowing the spread of the load
across multiple Cloud Providers and Regions, a change that reduced the reliance on a single entity
and provided a faster download experience for a large number of users.&lt;/p>
&lt;p>This release of Kubernetes is the first that is exclusively published in the new &lt;code>registry.k8s.io&lt;/code>
container image registry. In the (now legacy) &lt;code>k8s.gcr.io&lt;/code> image registry, no container images tags
for v1.26 will be published, and only tags from releases before v1.26 will continue to be
updated. Refer to &lt;a href="https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/">registry.k8s.io: faster, cheaper and Generally
Available&lt;/a> for more information on the
motivation, advantages, and implications of this significant change.&lt;/p>
&lt;h3 id="cri-v1alpha2-removed">CRI v1alpha2 removed&lt;/h3>
&lt;p>With the adoption of the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cri/">Container Runtime Interface&lt;/a> (CRI) and
the &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">removal of dockershim&lt;/a> in v1.24, the CRI is the only
supported and documented way through which Kubernetes interacts with different container
runtimes. Each kubelet negotiates which version of CRI to use with the container runtime on that
node.&lt;/p>
&lt;p>In the previous release, the Kubernetes project recommended using CRI version &lt;code>v1&lt;/code>, but kubelet
could still negotiate the use of CRI &lt;code>v1alpha2&lt;/code>, which was deprecated.&lt;/p>
&lt;p>Kubernetes v1.26 drops support for CRI &lt;code>v1alpha2&lt;/code>. That
&lt;a href="https://github.com/kubernetes/kubernetes/pull/110618">removal&lt;/a> will result in the kubelet not
registering the node if the container runtime doesn't support CRI &lt;code>v1&lt;/code>. This means that containerd
minor version 1.5 and older are not supported in Kubernetes 1.26; if you use containerd, you will
need to upgrade to containerd version 1.6.0 or later &lt;strong>before&lt;/strong> you upgrade that node to Kubernetes
v1.26. This applies equally to any other container runtimes that only support the &lt;code>v1alpha2&lt;/code>: if
that affects you, you should contact the container runtime vendor for advice or check their website
for additional instructions in how to move forward.&lt;/p>
&lt;h3 id="storage-improvements">Storage improvements&lt;/h3>
&lt;p>Following the GA of the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">core Container Storage Interface (CSI)
Migration&lt;/a>
feature in the previous release, CSI migration is an on-going effort that we've been working on for
a few releases now, and this release continues to add (and remove) features aligned with the
migration's goals, as well as other improvements to Kubernetes storage.&lt;/p>
&lt;h4 id="csi-migration-for-azure-file-and-vsphere-graduated-to-stable">CSI migration for Azure File and vSphere graduated to stable&lt;/h4>
&lt;p>Both the &lt;a href="https://github.com/kubernetes/enhancements/issues/1491">vSphere&lt;/a> and
&lt;a href="https://github.com/kubernetes/enhancements/issues/1885">Azure&lt;/a> in-tree driver migration to CSI have
graduated to Stable. You can find more information about them in the &lt;a href="https://github.com/kubernetes-sigs/vsphere-csi-driver">vSphere CSI
driver&lt;/a> and &lt;a href="https://github.com/kubernetes-sigs/azurefile-csi-driver">Azure File CSI
driver&lt;/a> repositories.&lt;/p>
&lt;h4 id="delegate-fsgroup-to-csi-driver-graduated-to-stable">&lt;em>Delegate FSGroup to CSI Driver&lt;/em> graduated to stable&lt;/h4>
&lt;p>This feature allows Kubernetes to &lt;a href="https://github.com/kubernetes/enhancements/issues/2317">supply the pod's &lt;code>fsGroup&lt;/code> to the CSI driver when a volume is
mounted&lt;/a> so that the driver can utilize
mount options to control volume permissions. Previously, the kubelet would always apply the
&lt;code>fsGroup&lt;/code>ownership and permission change to files in the volume according to the policy specified in
the Pod's &lt;code>.spec.securityContext.fsGroupChangePolicy&lt;/code> field. Starting with this release, CSI
drivers have the option to apply the &lt;code>fsGroup&lt;/code> settings during attach or mount time of the volumes.&lt;/p>
&lt;h4 id="in-tree-glusterfs-driver-removal">In-tree GlusterFS driver removal&lt;/h4>
&lt;p>Already deprecated in the v1.25 release, the in-tree GlusterFS driver was
&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">removed&lt;/a> in this release.&lt;/p>
&lt;h4 id="in-tree-openstack-cinder-driver-removal">In-tree OpenStack Cinder driver removal&lt;/h4>
&lt;p>This release removed the deprecated in-tree storage integration for OpenStack (the &lt;code>cinder&lt;/code> volume
type). You should migrate to external cloud provider and CSI driver from
&lt;a href="https://github.com/kubernetes/cloud-provider-openstack">https://github.com/kubernetes/cloud-provider-openstack&lt;/a> instead. For more information, visit &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">Cinder
in-tree to CSI driver migration&lt;/a>.&lt;/p>
&lt;h3 id="signing-kubernetes-release-artifacts-graduates-to-beta">Signing Kubernetes release artifacts graduates to beta&lt;/h3>
&lt;p>Introduced in Kubernetes v1.24, &lt;a href="https://github.com/kubernetes/enhancements/issues/3031">this
feature&lt;/a> constitutes a significant milestone
in improving the security of the Kubernetes release process. All release artifacts are signed
keyless using &lt;a href="https://github.com/sigstore/cosign/">cosign&lt;/a>, and both binary artifacts and images
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/">can be verified&lt;/a>.&lt;/p>
&lt;h3 id="support-for-windows-privileged-containers-graduates-to-stable">Support for Windows privileged containers graduates to stable&lt;/h3>
&lt;p>Privileged container support allows containers to run with similar access to the host as processes
that run on the host directly. Support for this feature in Windows nodes, called &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess
containers&lt;/a>, will now &lt;a href="https://github.com/kubernetes/enhancements/issues/1981">graduate to Stable&lt;/a>,
enabling access to host resources (including network resources) from privileged containers.&lt;/p>
&lt;h3 id="improvements-to-kubernetes-metrics">Improvements to Kubernetes metrics&lt;/h3>
&lt;p>This release has several noteworthy improvements on metrics.&lt;/p>
&lt;h4 id="metrics-framework-extension-graduates-to-alpha">Metrics framework extension graduates to alpha&lt;/h4>
&lt;p>The metrics framework extension &lt;a href="https://github.com/kubernetes/enhancements/issues/3498">graduates to
Alpha&lt;/a>, and
&lt;a href="https://kubernetes.io/docs/reference/instrumentation/metrics/">documentation is now published for every metric in the
Kubernetes codebase&lt;/a>.This enhancement adds two additional metadata
fields to Kubernetes metrics: &lt;code>Internal&lt;/code> and &lt;code>Beta&lt;/code>, representing different stages of metric maturity.&lt;/p>
&lt;h4 id="component-health-service-level-indicators-graduates-to-alpha">Component Health Service Level Indicators graduates to alpha&lt;/h4>
&lt;p>Also improving on the ability to consume Kubernetes metrics, &lt;a href="https://kubernetes.io/docs/reference/instrumentation/slis/">component health Service Level
Indicators (SLIs)&lt;/a> have &lt;a href="https://github.com/kubernetes/kubernetes/pull/112884">graduated to
Alpha&lt;/a>: by enabling the &lt;code>ComponentSLIs&lt;/code>
feature flag there will be an additional metrics endpoint which allows the calculation of Service
Level Objectives (SLOs) from raw healthcheck data converted into metric format.&lt;/p>
&lt;h4 id="feature-metrics-are-now-available">Feature metrics are now available&lt;/h4>
&lt;p>Feature metrics are now available for each Kubernetes component, making it possible to &lt;a href="https://github.com/kubernetes/kubernetes/pull/112690">track
whether each active feature gate is enabled&lt;/a>
by checking the component's metric endpoint for &lt;code>kubernetes_feature_enabled&lt;/code>.&lt;/p>
&lt;h3 id="dynamic-resource-allocation-graduates-to-alpha">Dynamic Resource Allocation graduates to alpha&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource
Allocation&lt;/a>
is a &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md">new feature&lt;/a>
that puts resource scheduling in the hands of third-party developers: it offers an
alternative to the limited &amp;quot;countable&amp;quot; interface for requesting access to resources
(e.g. &lt;code>nvidia.com/gpu: 2&lt;/code>), providing an API more akin to that of persistent volumes. Under the
hood, it uses the &lt;a href="https://github.com/container-orchestrated-devices/container-device-interface">Container Device
Interface&lt;/a> (CDI) to do
its device injection. This feature is blocked by the &lt;code>DynamicResourceAllocation&lt;/code> feature gate.&lt;/p>
&lt;h3 id="cel-in-admission-control-graduates-to-alpha">CEL in Admission Control graduates to alpha&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/3488">This feature&lt;/a> introduces a &lt;code>v1alpha1&lt;/code> API for &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/">validating admission
policies&lt;/a>, enabling extensible admission
control via &lt;a href="https://github.com/google/cel-spec">Common Expression Language&lt;/a> expressions. Currently,
custom policies are enforced via &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission
webhooks&lt;/a>,
which, while flexible, have a few drawbacks when compared to in-process policy enforcement. To use,
enable the &lt;code>ValidatingAdmissionPolicy&lt;/code> feature gate and the &lt;code>admissionregistration.k8s.io/v1alpha1&lt;/code>
API via &lt;code>--runtime-config&lt;/code>.&lt;/p>
&lt;h3 id="pod-scheduling-improvements">Pod scheduling improvements&lt;/h3>
&lt;p>Kubernetes v1.26 introduces some relevant enhancements to the ability to better control scheduling
behavior.&lt;/p>
&lt;h4 id="podschedulingreadiness-graduates-to-alpha">&lt;code>PodSchedulingReadiness&lt;/code> graduates to alpha&lt;/h4>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/3521">This feature&lt;/a> introduces a &lt;code>.spec.schedulingGates&lt;/code>
field to Pod's API, to &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">indicate whether the Pod is allowed to be scheduled or not&lt;/a>. External users/controllers can use this field to hold a Pod from scheduling based on their policies and
needs.&lt;/p>
&lt;h4 id="nodeinclusionpolicyinpodtopologyspread-graduates-to-beta">&lt;code>NodeInclusionPolicyInPodTopologySpread&lt;/code> graduates to beta&lt;/h4>
&lt;p>By specifying a &lt;code>nodeInclusionPolicy&lt;/code> in &lt;code>topologySpreadConstraints&lt;/code>, you can control whether to
&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">take taints/tolerations into consideration&lt;/a>
when calculating Pod Topology Spread skew.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduations-to-stable">Graduations to stable&lt;/h3>
&lt;p>This release includes a total of eleven enhancements promoted to Stable:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Support for Windows privileged containers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1491">vSphere in-tree to CSI driver migration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2317">Allow Kubernetes to supply pod's fsgroup to CSI driver on mount&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1885">Azure file in-tree to CSI driver migration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2307">Job tracking without lingering Pods&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2086">Service Internal Traffic Policy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2133">Kubelet Credential Provider&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1435">Support of mixed protocols in Services with type=LoadBalancer&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3070">Reserve Service IP Ranges For Dynamic and Static IP Allocation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3570">CPUManager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3573">DeviceManager&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations-and-removals">Deprecations and removals&lt;/h3>
&lt;p>12 features were &lt;a href="https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/">deprecated or removed&lt;/a> from
Kubernetes with this release.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/110618">CRI &lt;code>v1alpha2&lt;/code> API is removed&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v126">Removal of the &lt;code>v1beta1&lt;/code> flow control API group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#horizontalpodautoscaler-v126">Removal of the &lt;code>v2beta2&lt;/code> HorizontalPodAutoscaler API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS plugin removed from available in-tree drivers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112120">Removal of legacy command line arguments relating to logging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112133">Removal of &lt;code>kube-proxy&lt;/code> userspace modes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112341">Removal of in-tree credential management code&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1489">The in-tree OpenStack cloud provider is removed&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112643">Removal of dynamic kubelet configuration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/113116">Deprecation of non-inclusive &lt;code>kubectl&lt;/code> flag&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/38186">Deprecations for &lt;code>kube-apiserver&lt;/code> command line arguments&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112261">Deprecations for &lt;code>kubectl run&lt;/code> command line arguments&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release notes&lt;/h3>
&lt;p>The complete details of the Kubernetes v1.26 release are available in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md">release
notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes v1.26 is available for download on &lt;a href="https://k8s.io/releases/download/">the Kubernetes site&lt;/a>.
To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local
Kubernetes clusters using containers as &amp;quot;nodes&amp;quot;, with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>. You can also
easily install v1.26 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release team&lt;/h3>
&lt;p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each
release team is made up of dedicated community volunteers who work together to build the many pieces
that make up the Kubernetes releases you rely on. This requires the specialized skills of people
from all corners of our community, from the code itself to its documentation and project management.&lt;/p>
&lt;p>We would like to thank the entire &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.26/release-team.md">release team&lt;/a>
for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.26 release for our community.&lt;/p>
&lt;p>A very special thanks is in order for our Release Lead, Leonard Pahlke, for successfully steering
the entire release team throughout the entire release cycle, by making sure that we could all
contribute in the best way possible to this release through his constant support and attention to
the many and diverse details that make up the path to a successful release.&lt;/p>
&lt;h3 id="user-highlights">User highlights&lt;/h3>
&lt;ul>
&lt;li>Wortell faced increasingly higher ammounts of developer expertise and time for daily
infrastructure management. &lt;a href="https://www.cncf.io/case-studies/wortell/">They used Dapr to reduce the complexity and amount of required
infrastructure-related code, allowing them to focus more time on new
features&lt;/a>.&lt;/li>
&lt;li>Utmost handles sensitive personal data and needed SOC 2 Type II attestation, ISO 27001
certification, and zero trust networking. &lt;a href="https://www.cncf.io/case-studies/utmost/">Using Cilium, they created automated pipelines that
allowed developers to create new policies, supporting over 4,000 flows per
second&lt;/a>.&lt;/li>
&lt;li>Global cybersecurity company Ericom’s solutions depend on hyper-low latency and data
security. &lt;a href="https://www.cncf.io/case-studies/ericom/">With Ridge's managed Kubernetes service they were able to deploy, through a single API,
to a network of service providers worldwide&lt;/a>.&lt;/li>
&lt;li>Lunar, a Scandinavian online bank, wanted to implement quarterly production cluster failover
testing to prepare for disaster recovery, and needed a better way to managed their platform
services.&lt;a href="https://www.cncf.io/case-studies/lunar/">They started by centralizing their log management system, and followed-up with the
centralization of all platform services, using Linkerd to connect the
clusters&lt;/a>.&lt;/li>
&lt;li>Datadog runs 10s of clusters with 10,000+ of nodes and 100,000+ pods across multiple cloud
providers.&lt;a href="https://www.cncf.io/case-studies/datadog/">They turned to Cilium as their CNI and kube-proxy replacement to take advantage of the
power of eBPF and provide a consistent networking experience for their users across any
cloud&lt;/a>.&lt;/li>
&lt;li>Insiel wanted to update their software production methods and introduce a cloud native paradigm in
their software production. &lt;a href="https://www.cncf.io/case-studies/insiel/">Their digital transformation project with Kiratech and Microsoft Azure
allowed them to develop a cloud-first culture&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem updates&lt;/h3>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon Europe 2023 will take place in Amsterdam, The Netherlands, from 17 – 21
April 2023! You can find more information about the conference and registration on the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">event
site&lt;/a>.&lt;/li>
&lt;li>CloudNativeSecurityCon North America, a two-day event designed to foster collaboration, discussion
and knowledge sharing of cloud native security projects and how to best use these to address
security challenges and opportunities, will take place in Seattle, Washington (USA), from 1-2
February 2023. See the &lt;a href="https://events.linuxfoundation.org/cloudnativesecuritycon-north-america/">event
page&lt;/a> for more
information.&lt;/li>
&lt;li>The CNCF announced &lt;a href="https://www.cncf.io/announcements/2022/10/28/cloud-native-computing-foundation-reveals-2022-community-awards-winners/">the 2022 Community Awards
Winners&lt;/a>:
the Community Awards recognize CNCF community members that are going above and beyond to advance
cloud native technology.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project velocity&lt;/h3>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> project
aggregates a number of interesting data points related to the velocity of Kubernetes and various
sub-projects. This includes everything from individual contributions to the number of companies that
are contributing, and is an illustration of the depth and breadth of effort that goes into evolving
this ecosystem.&lt;/p>
&lt;p>In the v1.26 release cycle, which &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.26">ran for 14 weeks&lt;/a>
(September 5 to December 9), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.25.0%20-%20v1.26.0&amp;amp;var-metric=contributions">976 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.25.0%20-%20v1.26.0&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">6877 individuals&lt;/a>.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h2>
&lt;p>Join members of the Kubernetes v1.26 release team on Tuesday January 17, 2023 10am - 11am EST (3pm - 4pm UTC) to learn about the major features
of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v126-release/">event
page&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest
Groups&lt;/a> (SIGs) that align with your
interests.&lt;/p>
&lt;p>Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly
&lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through
the channels below:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes
Contributors&lt;/a> website&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for the latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="https://serverfault.com/questions/tagged/kubernetes">Server
Fault&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">Share&lt;/a> your Kubernetes story&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release
Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Forensic container checkpointing in Kubernetes</title><link>https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Adrian Reber (Red Hat)&lt;/p>
&lt;p>Forensic container checkpointing is based on &lt;a href="https://criu.org/">Checkpoint/Restore In
Userspace&lt;/a> (CRIU) and allows the creation of stateful copies
of a running container without the container knowing that it is being
checkpointed. The copy of the container can be analyzed and restored in a
sandbox environment multiple times without the original container being aware
of it. Forensic container checkpointing was introduced as an alpha feature in
Kubernetes v1.25.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>With the help of CRIU it is possible to checkpoint and restore containers.
CRIU is integrated in runc, crun, CRI-O and containerd and forensic container
checkpointing as implemented in Kubernetes uses these existing CRIU
integrations.&lt;/p>
&lt;h2 id="why-is-it-important">Why is it important?&lt;/h2>
&lt;p>With the help of CRIU and the corresponding integrations it is possible to get
all information and state about a running container on disk for later forensic
analysis. Forensic analysis might be important to inspect a suspicious
container without stopping or influencing it. If the container is really under
attack, the attacker might detect attempts to inspect the container. Taking a
checkpoint and analysing the container in a sandboxed environment offers the
possibility to inspect the container without the original container and maybe
attacker being aware of the inspection.&lt;/p>
&lt;p>In addition to the forensic container checkpointing use case, it is also
possible to migrate a container from one node to another node without loosing
the internal state. Especially for stateful containers with long initialization
times restoring from a checkpoint might save time after a reboot or enable much
faster startup times.&lt;/p>
&lt;h2 id="how-do-i-use-container-checkpointing">How do I use container checkpointing?&lt;/h2>
&lt;p>The feature is behind a &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>, so
make sure to enable the &lt;code>ContainerCheckpoint&lt;/code> gate before you can use the new
feature.&lt;/p>
&lt;p>The runtime must also support container checkpointing:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>containerd: support is currently under discussion. See containerd
pull request &lt;a href="https://github.com/containerd/containerd/pull/6965">#6965&lt;/a> for more details.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CRI-O: v1.25 has support for forensic container checkpointing.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="usage-example-with-cri-o">Usage example with CRI-O&lt;/h3>
&lt;p>To use forensic container checkpointing in combination with CRI-O, the runtime
needs to be started with the command-line option &lt;code>--enable-criu-support=true&lt;/code>.
For Kubernetes, you need to run your cluster with the &lt;code>ContainerCheckpoint&lt;/code>
feature gate enabled. As the checkpointing functionality is provided by CRIU it
is also necessary to install CRIU. Usually runc or crun depend on CRIU and
therefore it is installed automatically.&lt;/p>
&lt;p>It is also important to mention that at the time of writing the checkpointing functionality is
to be considered as an alpha level feature in CRI-O and Kubernetes and the
security implications are still under consideration.&lt;/p>
&lt;p>Once containers and pods are running it is possible to create a checkpoint.
&lt;a href="https://kubernetes.io/docs/reference/node/kubelet-checkpoint-api/">Checkpointing&lt;/a>
is currently only exposed on the &lt;strong>kubelet&lt;/strong> level. To checkpoint a container,
you can run &lt;code>curl&lt;/code> on the node where that container is running, and trigger a
checkpoint:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>curl -X POST &lt;span style="color:#b44">&amp;#34;https://localhost:10250/checkpoint/namespace/podId/container&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For a container named &lt;em>counter&lt;/em> in a pod named &lt;em>counters&lt;/em> in a namespace named
&lt;em>default&lt;/em> the &lt;strong>kubelet&lt;/strong> API endpoint is reachable at:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>curl -X POST &lt;span style="color:#b44">&amp;#34;https://localhost:10250/checkpoint/default/counters/counter&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For completeness the following &lt;code>curl&lt;/code> command-line options are necessary to
have &lt;code>curl&lt;/code> accept the &lt;em>kubelet&lt;/em>'s self signed certificate and authorize the
use of the &lt;em>kubelet&lt;/em> &lt;code>checkpoint&lt;/code> API:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>--insecure --cert /var/run/kubernetes/client-admin.crt --key /var/run/kubernetes/client-admin.key
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Triggering this &lt;strong>kubelet&lt;/strong> API will request the creation of a checkpoint from
CRI-O. CRI-O requests a checkpoint from your low-level runtime (for example,
&lt;code>runc&lt;/code>). Seeing that request, &lt;code>runc&lt;/code> invokes the &lt;code>criu&lt;/code> tool
to do the actual checkpointing.&lt;/p>
&lt;p>Once the checkpointing has finished the checkpoint should be available at
&lt;code>/var/lib/kubelet/checkpoints/checkpoint-&amp;lt;pod-name&amp;gt;_&amp;lt;namespace-name&amp;gt;-&amp;lt;container-name&amp;gt;-&amp;lt;timestamp&amp;gt;.tar&lt;/code>&lt;/p>
&lt;p>You could then use that tar archive to restore the container somewhere else.&lt;/p>
&lt;h3 id="restore-checkpointed-container-standalone">Restore a checkpointed container outside of Kubernetes (with CRI-O)&lt;/h3>
&lt;p>With the checkpoint tar archive it is possible to restore the container outside
of Kubernetes in a sandboxed instance of CRI-O. For better user experience
during restore, I recommend that you use the latest version of CRI-O from the
&lt;em>main&lt;/em> CRI-O GitHub branch. If you're using CRI-O v1.25, you'll need to
manually create certain directories Kubernetes would create before starting the
container.&lt;/p>
&lt;p>The first step to restore a container outside of Kubernetes is to create a pod sandbox
using &lt;em>crictl&lt;/em>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>crictl runp pod-config.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then you can restore the previously checkpointed container into the newly created pod sandbox:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>crictl create &amp;lt;POD_ID&amp;gt; container-config.json pod-config.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Instead of specifying a container image in a registry in &lt;code>container-config.json&lt;/code>
you need to specify the path to the checkpoint archive that you created earlier:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;counter&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>:{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;/var/lib/kubelet/checkpoints/&amp;lt;checkpoint-archive&amp;gt;.tar&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, run &lt;code>crictl start &amp;lt;CONTAINER_ID&amp;gt;&lt;/code> to start that container, and then a
copy of the previously checkpointed container should be running.&lt;/p>
&lt;h3 id="restore-checkpointed-container-k8s">Restore a checkpointed container within of Kubernetes&lt;/h3>
&lt;p>To restore the previously checkpointed container directly in Kubernetes it is
necessary to convert the checkpoint archive into an image that can be pushed to
a registry.&lt;/p>
&lt;p>One possible way to convert the local checkpoint archive consists of the
following steps with the help of &lt;a href="https://buildah.io/">buildah&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b8860b">newcontainer&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>buildah from scratch&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>buildah add &lt;span style="color:#b8860b">$newcontainer&lt;/span> /var/lib/kubelet/checkpoints/checkpoint-&amp;lt;pod-name&amp;gt;_&amp;lt;namespace-name&amp;gt;-&amp;lt;container-name&amp;gt;-&amp;lt;timestamp&amp;gt;.tar /
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>buildah config --annotation&lt;span style="color:#666">=&lt;/span>io.kubernetes.cri-o.annotations.checkpoint.name&lt;span style="color:#666">=&lt;/span>&amp;lt;container-name&amp;gt; &lt;span style="color:#b8860b">$newcontainer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>buildah commit &lt;span style="color:#b8860b">$newcontainer&lt;/span> checkpoint-image:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>buildah rm &lt;span style="color:#b8860b">$newcontainer&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The resulting image is not standardized and only works in combination with
CRI-O. Please consider this image format as pre-alpha. There are ongoing
&lt;a href="https://github.com/opencontainers/image-spec/issues/962">discussions&lt;/a> to standardize the format of checkpoint
images like this. Important to remember is that this not yet standardized image
format only works if CRI-O has been started with &lt;code>--enable-criu-support=true&lt;/code>.
The security implications of starting CRI-O with CRIU support are not yet clear
and therefore the functionality as well as the image format should be used with
care.&lt;/p>
&lt;p>Now, you'll need to push that image to a container image registry. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>buildah push localhost/checkpoint-image:latest container-image-registry.example/user/checkpoint-image:latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To restore this checkpoint image (&lt;code>container-image-registry.example/user/checkpoint-image:latest&lt;/code>), the
image needs to be listed in the specification for a Pod. Here's an example
manifest:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namePrefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&amp;lt;container-name&amp;gt;&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>container-image-registry.example/user/checkpoint-image:latest&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&amp;lt;destination-node&amp;gt;&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Kubernetes schedules the new Pod onto a node. The kubelet on that node
instructs the container runtime (CRI-O in this example) to create and start a
container based on an image specified as &lt;code>registry/user/checkpoint-image:latest&lt;/code>.
CRI-O detects that &lt;code>registry/user/checkpoint-image:latest&lt;/code>
is a reference to checkpoint data rather than a container image. Then,
instead of the usual steps to create and start a container,
CRI-O fetches the checkpoint data and restores the container from that
specified checkpoint.&lt;/p>
&lt;p>The application in that Pod would continue running as if the checkpoint had not been taken;
within the container, the application looks and behaves like any other container that had been
started normally and not restored from a checkpoint.&lt;/p>
&lt;p>With these steps, it is possible to replace a Pod running on one node
with a new equivalent Pod that is running on a different node,
and without losing the state of the containers in that Pod.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>You can reach SIG Node by several means:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Finding suspicious syscalls with the seccomp notifier</title><link>https://kubernetes.io/blog/2022/12/02/seccomp-notifier/</link><pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/02/seccomp-notifier/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sascha Grunert&lt;/p>
&lt;p>Debugging software in production is one of the biggest challenges we have to
face in our containerized environments. Being able to understand the impact of
the available security options, especially when it comes to configuring our
deployments, is one of the key aspects to make the default security in
Kubernetes stronger. We have all those logging, tracing and metrics data already
at hand, but how do we assemble the information they provide into something
human readable and actionable?&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp&lt;/a> is one of the standard mechanisms to protect a Linux based
Kubernetes application from malicious actions by interfering with its &lt;a href="https://en.wikipedia.org/wiki/Syscall">system
calls&lt;/a>. This allows us to restrict the application to a defined set of
actionable items, like modifying files or responding to HTTP requests. Linking
the knowledge of which set of syscalls is required to, for example, modify a
local file, to the actual source code is in the same way non-trivial. Seccomp
profiles for Kubernetes have to be written in &lt;a href="https://www.json.org">JSON&lt;/a> and can be understood
as an architecture specific allow-list with superpowers, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultAction&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;SCMP_ACT_ERRNO&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultErrnoRet&amp;#34;&lt;/span>: &lt;span style="color:#666">38&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultErrno&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ENOSYS&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;syscalls&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;names&amp;#34;&lt;/span>: [&lt;span style="color:#b44">&amp;#34;chmod&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;chown&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;open&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;write&amp;#34;&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;action&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;SCMP_ACT_ALLOW&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above profile errors by default specifying the &lt;code>defaultAction&lt;/code> of
&lt;code>SCMP_ACT_ERRNO&lt;/code>. This means we have to allow a set of syscalls via
&lt;code>SCMP_ACT_ALLOW&lt;/code>, otherwise the application would not be able to do anything at
all. Okay cool, for being able to allow file operations, all we have to do is
adding a bunch of file specific syscalls like &lt;code>open&lt;/code> or &lt;code>write&lt;/code>, and probably
also being able to change the permissions via &lt;code>chmod&lt;/code> and &lt;code>chown&lt;/code>, right?
Basically yes, but there are issues with the simplicity of that approach:&lt;/p>
&lt;p>Seccomp profiles need to include the minimum set of syscalls required to start
the application. This also includes some syscalls from the lower level
&lt;a href="https://opencontainers.org">Open Container Initiative (OCI)&lt;/a> container runtime, for example
&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> or &lt;a href="https://github.com/containers/crun">crun&lt;/a>. Beside that, we can only guarantee the required
syscalls for a very specific version of the runtimes and our application,
because the code parts can change between releases. The same applies to the
termination of the application as well as the target architecture we're
deploying on. Features like executing commands within containers also require
another subset of syscalls. Not to mention that there are multiple versions for
syscalls doing slightly different things and the seccomp profiles are able to
modify their arguments. It's also not always clearly visible to the developers
which syscalls are used by their own written code parts, because they rely on
programming language abstractions or frameworks.&lt;/p>
&lt;p>&lt;em>How can we know which syscalls are even required then? Who should create and
maintain those profiles during its development life-cycle?&lt;/em>&lt;/p>
&lt;p>Well, recording and distributing seccomp profiles is one of the problem domains
of the &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator">Security Profiles Operator&lt;/a>, which is already solving that. The
operator is able to record &lt;a href="https://en.wikipedia.org/wiki/Seccomp">seccomp&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">SELinux&lt;/a> and even
&lt;a href="https://en.wikipedia.org/wiki/AppArmor">AppArmor&lt;/a> profiles into a &lt;a href="https://k8s.io/docs/concepts/extend-kubernetes/api-extension/custom-resources">Custom Resource Definition (CRD)&lt;/a>,
reconciles them to each node and makes them available for usage.&lt;/p>
&lt;p>The biggest challenge about creating security profiles is to catch all code
paths which execute syscalls. We could achieve that by having &lt;strong>100%&lt;/strong> logical
coverage of the application when running an end-to-end test suite. You get the
problem with the previous statement: It's too idealistic to be ever fulfilled,
even without taking all the moving parts during application development and
deployment into account.&lt;/p>
&lt;p>Missing a syscall in the seccomp profiles' allow list can have tremendously
negative impact on the application. It's not only that we can encounter crashes,
which are trivially detectable. It can also happen that they slightly change
logical paths, change the business logic, make parts of the application
unusable, slow down performance or even expose security vulnerabilities. We're
simply not able to see the whole impact of that, especially because blocked
syscalls via &lt;code>SCMP_ACT_ERRNO&lt;/code> do not provide any additional &lt;a href="https://linux.die.net/man/8/auditd">audit&lt;/a>
logging on the system.&lt;/p>
&lt;p>Does that mean we're lost? Is it just not realistic to dream about a Kubernetes
where &lt;a href="https://github.com/kubernetes/enhancements/issues/2413">everyone uses the default seccomp profile&lt;/a>? Should we
stop striving towards maximum security in Kubernetes and accept that it's not
meant to be secure by default?&lt;/p>
&lt;p>&lt;strong>Definitely not.&lt;/strong> Technology evolves over time and there are many folks
working behind the scenes of Kubernetes to indirectly deliver features to
address such problems. One of the mentioned features is the &lt;em>seccomp notifier&lt;/em>,
which can be used to find suspicious syscalls in Kubernetes.&lt;/p>
&lt;p>The seccomp notify feature consists of a set of changes introduced in Linux 5.9.
It makes the kernel capable of communicating seccomp related events to the user
space. That allows applications to act based on the syscalls and opens for a
wide range of possible use cases. We not only need the right kernel version,
but also at least runc v1.1.0 (or crun v0.19) to be able to make the notifier
work at all. The Kubernetes container runtime &lt;a href="https://cri-o.io">CRI-O&lt;/a> gets &lt;a href="https://github.com/cri-o/cri-o/pull/6120">support for
the seccomp notifier in v1.26.0&lt;/a>. The new feature allows us to
identify possibly malicious syscalls in our application, and therefore makes it
possible to verify profiles for consistency and completeness. Let's give that a
try.&lt;/p>
&lt;p>First of all we need to run the latest &lt;code>main&lt;/code> version of CRI-O, because v1.26.0
has not been released yet at time of writing. You can do that by either
compiling it from the &lt;a href="https://github.com/cri-o/cri-o/blob/main/install.md#build-and-install-cri-o-from-source">source code&lt;/a> or by using the pre-built binary
bundle via &lt;a href="https://github.com/cri-o/cri-o#installing-cri-o">the get-script&lt;/a>. The seccomp notifier feature of CRI-O is
guarded by an annotation, which has to be explicitly allowed, for example by
using a configuration drop-in like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> cat /etc/crio/crio.conf.d/02-runtimes.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[crio.runtime]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default_runtime = &lt;span style="color:#b44">&amp;#34;runc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[crio.runtime.runtimes.runc]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>allowed_annotations = [ &lt;span style="color:#b44">&amp;#34;io.kubernetes.cri-o.seccompNotifierAction&amp;#34;&lt;/span> ]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If CRI-O is up and running, then it should indicate that the seccomp notifier is
available as well:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> sudo ./bin/crio --enable-metrics
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">INFO[…] Starting seccomp notifier watcher
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">INFO[…] Serving metrics on :9090 via HTTP
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We also enable the metrics, because they provide additional telemetry data about
the notifier. Now we need a running Kubernetes cluster for demonstration
purposes. For this demo, we mainly stick to the
&lt;a href="https://github.com/cri-o/cri-o#running-kubernetes-with-cri-o">&lt;code>hack/local-up-cluster.sh&lt;/code>&lt;/a> approach to locally spawn a single node
Kubernetes cluster.&lt;/p>
&lt;p>If everything is up and running, then we would have to define a seccomp profile
for testing purposes. But we do not have to create our own, we can just use the
&lt;code>RuntimeDefault&lt;/code> profile which gets shipped with each container runtime. For
example the &lt;code>RuntimeDefault&lt;/code> profile for CRI-O can be found in the
&lt;a href="https://github.com/containers/common/blob/afff1d6/pkg/seccomp/seccomp.json">containers/common&lt;/a> library.&lt;/p>
&lt;p>Now we need a test container, which can be a simple &lt;a href="https://www.nginx.com">nginx&lt;/a> pod like
this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">io.kubernetes.cri-o.seccompNotifierAction&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;stop&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx:1.23.2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">seccompProfile&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>RuntimeDefault&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note the annotation &lt;code>io.kubernetes.cri-o.seccompNotifierAction&lt;/code>, which
enables the seccomp notifier for this workload. The value of the annotation can
be either &lt;code>stop&lt;/code> for stopping the workload or anything else for doing nothing
else than logging and throwing metrics. Because of the termination we also use
the &lt;code>restartPolicy: Never&lt;/code> to not automatically recreate the container on
failure.&lt;/p>
&lt;p>Let's run the pod and check if it works:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl apply -f nginx.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl get pods -o wide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">nginx 1/1 Running 0 3m39s 10.85.0.3 127.0.0.1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can also test if the web server itself works as intended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> curl 10.85.0.3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;!DOCTYPE html&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;html&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;head&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While everything is now up and running, CRI-O also indicates that it has started
the seccomp notifier:&lt;/p>
&lt;pre tabindex="0">&lt;code>…
INFO[…] Injecting seccomp notifier into seccomp profile of container 662a3bb0fdc7dd1bf5a88a8aa8ef9eba6296b593146d988b4a9b85822422febb
…
&lt;/code>&lt;/pre>&lt;p>If we would now run a forbidden syscall inside of the container, then we can
expect that the workload gets terminated. Let's give that a try by running
&lt;code>chroot&lt;/code> in the containers namespaces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- bash
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">root@nginx:/# chroot /tmp
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">chroot: cannot change root directory to &amp;#39;/tmp&amp;#39;: Function not implemented
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">root@nginx:/# command terminated with exit code 137
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The exec session got terminated, so it looks like the container is not running
any more:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">nginx 0/1 seccomp killed 0 96s
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alright, the container got killed by seccomp, do we get any more information
about what was going on?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl describe pod nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Name: nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Containers:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> nginx:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> …
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> State: Terminated
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Reason: seccomp killed
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Message: Used forbidden syscalls: chroot (1x)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Exit Code: 137
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Started: Mon, 14 Nov 2022 12:19:46 +0100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Finished: Mon, 14 Nov 2022 12:20:26 +0100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The seccomp notifier feature of CRI-O correctly set the termination reason and
message, including which forbidden syscall has been used how often (&lt;code>1x&lt;/code>). How
often? Yes, the notifier gives the application up to 5 seconds after the last
seen syscall until it starts the termination. This means that it's possible to
catch multiple forbidden syscalls within one test by avoiding time-consuming
trial and errors.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- chroot /tmp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">chroot: cannot change root directory to &amp;#39;/tmp&amp;#39;: Function not implemented
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 125
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- chroot /tmp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">chroot: cannot change root directory to &amp;#39;/tmp&amp;#39;: Function not implemented
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 125
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- swapoff -a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 32
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- swapoff -a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 32
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl describe pod nginx | grep Message
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Message: Used forbidden syscalls: chroot (2x), swapoff (2x)
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The CRI-O metrics will also reflect that:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> curl -sf localhost:9090/metrics | grep seccomp_notifier
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">#&lt;/span> HELP container_runtime_crio_containers_seccomp_notifier_count_total Amount of containers stopped because they used a forbidden syscalls by their name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">#&lt;/span> TYPE container_runtime_crio_containers_seccomp_notifier_count_total counter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">container_runtime_crio_containers_seccomp_notifier_count_total{name=&amp;#34;…&amp;#34;,syscalls=&amp;#34;chroot (1x)&amp;#34;} 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">container_runtime_crio_containers_seccomp_notifier_count_total{name=&amp;#34;…&amp;#34;,syscalls=&amp;#34;chroot (2x), swapoff (2x)&amp;#34;} 1
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>How does it work in detail? CRI-O uses the chosen seccomp profile and injects
the action &lt;code>SCMP_ACT_NOTIFY&lt;/code> instead of &lt;code>SCMP_ACT_ERRNO&lt;/code>, &lt;code>SCMP_ACT_KILL&lt;/code>,
&lt;code>SCMP_ACT_KILL_PROCESS&lt;/code> or &lt;code>SCMP_ACT_KILL_THREAD&lt;/code>. It also sets a local listener
path which will be used by the lower level OCI runtime (runc or crun) to create
the seccomp notifier socket. If the connection between the socket and CRI-O has
been established, then CRI-O will receive notifications for each syscall being
interfered by seccomp. CRI-O stores the syscalls, allows a bit of timeout for
them to arrive and then terminates the container if the chosen
&lt;code>seccompNotifierAction=stop&lt;/code>. Unfortunately, the seccomp notifier is not able to
notify on the &lt;code>defaultAction&lt;/code>, which means that it's required to have
a list of syscalls to test for custom profiles. CRI-O does also state that
limitation in the logs:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-log" data-lang="log">INFO[…] The seccomp profile default action SCMP_ACT_ERRNO cannot be overridden to SCMP_ACT_NOTIFY,
which means that syscalls using that default action can&amp;#39;t be traced by the notifier
&lt;/code>&lt;/pre>&lt;p>As a conclusion, the seccomp notifier implementation in CRI-O can be used to
verify if your applications behave correctly when using &lt;code>RuntimeDefault&lt;/code> or any
other custom profile. Alerts can be created based on the metrics to create long
running test scenarios around that feature. Making seccomp understandable and
easier to use will increase adoption as well as help us to move towards a more
secure Kubernetes by default!&lt;/p>
&lt;p>Thank you for reading this blog post. If you'd like to read more about the
seccomp notifier, checkout the following resources:&lt;/p>
&lt;ul>
&lt;li>The Seccomp Notifier - New Frontiers in Unprivileged Container Development: &lt;a href="https://brauner.io/2020/07/23/seccomp-notify.html">https://brauner.io/2020/07/23/seccomp-notify.html&lt;/a>&lt;/li>
&lt;li>Bringing Seccomp Notify to Runc and Kubernetes: &lt;a href="https://kinvolk.io/blog/2022/03/bringing-seccomp-notify-to-runc-and-kubernetes">https://kinvolk.io/blog/2022/03/bringing-seccomp-notify-to-runc-and-kubernetes&lt;/a>&lt;/li>
&lt;li>Seccomp Agent reference implementation: &lt;a href="https://github.com/opencontainers/runc/tree/6b16d00/contrib/cmd/seccompagent">https://github.com/opencontainers/runc/tree/6b16d00/contrib/cmd/seccompagent&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Boosting Kubernetes container runtime observability with OpenTelemetry</title><link>https://kubernetes.io/blog/2022/12/01/runtime-observability-opentelemetry/</link><pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/01/runtime-observability-opentelemetry/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sascha Grunert&lt;/p>
&lt;p>When speaking about observability in the cloud native space, then probably
everyone will mention &lt;a href="https://opentelemetry.io">OpenTelemetry (OTEL)&lt;/a> at some point in the
conversation. That's great, because the community needs standards to rely on
for developing all cluster components into the same direction. OpenTelemetry
enables us to combine logs, metrics, traces and other contextual information
(called baggage) into a single resource. Cluster administrators or software
engineers can use this resource to get a viewport about what is going on in the
cluster over a defined period of time. But how can Kubernetes itself make use of
this technology stack?&lt;/p>
&lt;p>Kubernetes consists of multiple components where some are independent and others
are stacked together. Looking at the architecture from a container runtime
perspective, then there are from the top to the bottom:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>kube-apiserver&lt;/strong>: Validates and configures data for the API objects&lt;/li>
&lt;li>&lt;strong>kubelet&lt;/strong>: Agent running on each node&lt;/li>
&lt;li>&lt;strong>CRI runtime&lt;/strong>: Container Runtime Interface (CRI) compatible container runtime
like &lt;a href="https://cri-o.io">CRI-O&lt;/a> or &lt;a href="https://containerd.io">containerd&lt;/a>&lt;/li>
&lt;li>&lt;strong>OCI runtime&lt;/strong>: Lower level &lt;a href="https://opencontainers.org">Open Container Initiative (OCI)&lt;/a> runtime
like &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> or &lt;a href="https://github.com/containers/crun">crun&lt;/a>&lt;/li>
&lt;li>&lt;strong>Linux kernel&lt;/strong> or &lt;strong>Microsoft Windows&lt;/strong>: Underlying operating system&lt;/li>
&lt;/ul>
&lt;p>That means if we encounter a problem with running containers in Kubernetes, then
we start looking at one of those components. Finding the root cause for problems
is one of the most time consuming actions we face with the increased
architectural complexity from today's cluster setups. Even if we know the
component which seems to cause the issue, we still have to take the others into
account to maintain a mental timeline of events which are going on. How do we
achieve that? Well, most folks will probably stick to scraping logs, filtering
them and assembling them together over the components borders. We also have
metrics, right? Correct, but bringing metrics values in correlation with plain
logs makes it even harder to track what is going on. Some metrics are also not
made for debugging purposes. They have been defined based on the end user
perspective of the cluster for linking usable alerts and not for developers
debugging a cluster setup.&lt;/p>
&lt;p>OpenTelemetry to the rescue: the project aims to combine signals such as
&lt;a href="https://opentelemetry.io/docs/concepts/signals/traces">traces&lt;/a>, &lt;a href="https://opentelemetry.io/docs/concepts/signals/metrics">metrics&lt;/a> and &lt;a href="https://opentelemetry.io/docs/concepts/signals/logs">logs&lt;/a> together to maintain the
right viewport on the cluster state.&lt;/p>
&lt;p>What is the current state of OpenTelemetry tracing in Kubernetes? From an API
server perspective, we have alpha support for tracing since Kubernetes v1.22,
which will graduate to beta in one of the upcoming releases. Unfortunately the
beta graduation has missed the v1.26 Kubernetes release. The design proposal can
be found in the &lt;a href="https://github.com/kubernetes/enhancements/issues/647">&lt;em>API Server Tracing&lt;/em> Kubernetes Enhancement Proposal
(KEP)&lt;/a> which provides more information about it.&lt;/p>
&lt;p>The kubelet tracing part is tracked &lt;a href="https://github.com/kubernetes/enhancements/issues/2831">in another KEP&lt;/a>, which was
implemented in an alpha state in Kubernetes v1.25. A beta graduation is not
planned as time of writing, but more may come in the v1.27 release cycle.
There are other side-efforts going on beside both KEPs, for example &lt;a href="https://github.com/kubernetes/klog/issues/356">klog is
considering OTEL support&lt;/a>, which would boost the observability by
linking log messages to existing traces. Within SIG Instrumentation and SIG Node,
we're also discussing &lt;a href="https://github.com/kubernetes/kubernetes/issues/113414">how to link the
kubelet traces together&lt;/a>, because right now they're focused on the
&lt;a href="https://grpc.io">gRPC&lt;/a> calls between the kubelet and the CRI container runtime.&lt;/p>
&lt;p>CRI-O features OpenTelemetry tracing support &lt;a href="https://github.com/cri-o/cri-o/pull/4883">since v1.23.0&lt;/a> and is
working on continuously improving them, for example by &lt;a href="https://github.com/cri-o/cri-o/pull/6294">attaching the logs to the
traces&lt;/a> or extending the &lt;a href="https://github.com/cri-o/cri-o/pull/6343">spans to logical parts of the
application&lt;/a>. This helps users of the traces to gain the same
information like parsing the logs, but with enhanced capabilities of scoping and
filtering to other OTEL signals. The CRI-O maintainers are also working on a
container monitoring replacement for &lt;a href="https://github.com/containers/conmon">conmon&lt;/a>, which is called
&lt;a href="https://github.com/containers/conmon-rs">conmon-rs&lt;/a> and is purely written in &lt;a href="https://www.rust-lang.org">Rust&lt;/a>. One benefit of
having a Rust implementation is to be able to add features like OpenTelemetry
support, because the crates (libraries) for those already exist. This allows a
tight integration with CRI-O and lets consumers see the most low level tracing
data from their containers.&lt;/p>
&lt;p>The &lt;a href="https://containerd.io">containerd&lt;/a> folks added tracing support since v1.6.0, which is
available &lt;a href="https://github.com/containerd/containerd/blob/7def13d/docs/tracing.md">by using a plugin&lt;/a>. Lower level OCI runtimes like
&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> or &lt;a href="https://github.com/containers/crun">crun&lt;/a> feature no support for OTEL at all and it does not
seem to exist a plan for that. We always have to consider that there is a
performance overhead when collecting the traces as well as exporting them to a
data sink. I still think it would be worth an evaluation on how extended
telemetry collection could look like in OCI runtimes. Let's see if the Rust OCI
runtime &lt;a href="https://github.com/containers/youki/issues/1348">youki&lt;/a> is considering something like that in the future.&lt;/p>
&lt;p>I'll show you how to give it a try. For my demo I'll stick to a stack with a single local node
that has runc, conmon-rs, CRI-O, and a kubelet. To enable tracing in the kubelet, I need to
apply the following &lt;code>KubeletConfiguration&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet.config.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeletConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">featureGates&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">KubeletTracing&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">tracing&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">samplingRatePerMillion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A &lt;code>samplingRatePerMillion&lt;/code> equally to one million will internally translate to
sampling everything. A similar configuration has to be applied to CRI-O; I can
either start the &lt;code>crio&lt;/code> binary with &lt;code>--enable-tracing&lt;/code> and
&lt;code>--tracing-sampling-rate-per-million 1000000&lt;/code> or we use a drop-in configuration
like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat /etc/crio/crio.conf.d/99-tracing.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[crio.tracing]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>enable_tracing = &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tracing_sampling_rate_per_million = &lt;span style="color:#666">1000000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To configure CRI-O to use conmon-rs, you require at least the latest CRI-O
v1.25.x and conmon-rs v0.4.0. Then a configuration drop-in like this can be used
to make CRI-O use conmon-rs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat /etc/crio/crio.conf.d/99-runtimes.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[crio.runtime]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default_runtime = &lt;span style="color:#b44">&amp;#34;runc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[crio.runtime.runtimes.runc]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>runtime_type = &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>monitor_path = &lt;span style="color:#b44">&amp;#34;/path/to/conmonrs&amp;#34;&lt;/span> &lt;span style="color:#080;font-style:italic"># or will be looked up in $PATH&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That's it, the default configuration will point to an &lt;a href="https://opentelemetry.io/docs/collector/getting-started">OpenTelemetry
collector&lt;/a> &lt;a href="https://grpc.io">gRPC&lt;/a> endpoint of &lt;code>localhost:4317&lt;/code>, which has to be up and
running as well. There are multiple ways to run OTLP as &lt;a href="https://opentelemetry.io/docs/collector/getting-started">described in the
docs&lt;/a>, but it's also possible to &lt;code>kubectl proxy&lt;/code> into an existing
instance running within Kubernetes.&lt;/p>
&lt;p>If everything is set up, then the collector should log that there are incoming
traces:&lt;/p>
&lt;pre tabindex="0">&lt;code>ScopeSpans #0
ScopeSpans SchemaURL:
InstrumentationScope go.opentelemetry.io/otel/sdk/tracer
Span #0
Trace ID : 71896e69f7d337730dfedb6356e74f01
Parent ID : a2a7714534c017e6
ID : 1d27dbaf38b9da8b
Name : github.com/cri-o/cri-o/server.(*Server).filterSandboxList
Kind : SPAN_KIND_INTERNAL
Start time : 2022-11-15 09:50:20.060325562 +0000 UTC
End time : 2022-11-15 09:50:20.060326291 +0000 UTC
Status code : STATUS_CODE_UNSET
Status message :
Span #1
Trace ID : 71896e69f7d337730dfedb6356e74f01
Parent ID : a837a005d4389579
ID : a2a7714534c017e6
Name : github.com/cri-o/cri-o/server.(*Server).ListPodSandbox
Kind : SPAN_KIND_INTERNAL
Start time : 2022-11-15 09:50:20.060321973 +0000 UTC
End time : 2022-11-15 09:50:20.060330602 +0000 UTC
Status code : STATUS_CODE_UNSET
Status message :
Span #2
Trace ID : fae6742709d51a9b6606b6cb9f381b96
Parent ID : 3755d12b32610516
ID : 0492afd26519b4b0
Name : github.com/cri-o/cri-o/server.(*Server).filterContainerList
Kind : SPAN_KIND_INTERNAL
Start time : 2022-11-15 09:50:20.0607746 +0000 UTC
End time : 2022-11-15 09:50:20.060795505 +0000 UTC
Status code : STATUS_CODE_UNSET
Status message :
Events:
SpanEvent #0
-&amp;gt; Name: log
-&amp;gt; Timestamp: 2022-11-15 09:50:20.060778668 +0000 UTC
-&amp;gt; DroppedAttributesCount: 0
-&amp;gt; Attributes::
-&amp;gt; id: Str(adf791e5-2eb8-4425-b092-f217923fef93)
-&amp;gt; log.message: Str(No filters were applied, returning full container list)
-&amp;gt; log.severity: Str(DEBUG)
-&amp;gt; name: Str(/runtime.v1.RuntimeService/ListContainers)
&lt;/code>&lt;/pre>&lt;p>I can see that the spans have a trace ID and typically have a parent attached.
Events such as logs are part of the output as well. In the above case, the kubelet is
periodically triggering a &lt;code>ListPodSandbox&lt;/code> RPC to CRI-O caused by the Pod
Lifecycle Event Generator (PLEG). Displaying those traces can be done via,
for example, &lt;a href="https://www.jaegertracing.io/">Jaeger&lt;/a>. When running the tracing stack locally, then a Jaeger
instance should be exposed on &lt;code>http://localhost:16686&lt;/code> per default.&lt;/p>
&lt;p>The &lt;code>ListPodSandbox&lt;/code> requests are directly visible within the Jaeger UI:&lt;/p>
&lt;p>&lt;img src="list_pod_sandbox.png" alt="ListPodSandbox RPC in the Jaeger UI">&lt;/p>
&lt;p>That's not too exciting, so I'll run a workload directly via &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl run -it --rm --restart&lt;span style="color:#666">=&lt;/span>Never --image&lt;span style="color:#666">=&lt;/span>alpine alpine -- &lt;span style="color:#a2f">echo&lt;/span> hi
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code>hi
pod &amp;#34;alpine&amp;#34; deleted
&lt;/code>&lt;/pre>&lt;p>Looking now at Jaeger, we can see that we have traces for &lt;code>conmonrs&lt;/code>, &lt;code>crio&lt;/code> as
well as the &lt;code>kubelet&lt;/code> for the &lt;code>RunPodSandbox&lt;/code> and &lt;code>CreateContainer&lt;/code> CRI RPCs:&lt;/p>
&lt;p>&lt;img src="create_container.png" alt="Container creation in the Jaeger UI">&lt;/p>
&lt;p>The kubelet and CRI-O spans are connected to each other to make investigation
easier. If we now take a closer look at the spans, then we can see that CRI-O's
logs are correctly accosted with the corresponding functionality. For example we
can extract the container user from the traces like this:&lt;/p>
&lt;p>&lt;img src="crio_spans.png" alt="CRI-O in the Jaeger UI">&lt;/p>
&lt;p>The lower level spans of conmon-rs are also part of this trace. For example
conmon-rs maintains an internal &lt;code>read_loop&lt;/code> for handling IO between the
container and the end user. The logs for reading and writing bytes are part of
the span. The same applies to the &lt;code>wait_for_exit_code&lt;/code> span, which tells us that
the container exited successfully with code &lt;code>0&lt;/code>:&lt;/p>
&lt;p>&lt;img src="conmonrs_spans.png" alt="conmon-rs in the Jaeger UI">&lt;/p>
&lt;p>Having all that information at hand side by side to the filtering capabilities
of Jaeger makes the whole stack a great solution for debugging container issues!
Mentioning the &amp;quot;whole stack&amp;quot; also shows the biggest downside of the overall
approach: Compared to parsing logs it adds a noticeable overhead on top of the
cluster setup. Users have to maintain a sink like &lt;a href="https://www.elastic.co">Elasticsearch&lt;/a> to
persist the data, expose the Jaeger UI and possibly take the performance
drawback into account. Anyways, it's still one of the best ways to increase the
observability aspect of Kubernetes.&lt;/p>
&lt;p>Thank you for reading this blog post, I'm pretty sure we're looking into a
bright future for OpenTelemetry support in Kubernetes to make troubleshooting
simpler.&lt;/p></description></item><item><title>Blog: registry.k8s.io: faster, cheaper and Generally Available (GA)</title><link>https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Adolfo García Veytia (Chainguard), Bob Killen (Google)&lt;/p>
&lt;p>Starting with Kubernetes 1.25, our container image registry has changed from k8s.gcr.io to &lt;a href="https://registry.k8s.io">registry.k8s.io&lt;/a>. This new registry spreads the load across multiple Cloud Providers &amp;amp; Regions, functioning as a sort of content delivery network (CDN) for Kubernetes container images. This change reduces the project’s reliance on a single entity and provides a faster download experience for a large number of users.&lt;/p>
&lt;h2 id="tl-dr-what-you-need-to-know-about-this-change">TL;DR: What you need to know about this change&lt;/h2>
&lt;ul>
&lt;li>Container images for Kubernetes releases from 1.25 onward are no longer published to k8s.gcr.io, only to registry.k8s.io.&lt;/li>
&lt;li>In the upcoming December patch releases, the new registry domain default will be backported to all branches still in support (1.22, 1.23, 1.24).&lt;/li>
&lt;li>If you run in a restricted environment and apply strict domain/IP address access policies limited to k8s.gcr.io, the &lt;strong>image pulls will not function&lt;/strong> after the migration to this new registry. For these users, the recommended method is to mirror the release images to a private registry.&lt;/li>
&lt;/ul>
&lt;p>If you’d like to know more about why we made this change, or some potential issues you might run into, keep reading.&lt;/p>
&lt;h2 id="why-has-kubernetes-changed-to-a-different-image-registry">Why has Kubernetes changed to a different image registry?&lt;/h2>
&lt;p>k8s.gcr.io is hosted on a custom &lt;a href="https://cloud.google.com/container-registry">Google Container Registry&lt;/a> (GCR) domain that was setup solely for the Kubernetes project. This has worked well since the inception of the project, and we thank Google for providing these resources, but today there are other cloud providers and vendors that would like to host images to provide a better experience for the people on their platforms. In addition to Google’s &lt;a href="https://www.cncf.io/google-cloud-recommits-3m-to-kubernetes/">renewed commitment to donate $3 million&lt;/a> to support the project's infrastructure, Amazon announced a matching donation during their Kubecon NA 2022 keynote in Detroit. This will provide a better experience for users (closer servers = faster downloads) and will reduce the egress bandwidth and costs from GCR at the same time. registry.k8s.io will spread the load between Google and Amazon, with other providers to follow in the future.&lt;/p>
&lt;h2 id="why-isn-t-there-a-stable-list-of-domains-ips-why-can-t-i-restrict-image-pulls">Why isn’t there a stable list of domains/IPs? Why can’t I restrict image pulls?&lt;/h2>
&lt;p>registry.k8s.io is a &lt;a href="https://github.com/kubernetes/registry.k8s.io/blob/main/cmd/archeio/docs/request-handling.md">secure blob redirector&lt;/a> that connects clients to the closest cloud provider. The nature of this change means that a client pulling an image could be redirected to any one of a large number of backends. We expect the set of backends to keep changing and will only increase as more and more cloud providers and vendors come on board to help mirror the release images.&lt;/p>
&lt;p>Restrictive control mechanisms like man-in-the-middle proxies or network policies that restrict access to a specific list of IPs/domains will break with this change. For these scenarios, we encourage you to mirror the release images to a local registry that you have strict control over.&lt;/p>
&lt;p>For more information on this policy, please see the &lt;a href="https://github.com/kubernetes/registry.k8s.io#stability">stability section of the registry.k8s.io documentation&lt;/a>.&lt;/p>
&lt;h2 id="what-kind-of-errors-will-i-see-how-will-i-know-if-i-m-still-using-the-old-address">What kind of errors will I see? How will I know if I’m still using the old address?&lt;/h2>
&lt;p>Errors may depend on what kind of container runtime you are using, and what endpoint you are routed to, but it should present as a container failing to be created with the warning &lt;code>FailedCreatePodSandBox&lt;/code>.&lt;/p>
&lt;p>Below is an example error message showing a proxied deployment failing to pull due to an unknown certificate:&lt;/p>
&lt;pre tabindex="0">&lt;code>FailedCreatePodSandBox: Failed to create pod sandbox: rpc error: code = Unknown desc = Error response from daemon: Head “https://us-west1-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.8”: x509: certificate signed by unknown authority
&lt;/code>&lt;/pre>&lt;h2 id="i-m-impacted-by-this-change-how-do-i-revert-to-the-old-registry-address">I’m impacted by this change, how do I revert to the old registry address?&lt;/h2>
&lt;p>If using the new registry domain name is not an option, you can revert to the old domain name for cluster versions less than 1.25. Keep in mind that, eventually, you will have to switch to the new registry, as new image tags will no longer be pushed to GCR.&lt;/p>
&lt;h3 id="reverting-the-registry-name-in-kubeadm">Reverting the registry name in kubeadm&lt;/h3>
&lt;p>The registry used by kubeadm to pull its images can be controlled by two methods:&lt;/p>
&lt;p>Setting the &lt;code>--image-repository&lt;/code> flag.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubeadm init --image-repository=k8s.gcr.io
&lt;/code>&lt;/pre>&lt;p>Or in &lt;a href="https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/">kubeadm config&lt;/a> &lt;code>ClusterConfiguration&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta3&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">imageRepository&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;k8s.gcr.io&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="reverting-the-registry-name-in-kubelet">Reverting the Registry Name in kubelet&lt;/h3>
&lt;p>The image used by kubelet for the pod sandbox (&lt;code>pause&lt;/code>) can be overridden by setting the &lt;code>--pod-infra-container-image&lt;/code> flag. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubelet --pod-infra-container-image=k8s.gcr.io/pause:3.5
&lt;/code>&lt;/pre>&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>&lt;strong>Change is hard&lt;/strong>, and evolving our image-serving platform is needed to ensure a sustainable future for the project. We strive to make things better for everyone using Kubernetes. Many contributors from all corners of our community have been working long and hard to ensure we are making the best decisions possible, executing plans, and doing our best to communicate those plans.&lt;/p>
&lt;p>Thanks to Aaron Crickenberger, Arnaud Meukam, Benjamin Elder, Caleb Woodbine, Davanum Srinivas, Mahamed Ali, and Tim Hockin from SIG K8s Infra, Brian McQueen, and Sergey Kanzhelev from SIG Node, Lubomir Ivanov from SIG Cluster Lifecycle, Adolfo García Veytia, Jeremy Rickard, Sascha Grunert, and Stephen Augustus from SIG Release, Bob Killen and Kaslin Fields from SIG Contribex, Tim Allclair from the Security Response Committee. Also a big thank you to our friends acting as liaisons with our cloud provider partners: Jay Pipes from Amazon and Jon Johnson Jr. from Google.&lt;/p></description></item><item><title>Blog: Kubernetes Removals, Deprecations, and Major Changes in 1.26</title><link>https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/</link><pubDate>Fri, 18 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Frederico Muñoz (SAS)&lt;/p>
&lt;p>Change is an integral part of the Kubernetes life-cycle: as Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements for the health of the project. For Kubernetes v1.26 there are several planned: this article identifies and describes some of them, based on the information available at this mid-cycle point in the v1.26 release process, which is still ongoing and can introduce additional changes.&lt;/p>
&lt;h2 id="k8s-api-deprecation-process">The Kubernetes API Removal and Deprecation process&lt;/h2>
&lt;p>The Kubernetes project has a &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">well-documented deprecation policy&lt;/a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.&lt;/p>
&lt;ul>
&lt;li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.&lt;/li>
&lt;li>Beta or pre-release API versions must be supported for 3 releases after deprecation.&lt;/li>
&lt;li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.&lt;/li>
&lt;/ul>
&lt;p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.&lt;/p>
&lt;h2 id="cri-api-removal">A note about the removal of the CRI &lt;code>v1alpha2&lt;/code> API and containerd 1.5 support&lt;/h2>
&lt;p>Following the adoption of the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cri/">Container Runtime Interface&lt;/a> (CRI) and the [removal of dockershim] in v1.24 , the CRI is the supported and documented way through which Kubernetes interacts with different container runtimes. Each kubelet negotiates which version of CRI to use with the container runtime on that node.&lt;/p>
&lt;p>The Kubernetes project recommends using CRI version &lt;code>v1&lt;/code>; in Kubernetes v1.25 the kubelet can also negotiate the use of CRI &lt;code>v1alpha2&lt;/code> (which was deprecated along at the same time as adding support for the stable &lt;code>v1&lt;/code> interface).&lt;/p>
&lt;p>Kubernetes v1.26 will not support CRI &lt;code>v1alpha2&lt;/code>. That &lt;a href="https://github.com/kubernetes/kubernetes/pull/110618">removal&lt;/a> will result in the kubelet not registering the node if the container runtime doesn't support CRI &lt;code>v1&lt;/code>. This means that containerd minor version 1.5 and older will not be supported in Kubernetes 1.26; if you use containerd, you will need to upgrade to containerd version 1.6.0 or later &lt;strong>before&lt;/strong> you upgrade that node to Kubernetes v1.26. Other container runtimes that only support the &lt;code>v1alpha2&lt;/code> are equally affected: if that affects you, you should contact the container runtime vendor for advice or check their website for additional instructions in how to move forward.&lt;/p>
&lt;p>If you want to benefit from v1.26 features and still use an older container runtime, you can run an older kubelet. The &lt;a href="https://kubernetes.io/releases/version-skew-policy/#kubelet">supported skew&lt;/a> for the kubelet allows you to run a v1.25 kubelet, which still is still compatible with &lt;code>v1alpha2&lt;/code> CRI support, even if you upgrade the control plane to the 1.26 minor release of Kubernetes.&lt;/p>
&lt;p>As well as container runtimes themselves, that there are tools like &lt;a href="https://github.com/containerd/stargz-snapshotter">stargz-snapshotter&lt;/a> that act as a proxy between kubelet and container runtime and those also might be affected.&lt;/p>
&lt;h2 id="deprecations-removals">Deprecations and removals in Kubernetes v1.26&lt;/h2>
&lt;p>In addition to the above, Kubernetes v1.26 is targeted to include several additional removals and deprecations.&lt;/p>
&lt;h3 id="removal-of-the-v1beta1-flow-control-api-group">Removal of the &lt;code>v1beta1&lt;/code> flow control API group&lt;/h3>
&lt;p>The &lt;code>flowcontrol.apiserver.k8s.io/v1beta1&lt;/code> API version of FlowSchema and PriorityLevelConfiguration &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v126">will no longer be served in v1.26&lt;/a>. Users should migrate manifests and API clients to use the &lt;code>flowcontrol.apiserver.k8s.io/v1beta2&lt;/code> API version, available since v1.23.&lt;/p>
&lt;h3 id="removal-of-the-v2beta2-horizontalpodautoscaler-api">Removal of the &lt;code>v2beta2&lt;/code> HorizontalPodAutoscaler API&lt;/h3>
&lt;p>The &lt;code>autoscaling/v2beta2&lt;/code> API version of HorizontalPodAutoscaler &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#horizontalpodautoscaler-v126">will no longer be served in v1.26&lt;/a>. Users should migrate manifests and API clients to use the &lt;code>autoscaling/v2&lt;/code> API version, available since v1.23.&lt;/p>
&lt;h3 id="removal-of-in-tree-credential-management-code">Removal of in-tree credential management code&lt;/h3>
&lt;p>In this upcoming release, legacy vendor-specific authentication code that is part of Kubernetes
will be &lt;a href="https://github.com/kubernetes/kubernetes/pull/112341">removed&lt;/a> from both
&lt;code>client-go&lt;/code> and &lt;code>kubectl&lt;/code>.
The existing mechanism supports authentication for two specific cloud providers:
Azure and Google Cloud.
In its place, Kubernetes already offers a vendor-neutral
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">authentication plugin mechanism&lt;/a> -
you can switch over right now, before the v1.26 release happens.
If you're affected, you can find additional guidance on how to proceed for
&lt;a href="https://github.com/Azure/kubelogin#readme">Azure&lt;/a> and for
&lt;a href="https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke">Google Cloud&lt;/a>.&lt;/p>
&lt;h3 id="removal-of-kube-proxy-userspace-modes">Removal of &lt;code>kube-proxy&lt;/code> userspace modes&lt;/h3>
&lt;p>The &lt;code>userspace&lt;/code> proxy mode, deprecated for over a year, is &lt;a href="https://github.com/kubernetes/kubernetes/pull/112133">no longer supported on either Linux or Windows&lt;/a> and will be removed in this release. Users should use &lt;code>iptables&lt;/code> or &lt;code>ipvs&lt;/code> on Linux, or &lt;code>kernelspace&lt;/code> on Windows: using &lt;code>--mode userspace&lt;/code> will now fail.&lt;/p>
&lt;h3 id="removal-of-in-tree-openstack-cloud-provider">Removal of in-tree OpenStack cloud provider&lt;/h3>
&lt;p>Kubernetes is switching from in-tree code for storage integrations, in favor of the Container Storage Interface (CSI).
As part of this, Kubernetes v1.26 will remove the deprecated in-tree storage integration for OpenStack
(the &lt;code>cinder&lt;/code> volume type). You should migrate to external cloud provider and CSI driver from
&lt;a href="https://github.com/kubernetes/cloud-provider-openstack">https://github.com/kubernetes/cloud-provider-openstack&lt;/a> instead.
For more information, visit &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">Cinder in-tree to CSI driver migration&lt;/a>.&lt;/p>
&lt;h3 id="removal-of-the-glusterfs-in-tree-driver">Removal of the GlusterFS in-tree driver&lt;/h3>
&lt;p>The in-tree GlusterFS driver was &lt;a href="https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/#deprecations-and-removals">deprecated in v1.25&lt;/a>, and will be removed from Kubernetes v1.26.&lt;/p>
&lt;h3 id="deprecation-of-non-inclusive-kubectl-flag">Deprecation of non-inclusive &lt;code>kubectl&lt;/code> flag&lt;/h3>
&lt;p>As part of the implementation effort of the &lt;a href="https://www.cncf.io/announcements/2021/10/13/inclusive-naming-initiative-announces-new-community-resources-for-a-more-inclusive-future/">Inclusive Naming Initiative&lt;/a>,
the &lt;code>--prune-whitelist&lt;/code> flag will be &lt;a href="https://github.com/kubernetes/kubernetes/pull/113116">deprecated&lt;/a>, and replaced with &lt;code>--prune-allowlist&lt;/code>.
Users that use this flag are strongly advised to make the necessary changes prior to the final removal of the flag, in a future release.&lt;/p>
&lt;h3 id="removal-of-dynamic-kubelet-configuration">Removal of dynamic kubelet configuration&lt;/h3>
&lt;p>&lt;em>Dynamic kubelet configuration&lt;/em> allowed &lt;a href="https://github.com/kubernetes/enhancements/tree/2cd758cc6ab617a93f578b40e97728261ab886ed/keps/sig-node/281-dynamic-kubelet-configuration">new kubelet configurations to be rolled out via the Kubernetes API&lt;/a>, even in a live cluster.
A cluster operator could reconfigure the kubelet on a Node by specifying a ConfigMap
that contained the configuration data that the kubelet should use.
Dynamic kubelet configuration was removed from the kubelet in v1.24, and will be
&lt;a href="https://github.com/kubernetes/kubernetes/pull/112643">removed from the API server&lt;/a> in the v1.26 release.&lt;/p>
&lt;h3 id="deprecations-for-kube-apiserver-command-line-arguments">Deprecations for &lt;code>kube-apiserver&lt;/code> command line arguments&lt;/h3>
&lt;p>The &lt;code>--master-service-namespace&lt;/code> command line argument to the kube-apiserver doesn't have
any effect, and was already informally &lt;a href="https://github.com/kubernetes/kubernetes/pull/38186">deprecated&lt;/a>.
That command line argument will be formally marked as deprecated in v1.26, preparing for its
removal in a future release.
The Kubernetes project does not expect any impact from this deprecation and removal.&lt;/p>
&lt;h3 id="deprecations-for-kubectl-run-command-line-arguments">Deprecations for &lt;code>kubectl run&lt;/code> command line arguments&lt;/h3>
&lt;p>Several unused option arguments for the &lt;code>kubectl run&lt;/code> subcommand will be &lt;a href="https://github.com/kubernetes/kubernetes/pull/112261">marked as deprecated&lt;/a>, including:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--cascade&lt;/code>&lt;/li>
&lt;li>&lt;code>--filename&lt;/code>&lt;/li>
&lt;li>&lt;code>--force&lt;/code>&lt;/li>
&lt;li>&lt;code>--grace-period&lt;/code>&lt;/li>
&lt;li>&lt;code>--kustomize&lt;/code>&lt;/li>
&lt;li>&lt;code>--recursive&lt;/code>&lt;/li>
&lt;li>&lt;code>--timeout&lt;/code>&lt;/li>
&lt;li>&lt;code>--wait&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These arguments are already ignored so no impact is expected: the explicit deprecation sets a warning message and prepares the removal of the arguments in a future release.&lt;/p>
&lt;h3 id="removal-of-legacy-command-line-arguments-relating-to-logging">Removal of legacy command line arguments relating to logging&lt;/h3>
&lt;p>Kubernetes v1.26 will &lt;a href="https://github.com/kubernetes/kubernetes/pull/112120">remove&lt;/a> some
command line arguments relating to logging. These command line arguments were
already deprecated.
For more information, see &lt;a href="https://github.com/kubernetes/enhancements/tree/3cb66bd0a1ef973ebcc974f935f0ac5cba9db4b2/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">Deprecate klog specific flags in Kubernetes Components&lt;/a>.&lt;/p>
&lt;h2 id="looking-ahead">Looking ahead&lt;/h2>
&lt;p>The official list of &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-27">API removals&lt;/a> planned for Kubernetes 1.27 includes:&lt;/p>
&lt;ul>
&lt;li>All beta versions of the CSIStorageCapacity API; specifically: &lt;code>storage.k8s.io/v1beta1&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="want-to-know-more">Want to know more?&lt;/h3>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation">Kubernetes 1.25&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>We will formally announce the deprecations that come with &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#deprecation">Kubernetes 1.26&lt;/a> as part of the CHANGELOG for that release.&lt;/p></description></item><item><title>Blog: Live and let live with Kluctl and Server Side Apply</title><link>https://kubernetes.io/blog/2022/11/04/live-and-let-live-with-kluctl-and-ssa/</link><pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/11/04/live-and-let-live-with-kluctl-and-ssa/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Alexander Block&lt;/p>
&lt;p>This blog post was inspired by a previous Kubernetes blog post about
&lt;a href="https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/">Advanced Server Side Apply&lt;/a>.
The author of said blog post listed multiple benefits for applications and
controllers when switching to server-side apply (from now on abbreviated with
SSA). Especially the chapter about
&lt;a href="https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/#ci-cd-systems">CI/CD systems&lt;/a>
motivated me to respond and write down my thoughts and experiences.&lt;/p>
&lt;p>These thoughts and experiences are the results of me working on &lt;a href="https://kluctl.io">Kluctl&lt;/a>
for the past 2 years. I describe Kluctl as &amp;quot;The missing glue to put together
large Kubernetes deployments, composed of multiple smaller parts
(Helm/Kustomize/...) in a manageable and unified way.&amp;quot;&lt;/p>
&lt;p>To get a basic understanding of Kluctl, I suggest to visit the &lt;a href="https://kluctl.io">kluctl.io&lt;/a>
website and read through the documentation and tutorials, for example the
&lt;a href="https://kluctl.io/docs/guides/tutorials/microservices-demo/">microservices demo tutorial&lt;/a>.
As an alternative, you can watch &lt;a href="https://www.youtube.com/watch?v=9LoYLjDjOdg">Hands-on Introduction to kluctl&lt;/a>
from the Rawkode Academy YouTube channel which shows a hands-on demo session.&lt;/p>
&lt;p>There is also a &lt;a href="https://github.com/codablock/podtato-head/tree/kluctl/delivery/kluctl">Kluctl delivery scenario&lt;/a>
available in my fork of the &lt;a href="https://github.com/codablock/podtato-head">podtato-head&lt;/a> demo project.&lt;/p>
&lt;h2 id="live-and-let-live">Live and let live&lt;/h2>
&lt;p>One of the main philosophies that Kluctl follows is &lt;a href="https://kluctl.io/docs/philosophy/#live-and-let-live">&amp;quot;live and let live&amp;quot;&lt;/a>,
meaning that it will try its best to work in conjunction with any other tool or
controller running outside or inside your clusters. Kluctl will not overwrite
any fields that it lost ownership of, unless you explicitly tell it to do so.&lt;/p>
&lt;p>Achieving this would not have been possible (or at least several magnitudes
harder) without the use of SSA. Server-side apply allows Kluctl
to detect when ownership for a field got lost, for example when another controller
or operator updates that field to another value. Kluctl can then decide on a
field-by-field basis if force-applying is required before retrying based on these
decisions.&lt;/p>
&lt;h2 id="the-days-before-ssa">The days before SSA&lt;/h2>
&lt;p>The first versions of Kluctl were based on shelling out to &lt;code>kubectl&lt;/code> and thus
implicitly relied on client-side apply. At that time, SSA was
still alpha and quite buggy. And to be honest, I didn't even know it was a
thing at that time.&lt;/p>
&lt;p>The way client-side apply worked had some serious drawbacks. The most obvious one
(it was guaranteed that you'd stumble on this by yourself if enough time passed)
is that it relied on an annotation (&lt;code>kubectl.kubernetes.io/last-applied-configuration&lt;/code>)
being added to the object, bringing in all the limitations and issues with huge
annotation values. A good example of such issues are
&lt;a href="https://github.com/prometheus-operator/prometheus-operator/issues/4439">CRDs being so large&lt;/a>,
that they don't fit into the annotation's value anymore.&lt;/p>
&lt;p>Another drawback can be seen just by looking at the name (&lt;strong>client&lt;/strong>-side apply).
Being &lt;strong>client&lt;/strong> side means that each client has to provide the apply-logic on
its own, which at that time was only properly implemented inside &lt;code>kubectl&lt;/code>,
making it hard to be replicated inside controllers.&lt;/p>
&lt;p>This added &lt;code>kubectl&lt;/code> as a dependency (either as an executable or in the form of
Go packages) to all controllers that wanted to leverage the apply-logic.&lt;/p>
&lt;p>However, even if one managed to get client-side apply running from inside a
controller, you ended up with a solution that gave no control over how it
worked internally. As an example, there was no way to individually decide which
fields to overwrite in case of external changes and which ones to let go.&lt;/p>
&lt;h2 id="discovering-ssa-apply">Discovering SSA apply&lt;/h2>
&lt;p>I was never happy with the solution described above and then somehow stumbled
across &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">server-side apply&lt;/a>,
which was still in beta at that time. Experimenting with it via
&lt;code>kubectl apply --server-side&lt;/code> revealed immediately that the true power of
SSA can not be easily leveraged by shelling out to &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>The way SSA is implemented in &lt;code>kubectl&lt;/code> does not allow enough
control over conflict resolution as it can only switch between
&amp;quot;not force-applying anything and erroring out&amp;quot; and &amp;quot;force-applying everything
without showing any mercy!&amp;quot;.&lt;/p>
&lt;p>The API documentation however made it clear that SSA is able to
control conflict resolution on field level, simply by choosing which fields
to include and which fields to omit from the supplied object.&lt;/p>
&lt;h2 id="moving-away-from-kubectl">Moving away from kubectl&lt;/h2>
&lt;p>This meant that Kluctl had to move away from shelling out to &lt;code>kubectl&lt;/code> first. Only
after that was done, I would have been able to properly implement SSA
with its powerful conflict resolution.&lt;/p>
&lt;p>To achieve this, I first implemented access to the target clusters via a
Kubernetes client library. This had the nice side effect of dramatically
speeding up Kluctl as well. It also improved the security and usability of
Kluctl by ensuring that a running Kluctl command could not be messed around
with by externally modifying the kubeconfig while it was running.&lt;/p>
&lt;h2 id="implementing-ssa">Implementing SSA&lt;/h2>
&lt;p>After switching to a Kubernetes client library, leveraging SSA
felt easy. Kluctl now has to send each manifest to the API server as part of a
&lt;code>PATCH&lt;/code> request, which signals
that Kluctl wants to perform a SSA operation. The API server then
responds with an OK response (HTTP status code 200), or with a Conflict response
(HTTP status 409).&lt;/p>
&lt;p>In case of a Conflict response, the body of that response includes machine-readable
details about the conflicts. Kluctl can then use these details to figure out
which fields are in conflict and which actors (field managers) have taken
ownership of the conflicted fields.&lt;/p>
&lt;p>Then, for each field, Kluctl will decide if the conflict should be ignored or
if it should be force-applied. If any field needs to be force-applied, Kluctl
will retry the apply operation with the ignored fields omitted and the &lt;code>force&lt;/code>
flag being set on the API call.&lt;/p>
&lt;p>In case a conflict is ignored, Kluctl will issue a warning to the user so that
the user can react properly (or ignore it forever...).&lt;/p>
&lt;p>That's basically it. That is all that is required to leverage SSA.
Big thanks and thumbs-up to the Kubernetes developers who made this possible!&lt;/p>
&lt;h2 id="conflict-resolution">Conflict Resolution&lt;/h2>
&lt;p>Kluctl has a few simple rules to figure out if a conflict should be ignored
or force-applied.&lt;/p>
&lt;p>It first checks the field's actor (the field manager) against a list of known
field manager strings from tools that are frequently used to perform manual modifications. These
are for example &lt;code>kubectl&lt;/code> and &lt;code>k9s&lt;/code>. Any modifications performed with these tools
are considered &amp;quot;temporary&amp;quot; and will be overwritten by Kluctl.&lt;/p>
&lt;p>If you're using Kluctl along with &lt;code>kubectl&lt;/code> where you don't want the changes from
&lt;code>kubectl&lt;/code> to be overwritten (for example, using in a script) then you can specify
&lt;code>--field-manager=&amp;lt;manager-name&amp;gt;&lt;/code> on the command line to &lt;code>kubectl&lt;/code>, and Kluctl
doesn't apply its special heuristic.&lt;/p>
&lt;p>If the field manager is not known by Kluctl, it will check if force-applying is
requested for that field. Force-applying can be requested in different ways:&lt;/p>
&lt;ol>
&lt;li>By passing &lt;code>--force-apply&lt;/code> to Kluctl. This will cause ALL fields to be force-applied on conflicts.&lt;/li>
&lt;li>By adding the &lt;a href="https://kluctl.io/docs/reference/deployments/annotations/all-resources/#kluctlioforce-apply">&lt;code>kluctl.io/force-apply=true&lt;/code>&lt;/a> annotation to the object in question. This will cause all fields of that object to be force-applied on conflicts.&lt;/li>
&lt;li>By adding the &lt;a href="https://kluctl.io/docs/reference/deployments/annotations/all-resources/#kluctlioforce-apply-field">&lt;code>kluctl.io/force-apply-field=my.json.path&lt;/code>&lt;/a> annotation to the object in question. This causes only fields matching the JSON path to be force-applied on conflicts.&lt;/li>
&lt;/ol>
&lt;p>Marking a field to be force-applied is required whenever some other actor is
known to erroneously claim fields (the ECK operator does this to the nodeSets
field for example), you can ensure that Kluctl always overwrites these fields
to the original or a new value.&lt;/p>
&lt;p>In the future, Kluctl will allow even more control about conflict resolution.
For example, the CLI will allow to control force-applying on field level.&lt;/p>
&lt;h2 id="devops-vs-controllers">DevOps vs Controllers&lt;/h2>
&lt;p>So how does SSA in Kluctl lead to &amp;quot;live and let live&amp;quot;?&lt;/p>
&lt;p>It allows the co-existence of classical pipelines (e.g. Github Actions or
Gitlab CI), controllers (e.g. the HPA controller or GitOps style controllers)
and even admins running deployments from their local machines.&lt;/p>
&lt;p>Wherever you are on your infrastructure automation journey, Kluctl has a place
for you. From running deployments using a script on your PC, all the way to
fully automated CI/CD with the pipelines themselves defined in code, Kluctl
aims to complement the workflow that's right for you.&lt;/p>
&lt;p>And even after fully automating everything, you can intervene with your admin
permissions if required and run a &lt;code>kubectl&lt;/code> command that will modify a field
and prevent Kluctl from overwriting it. You'd just have to switch to a
field-manager (e.g. &amp;quot;admin-override&amp;quot;) that is not overwritten by Kluctl.&lt;/p>
&lt;h2 id="a-few-takeaways">A few takeaways&lt;/h2>
&lt;p>Server-side apply is a great feature and essential for the future of
controllers and tools in Kubernetes. The amount of controllers involved
will only get more and proper modes of working together are a must.&lt;/p>
&lt;p>I believe that CI/CD-related controllers and tools should leverage
SSA to perform proper conflict resolution. I also believe that
other controllers (e.g. Flux and ArgoCD) would benefit from the same kind
of conflict resolution control on field-level.&lt;/p>
&lt;p>It might even be a good idea to come together and work on a standardized
set of annotations to control conflict resolution for CI/CD-related tooling.&lt;/p>
&lt;p>On the other side, non CI/CD-related controllers should ensure that they don't
cause unnecessary conflicts when modifying objects. As of
&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/#using-server-side-apply-in-a-controller">the server-side apply documentation&lt;/a>,
it is strongly recommended for controllers to always perform force-applying. When
following this recommendation, controllers should really make sure that only
fields related to the controller are included in the applied object.
Otherwise, unnecessary conflicts are guaranteed.&lt;/p>
&lt;p>In many cases, controllers are meant to only modify the status subresource
of the objects they manage. In this case, controllers should only patch the
status subresource and not touch the actual object. If this is followed,
conflicts become impossible to occur.&lt;/p>
&lt;p>If you are a developer of such a controller and unsure about your controller
adhering to the above, simply try to retrieve an object managed by your
controller and look at the &lt;code>managedFields&lt;/code> (you'll need to pass
&lt;code>--show-managed-fields -oyaml&lt;/code> to &lt;code>kubectl get&lt;/code>) to see if some field got
claimed unexpectedly.&lt;/p></description></item><item><title>Blog: Server Side Apply Is Great And You Should Be Using It</title><link>https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/</link><pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Daniel Smith (Google)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side apply&lt;/a> (SSA) has now
been &lt;a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/">GA for a few releases&lt;/a>, and I
have found myself in a number of conversations, recommending that people / teams
in various situations use it. So I’d like to write down some of those reasons.&lt;/p>
&lt;h2 id="benefits">Obvious (and not-so-obvious) benefits of SSA&lt;/h2>
&lt;p>A list of improvements / niceties you get from switching from various things to
Server-side apply!&lt;/p>
&lt;ul>
&lt;li>Versus client-side-apply (that is, plain &lt;code>kubectl apply&lt;/code>):
&lt;ul>
&lt;li>The system gives you conflicts when you accidentally fight with another
actor over the value of a field!&lt;/li>
&lt;li>When combined with &lt;code>--dry-run&lt;/code>, there’s no chance of accidentally running a
client-side dry run instead of a server side dry run.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Versus hand-rolling patches:
&lt;ul>
&lt;li>The SSA patch format is extremely natural to write, with no weird syntax.
It’s just a regular object, but you can (and should) omit any field you
don’t care about.&lt;/li>
&lt;li>The old patch format (“strategic merge patch”) was ad-hoc and still has some
bugs; JSON-patch and JSON merge-patch fail to handle some cases that are
common in the Kubernetes API, namely lists with items that should be
recursively merged based on a “name” or other identifying field.&lt;/li>
&lt;li>There’s also now great &lt;a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#using-server-side-apply-in-a-controller">go-language library support&lt;/a>
for building apply calls programmatically!&lt;/li>
&lt;li>You can use SSA to explicitly delete fields you don’t “own” by setting them
to &lt;code>null&lt;/code>, which makes it a feature-complete replacement for all of the old
patch formats.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Versus shelling out to kubectl:
&lt;ul>
&lt;li>You can use the &lt;strong>apply&lt;/strong> API call from any language without shelling out to
kubectl!&lt;/li>
&lt;li>As stated above, the &lt;a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#server-side-apply-support-in-client-go">Go library has dedicated mechanisms&lt;/a>
to make this easy now.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Versus GET-modify-PUT:
&lt;ul>
&lt;li>(This one is more complicated and you can skip it if you've never written a
controller!)&lt;/li>
&lt;li>To use GET-modify-PUT correctly, you have to handle and retry a write
failure in the case that someone else has modified the object in any way
between your GET and PUT. This is an “optimistic concurrency failure” when
it happens.&lt;/li>
&lt;li>SSA offloads this task to the server– you only have to retry if there’s a
conflict, and the conflicts you can get are all meaningful, like when you’re
actually trying to take a field away from another actor in the system.&lt;/li>
&lt;li>To put it another way, if 10 actors do a GET-modify-PUT cycle at the same
time, 9 will get an optimistic concurrency failure and have to retry, then
8, etc, for up to 50 total GET-PUT attempts in the worst case (that’s .5N^2
GET and PUT calls for N actors making simultaneous changes). If the actors
are using SSA instead, and the changes don’t actually conflict over specific
fields, then all the changes can go in in any order. Additionally, SSA
changes can often be done without a GET call at all. That’s only N &lt;strong>apply&lt;/strong>
requests for N actors, which is a drastic improvement!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-can-i-use-ssa">How can I use SSA?&lt;/h2>
&lt;h3 id="users">Users&lt;/h3>
&lt;p>Use &lt;code>kubectl apply --server-side&lt;/code>! Soon we (SIG API Machinery) hope to make this
the default and remove the “client side” apply completely!&lt;/p>
&lt;h3 id="controller-authors">Controller authors&lt;/h3>
&lt;p>There’s two main categories here, but for both of them, &lt;strong>you should probably
&lt;em>force conflicts&lt;/em> when using SSA&lt;/strong>. This is because your controller probably
doesn’t know what to do when some other entity in the system has a different
desire than your controller about a particular field. (See the &lt;a href="#ci-cd-systems">CI/CD
section&lt;/a>, though!)&lt;/p>
&lt;h4 id="get-modify-put-patch-controllers">Controllers that use either a GET-modify-PUT sequence or a PATCH&lt;/h4>
&lt;p>This kind of controller GETs an object (possibly from a
&lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes">&lt;strong>watch&lt;/strong>&lt;/a>),
modifies it, and then PUTs it back to write its changes. Sometimes it constructs
a custom PATCH, but the semantics are the same. Most existing controllers
(especially those in-tree) work like this.&lt;/p>
&lt;p>If your controller is perfect, great! You don’t need to change it. But if you do
want to change it, you can take advantage of the new client library’s &lt;em>extract&lt;/em>
workflow– that is, &lt;strong>get&lt;/strong> the existing object, extract your existing desires,
make modifications, and re-&lt;strong>apply&lt;/strong>. For many controllers that were computing
the smallest API changes possible, this will be a minor update to the existing
implementation.&lt;/p>
&lt;p>This workflow avoids the failure mode of accidentally trying to own every field
in the object, which is what happens if you just GET the object, make changes,
and then &lt;strong>apply&lt;/strong>. (Note that the server will notice you did this and reject
your change!)&lt;/p>
&lt;h4 id="reconstructive-controllers">Reconstructive controllers&lt;/h4>
&lt;p>This kind of controller wasn't really possible prior to SSA. The idea here is to
(whenever something changes etc) reconstruct from scratch the fields of the
object as the controller wishes them to be, and then &lt;strong>apply&lt;/strong> the change to the
server, letting it figure out the result. I now recommend that new controllers
start out this way–it's less fiddly to say what you want an object to look like
than it is to say how you want it to change.&lt;/p>
&lt;p>The client library supports this method of operation by default.&lt;/p>
&lt;p>The only downside is that you may end up sending unneeded &lt;strong>apply&lt;/strong> requests to
the API server, even if actually the object already matches your controller’s
desires. This doesn't matter if it happens once in a while, but for extremely
high-throughput controllers, it might cause a performance problem for the
cluster–specifically, the API server. No-op writes are not written to storage
(etcd) or broadcast to any watchers, so it’s not really that big of a deal. If
you’re worried about this anyway, today you could use the method explained in
the previous section, or you could still do it this way for now, and wait for an
additional client-side mechanism to suppress zero-change applies.&lt;/p>
&lt;p>To get around this downside, why not GET the object and only send your &lt;strong>apply&lt;/strong>
if the object needs it? Surprisingly, it doesn't help much – a no-op &lt;strong>apply&lt;/strong> is
not very much more work for the API server than an extra GET; and an &lt;strong>apply&lt;/strong>
that changes things is cheaper than that same &lt;strong>apply&lt;/strong> with a preceding GET.
Worse, since it is a distributed system, something could change between your GET
and &lt;strong>apply&lt;/strong>, invalidating your computation. Instead, you can use this
optimization on an object retrieved from a cache–then it legitimately will
reduce load on the system (at the cost of a delay when a change is needed and
the cache is a bit behind).&lt;/p>
&lt;h4 id="ci-cd-systems">CI/CD systems&lt;/h4>
&lt;p>Continuous integration (CI) and/or continuous deployment (CD) systems are a
special kind of controller which is doing something like reading manifests from
source control (such as a Git repo) and automatically pushing them into the
cluster. Perhaps the CI / CD process first generates manifests from a template,
then runs some tests, and then deploys a change. Typically, users are the
entities pushing changes into source control, although that’s not necessarily
always the case.&lt;/p>
&lt;p>Some systems like this continuously reconcile with the cluster, others may only
operate when a change is pushed to the source control system. The following
considerations are important for both, but more so for the continuously
reconciling kind.&lt;/p>
&lt;p>CI/CD systems are literally controllers, but for the purpose of &lt;strong>apply&lt;/strong>, they
are more like users, and unlike other controllers, they need to pay attention to
conflicts. Reasoning:&lt;/p>
&lt;ul>
&lt;li>Abstractly, CI/CD systems can change anything, which means they could conflict
with &lt;strong>any&lt;/strong> controller out there. The recommendation that controllers force
conflicts is assuming that controllers change a limited number of things and
you can be reasonably sure that they won’t fight with other controllers about
those things; that’s clearly not the case for CI/CD controllers.&lt;/li>
&lt;li>Concrete example: imagine the CI/CD system wants &lt;code>.spec.replicas&lt;/code> for some
Deployment to be 3, because that is the value that is checked into source
code; however there is also a HorizontalPodAutoscaler (HPA) that targets the
same deployment. The HPA computes a target scale and decides that there should
be 10 replicas. Which should win? I just said that most controllers–including
the HPA–should ignore conflicts. The HPA has no idea if it has been enabled
incorrectly, and the HPA has no convenient way of informing users of errors.&lt;/li>
&lt;li>The other common cause of a CI/CD system getting a conflict is probably when
it is trying to overwrite a hot-fix (hand-rolled patch) placed there by a
system admin / SRE / dev-on-call. You almost certainly don’t want to override
that automatically.&lt;/li>
&lt;li>Of course, sometimes SRE makes an accidental change, or a dev makes an
unauthorized change – those you do want to notice and overwrite; however, the
CI/CD system can’t tell the difference between these last two cases.&lt;/li>
&lt;/ul>
&lt;p>Hopefully this convinces you that CI/CD systems need error paths–a way to
back-propagate these conflict errors to humans; in fact, they should have this
already, certainly continuous integration systems need some way to report that
tests are failing. But maybe I can also say something about how &lt;em>humans&lt;/em> can
deal with errors:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Reject the hotfix: the (human) administrator of the CI/CD system observes the
error, and manually force-applies the manifest in question. Then the CI/CD
system will be able to apply the manifest successfully and become a co-owner.&lt;/p>
&lt;p>Optional: then the administrator applies a blank manifest (just the object
type / namespace / name) to relinquish any fields they became a manager for.
if this step is omitted, there's some chance the administrator will end up
owning fields and causing an unwanted future conflict.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: why an administrator? I'm assuming that developers which ordinarily
push to the CI/CD system and / or its source control system may not have
permissions to push directly to the cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Accept the hotfix: the author of the change in question sees the conflict, and
edits their change to accept the value running in production.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Accept then reject: as in the accept option, but after that manifest is
applied, and the CI/CD queue owns everything again (so no conflicts), re-apply
the original manifest.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I can also imagine the CI/CD system permitting you to mark a manifest as
“force conflicts” somehow– if there’s demand for this we could consider making
a more standardized way to do this. A rigorous version of this which lets you
declare exactly which conflicts you intend to force would require support from
the API server; in lieu of that, you can make a second manifest with only that
subset of fields.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Future work: we could imagine an especially advanced CI/CD system that could
parse &lt;code>metadata.managedFields&lt;/code> data to see who or what they are conflicting
with, over what fields, and decide whether or not to ignore the conflict. In
fact, this information is also presented in any conflict errors, though
perhaps not in an easily machine-parseable format. We (SIG API Machinery)
mostly didn't expect that people would want to take this approach — so we
would love to know if in fact people want/need the features implied by this
approach, such as the ability, when &lt;strong>apply&lt;/strong>ing to request to override
certain conflicts but not others.&lt;/p>
&lt;p>If this sounds like an approach you'd want to take for your own controller,
come talk to SIG API Machinery!&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Happy &lt;strong>apply&lt;/strong>ing!&lt;/p></description></item><item><title>Blog: Current State: 2019 Third Party Security Audit of Kubernetes</title><link>https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/</link><pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong> (in alphabetical order): Cailyn Edwards (Shopify), Pushkar Joglekar (VMware), Rey Lejano (SUSE) and Rory McCune (DataDog)&lt;/p>
&lt;p>We expect the brand new Third Party Security Audit of Kubernetes will be
published later this month (Oct 2022).&lt;/p>
&lt;p>In preparation for that, let's look at the state of findings that were made
public as part of the last &lt;a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-external-audit/security-audit-2019">third party security audit of
2019&lt;/a>
that was based on &lt;a href="https://github.com/kubernetes/kubernetes/tree/release-1.13">Kubernetes v1.13.4&lt;/a>.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>&lt;a href="https://github.com/cji">Craig Ingram&lt;/a> has graciously attempted over the years to keep track of the
status of the findings reported in the last audit in this issue:
&lt;a href="https://github.com/kubernetes/kubernetes/issues/81146">kubernetes/kubernetes#81146&lt;/a>.
This blog post will attempt to dive deeper into this, address any gaps
in tracking and become a point in time summary of the state of the
findings reported from 2019.&lt;/p>
&lt;p>This article should also help readers gain confidence through transparent
communication, of work done by the community to address these findings and
bubble up any findings that need help from community contributors.&lt;/p>
&lt;h2 id="current-state">Current State&lt;/h2>
&lt;p>The status of each issue / finding here is represented in a best effort manner.
Authors do not claim to be 100% accurate on the status and welcome any
corrections or feedback if the current state is not reflected accurately by
commenting directly on the relevant issue.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>#&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Title&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Issue&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Status&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>hostPath PersistentVolumes enable PodSecurityPolicy bypass&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81110">#81110&lt;/a>&lt;/td>
&lt;td>closed, addressed by &lt;a href="https://github.com/kubernetes/website/pull/15756">kubernetes/website#15756&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/pull/109798">kubernetes/kubernetes#109798&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>Kubernetes does not facilitate certificate revocation&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81111">#81111&lt;/a>&lt;/td>
&lt;td>duplicate of &lt;a href="https://github.com/kubernetes/kubernetes/issues/18982">#18982&lt;/a> and &lt;strong>needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>HTTPS connections are not authenticated&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81112">#81112&lt;/a>&lt;/td>
&lt;td>Largely left as an end user exercise in setting up the right configuration&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>&lt;abbr title="Time-of-check to time-of-use bug">TOCTOU&lt;/abbr> when moving PID to manager's cgroup via kubelet&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81113">#81113&lt;/a>&lt;/td>
&lt;td>Requires Node access for successful exploitation. Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>Improperly patched directory traversal in &lt;code>kubectl cp&lt;/code>&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/pull/76788">#76788&lt;/a>&lt;/td>
&lt;td>closed, assigned &lt;a href="https://github.com/advisories/GHSA-v8c4-hw4j-x4pr">CVE-2019-11249&lt;/a>, fixed in &lt;a href="https://github.com/kubernetes/kubernetes/pull/80436">#80436&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>Bearer tokens are revealed in logs&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81114">#81114&lt;/a>&lt;/td>
&lt;td>closed, assigned &lt;a href="https://github.com/advisories/GHSA-jmrx-5g74-6v2f">CVE-2019-11250&lt;/a>, fixed in &lt;a href="https://github.com/kubernetes/kubernetes/pull/81330">#81330&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>Seccomp is disabled by default&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81115">#81115&lt;/a>&lt;/td>
&lt;td>closed, addressed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/101943">#101943&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>Pervasive world-accessible file permissions&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81116">#81116&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112384">#112384&lt;/a> ( in progress)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>Environment variables expose sensitive data&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81117">#81117&lt;/a>&lt;/td>
&lt;td>closed, addressed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/84992">#84992&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/pull/84677">#84677&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>Use of InsecureIgnoreHostKey in SSH connections&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81118">#81118&lt;/a>&lt;/td>
&lt;td>This feature was removed in v1.22: &lt;a href="https://github.com/kubernetes/kubernetes/pull/102297">#102297&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>Use of InsecureSkipVerify and other TLS weaknesses&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81119">#81119&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>&lt;code>kubeadm&lt;/code> performs potentially-dangerous reset operations&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81120">#81120&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81495">#81495&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/81494">#81494&lt;/a>, and &lt;a href="https://github.com/kubernetes/website/pull/15881">kubernetes/website#15881&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>Overflows when using strconv.Atoi and downcasting the result&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81121">#81121&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/89120">#89120&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>kubelet can cause an Out of Memory error with a malicious manifest&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81122">#81122&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/76518">#76518&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>&lt;code>kubectl&lt;/code> can cause an Out Of Memory error with a malicious Pod specification&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81123">#81123&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>Improper fetching of PIDs allows incorrect cgroup movement&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81124">#81124&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>Directory traversal of host logs running kube-apiserver and kubelet&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81125">#81125&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/87273">#87273&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>Non-constant time password comparison&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81126">#81126&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81152">#81152&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>Encryption recommendations not in accordance with best practices&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81127">#81127&lt;/a>&lt;/td>
&lt;td>Work in Progress&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>Adding credentials to containers by default is unsafe&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81128">#81128&lt;/a>&lt;/td>
&lt;td>Closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/89193">#89193&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>kubelet liveness probes can be used to enumerate host network&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81129">#81129&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>iSCSI volume storage cleartext secrets in logs&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81130">#81130&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81215">#81215&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>Hard coded credential paths&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81131">#81131&lt;/a>&lt;/td>
&lt;td>closed, awaiting more evidence&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>Log rotation is not atomic&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81132">#81132&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>Arbitrary file paths without bounding&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81133">#81133&lt;/a>&lt;/td>
&lt;td>Fix needed.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>Unsafe JSON construction&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81134">#81134&lt;/a>&lt;/td>
&lt;td>Partially fixed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>kubelet crash due to improperly handled errors&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81135">#81135&lt;/a>&lt;/td>
&lt;td>Closed. Fixed by &lt;a href="https://github.com/kubernetes/kubernetes/issues/81135">#81135&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>Legacy tokens do not expire&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81136">#81136&lt;/a>&lt;/td>
&lt;td>closed, fixed as part of &lt;a href="https://github.com/kubernetes/kubernetes/issues/70679">#70679&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>CoreDNS leaks internal cluster information across namespaces&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81137">#81137&lt;/a>&lt;/td>
&lt;td>Closed, resolved with CoreDNS v1.6.2. &lt;a href="https://github.com/kubernetes/kubernetes/issues/81137">#81137&lt;/a> (comment)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>Services use questionable default functions&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81138">#81138&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>31&lt;/td>
&lt;td>Incorrect docker daemon process name in container manager&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81139">#81139&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81083">#81083&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>Use standard formats everywhere&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81140">#81140&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>33&lt;/td>
&lt;td>Superficial health check provides false sense of safety&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81141">#81141&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81319">#81319&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>34&lt;/td>
&lt;td>Hardcoded use of insecure gRPC transport&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81142">#81142&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>35&lt;/td>
&lt;td>Incorrect handling of &lt;code>Retry-After&lt;/code>&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81143">#81143&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/91048">#91048&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>36&lt;/td>
&lt;td>Incorrect isKernelPid check&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81144">#81144&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81086">#81086&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>37&lt;/td>
&lt;td>Kubelet supports insecure TLS ciphersuites&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81145">#81145&lt;/a>&lt;/td>
&lt;td>closed but fix needed for &lt;a href="https://github.com/kubernetes/kubernetes/issues/91444">#91444&lt;/a> (see &lt;a href="https://github.com/kubernetes/kubernetes/issues/81145#issuecomment-630291221">this comment&lt;/a>)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inspired-outcomes">Inspired outcomes&lt;/h3>
&lt;p>Apart from fixes to the specific issues, the 2019 third party security audit
also motivated security focussed enhancements in the next few releases of
Kubernetes. One such example is
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-security/1933-secret-logging-static-analysis">Kubernetes Enhancement Proposal (KEP) 1933 Defend Against Logging Secrets via Static Analysis&lt;/a> to prevent exposing
secrets to logs with &lt;a href="@PurelyApplied">Patrick Rhomberg&lt;/a> driving the
implementation. As a result of this KEP,
&lt;a href="https://github.com/google/go-flow-levee">&lt;code>go-flow-levee&lt;/code>&lt;/a>, a taint propagation
analysis tool configured to detect logging of secrets, is executed in a
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/hack/verify-govet-levee.sh">script&lt;/a>
as a Prow presubmit job. This KEP was introduced in v1.20.0 as an alpha
feature, then graduated to beta in v1.21.0, and graduated to stable in
v1.23.0. As stable, the analysis runs as a blocking presubmit test. This
KEP also helped resolve the following issues from the 2019 third party security audit:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81114">#81114 Bearer tokens are revealed in logs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81117">#81117 Environment variables expose sensitive data&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81130">#81130 iSCSI volume storage cleartext secrets in logs&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="remaining-work">Remaining Work&lt;/h2>
&lt;p>Many of the 37 findings identified were fixed by work from
our community members over the last 3 years. However, we still have some work
left to do. Here's a breakdown of remaining work with rough estimates on
time commitment, complexity and benefits to the ecosystem on fixing
these pending issues.&lt;/p>
&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> Anything requiring a KEP (Kubernetes Enhancement Proposal) is considered
&lt;em>high&lt;/em> time commitment and &lt;em>high&lt;/em> complexity. Benefits to Ecosystem are
roughly equivalent to risk of keeping the finding unfixed which is
determined by Severity Level + Likelihood of a successful vulnerability
exploit. These estimates and values in the table below are the authors'
personal opinion. An individual or end users' threat model may rate the
benefits to fix a particular issue higher or lower.
&lt;/div>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Title&lt;/th>
&lt;th>Issue&lt;/th>
&lt;th>Time Commitment&lt;/th>
&lt;th>Complexity&lt;/th>
&lt;th>Benefit to Ecosystem&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kubernetes does not facilitate certificate revocation&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81111">#81111&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Use of InsecureSkipVerify and other TLS weaknesses&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81119">#81119&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>kubectl&lt;/code> can cause a local Out Of Memory error with a malicious Pod specification&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81123">#81123&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Improper fetching of PIDs allows incorrect cgroup movement&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81124">#81124&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kubelet liveness probes can be used to enumerate host network&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81129">#81129&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>API Server supports insecure TLS ciphersuites&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81145">#81145&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;abbr title="Time-of-check to time-of-use bug">TOCTOU&lt;/abbr> when moving PID to manager's cgroup via kubelet&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81113">#81113&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Log rotation is not atomic&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81132">#81132&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Arbitrary file paths without bounding&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81133">#81133&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Services use questionable default functions&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81138">#81138&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Use standard formats everywhere&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81140">#81140&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Very Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hardcoded use of insecure gRPC transport&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81142">#81142&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Very Low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>To get started on fixing any of these findings that need help, please
consider getting involved in &lt;a href="https://github.com/kubernetes/community/tree/master/sig-security#contact">Kubernetes SIG
Security&lt;/a>
by joining our bi-weekly meetings or hanging out with us on our Slack
Channel.&lt;/p></description></item><item><title>Blog: Introducing Kueue</title><link>https://kubernetes.io/blog/2022/10/04/introducing-kueue/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/04/introducing-kueue/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Abdullah Gharaibeh (Google), Aldo Culquicondor (Google)&lt;/p>
&lt;p>Whether on-premises or in the cloud, clusters face real constraints for resource usage, quota, and cost management reasons. Regardless of the autoscalling capabilities, clusters have finite capacity. As a result, users want an easy way to fairly and
efficiently share resources.&lt;/p>
&lt;p>In this article, we introduce &lt;a href="https://github.com/kubernetes-sigs/kueue/tree/main/docs#readme">Kueue&lt;/a>,
an open source job queueing controller designed to manage batch jobs as a single unit.
Kueue leaves pod-level orchestration to existing stable components of Kubernetes.
Kueue natively supports the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job&lt;/a>
API and offers hooks for integrating other custom-built APIs for batch jobs.&lt;/p>
&lt;h2 id="why-kueue">Why Kueue?&lt;/h2>
&lt;p>Job queueing is a key feature to run batch workloads at scale in both on-premises and cloud environments. The main goal
of job queueing is to manage access to a limited pool of resources shared by multiple tenants. Job queueing decides which
jobs should wait, which can start immediately, and what resources they can use.&lt;/p>
&lt;p>Some of the most desired job queueing requirements include:&lt;/p>
&lt;ul>
&lt;li>Quota and budgeting to control who can use what and up to what limit. This is not only needed in clusters with static resources like on-premises,
but it is also needed in cloud environments to control spend or usage of scarce resources.&lt;/li>
&lt;li>Fair sharing of resources between tenants. To maximize the usage of available resources, any unused quota assigned to inactive tenants should be
allowed to be shared fairly between active tenants.&lt;/li>
&lt;li>Flexible placement of jobs across different resource types based on availability. This is important in cloud environments which have heterogeneous
resources such as different architectures (GPU or CPU models) and different provisioning modes (spot vs on-demand).&lt;/li>
&lt;li>Support for autoscaled environments where resources can be provisioned on demand.&lt;/li>
&lt;/ul>
&lt;p>Plain Kubernetes doesn't address the above requirements. In normal circumstances, once a Job is created, the job-controller instantly creates the
pods and kube-scheduler continuously attempts to assign the pods to nodes. At scale, this situation can work the control plane to death. There is
also currently no good way to control at the job level which jobs should get which resources first, and no way to express order or fair sharing. The
current ResourceQuota model is not a good fit for these needs because quotas are enforced on resource creation, and there is no queueing of requests. The
intent of ResourceQuotas is to provide a builtin reliability mechanism with policies needed by admins to protect clusters from failing over.&lt;/p>
&lt;p>In the Kubernetes ecosystem, there are several solutions for job scheduling. However, we found that these alternatives have one or more of the following problems:&lt;/p>
&lt;ul>
&lt;li>They replace existing stable components of Kubernetes, like kube-scheduler or the job-controller. This is problematic not only from an operational point of view, but
also the duplication in the job APIs causes fragmentation of the ecosystem and reduces portability.&lt;/li>
&lt;li>They don't integrate with autoscaling, or&lt;/li>
&lt;li>They lack support for resource flexibility.&lt;/li>
&lt;/ul>
&lt;h2 id="overview">How Kueue works&lt;/h2>
&lt;p>With Kueue we decided to take a different approach to job queueing on Kubernetes that is anchored around the following aspects:&lt;/p>
&lt;ul>
&lt;li>Not duplicating existing functionalities already offered by established Kubernetes components for pod scheduling, autoscaling and job
lifecycle management.&lt;/li>
&lt;li>Adding key features that are missing to existing components. For example, we invested in the Job API to cover more use cases like
&lt;a href="https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs">IndexedJob&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-tracking-with-finalizers">fixed long standing issues related to pod
tracking&lt;/a>. While this path takes longer to
land features, we believe it is the more sustainable long term solution.&lt;/li>
&lt;li>Ensuring compatibility with cloud environments where compute resources are elastic and heterogeneous.&lt;/li>
&lt;/ul>
&lt;p>For this approach to be feasible, Kueue needs knobs to influence the behavior of those established components so it can effectively manage
when and where to start a job. We added those knobs to the Job API in the form of two features:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#suspending-a-job">Suspend field&lt;/a>, which allows Kueue to signal to the job-controller
when to start or stop a Job.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#mutable-scheduling-directives">Mutable scheduling directives&lt;/a>, which allows Kueue to
update a Job's &lt;code>.spec.template.spec.nodeSelector&lt;/code> before starting the Job. This way, Kueue can control Pod placement while still
delegating to kube-scheduler the actual pod-to-node scheduling.&lt;/li>
&lt;/ul>
&lt;p>Note that any custom job API can be managed by Kueue if that API offers the above two capabilities.&lt;/p>
&lt;h3 id="resource-model">Resource model&lt;/h3>
&lt;p>Kueue defines new APIs to address the requirements mentioned at the beginning of this post. The three main APIs are:&lt;/p>
&lt;ul>
&lt;li>ResourceFlavor: a cluster-scoped API to define resource flavor available for consumption, like a GPU model. At its core, a ResourceFlavor is
a set of labels that mirrors the labels on the nodes that offer those resources.&lt;/li>
&lt;li>ClusterQueue: a cluster-scoped API to define resource pools by setting quotas for one or more ResourceFlavor.&lt;/li>
&lt;li>LocalQueue: a namespaced API for grouping and managing single tenant jobs. In its simplest form, a LocalQueue is a pointer to the ClusterQueue
that the tenant (modeled as a namespace) can use to start their jobs.&lt;/li>
&lt;/ul>
&lt;p>For more details, take a look at the &lt;a href="https://sigs.k8s.io/kueue/docs/concepts">API concepts documentation&lt;/a>. While the three APIs may look overwhelming,
most of Kueue’s operations are centered around ClusterQueue; the ResourceFlavor and LocalQueue APIs are mainly organizational wrappers.&lt;/p>
&lt;h3 id="example-use-case">Example use case&lt;/h3>
&lt;p>Imagine the following setup for running batch workloads on a Kubernetes cluster on the cloud:&lt;/p>
&lt;ul>
&lt;li>You have &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster-autoscaler&lt;/a> installed in the cluster to automatically
adjust the size of your cluster.&lt;/li>
&lt;li>There are two types of autoscaled node groups that differ on their provisioning policies: spot and on-demand. The nodes of each group are
differentiated by the label &lt;code>instance-type=spot&lt;/code> or &lt;code>instance-type=ondemand&lt;/code>.
Moreover, since not all Jobs can tolerate running on spot nodes, the nodes are tainted with &lt;code>spot=true:NoSchedule&lt;/code>.&lt;/li>
&lt;li>To strike a balance between cost and resource availability, imagine you want Jobs to use up to 1000 cores of on-demand nodes, then use up to
2000 cores of spot nodes.&lt;/li>
&lt;/ul>
&lt;p>As an admin for the batch system, you define two ResourceFlavors that represent the two types of nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceFlavor&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ondemand&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">instance-type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ondemand &lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceFlavor&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">instance-type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">taints&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>- &lt;span style="color:#008000;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then you define the quotas by creating a ClusterQueue as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterQueue&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>research-pool&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{}&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;cpu&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">flavors&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ondemand&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">quota&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">min&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">quota&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">min&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that the order of flavors in the ClusterQueue resources matters: Kueue will attempt to fit jobs in the available quotas according to
the order unless the job has an explicit affinity to specific flavors.&lt;/p>
&lt;p>For each namespace, you define a LocalQueue that points to the ClusterQueue above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LocalQueue&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>training&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>team-ml&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clusterQueue&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>research-pool&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Admins create the above setup once. Batch users are able to find the queues they are allowed to
submit to by listing the LocalQueues in their namespace(s). The command is similar to the following: &lt;code>kubectl get -n my-namespace localqueues&lt;/code>&lt;/p>
&lt;p>To submit work, create a Job and set the &lt;code>kueue.x-k8s.io/queue-name&lt;/code> annotation as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">generateName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sample-job-&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kueue.x-k8s.io/queue-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>training&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parallelism&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">tolerations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Exists&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;NoSchedule&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-batch-workload&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>registry.example/batch/calculate-pi:3.14&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;30s&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Kueue intervenes to suspend the Job as soon as it is created. Once the Job is at the head of the ClusterQueue, Kueue evaluates if it can start
by checking if the resources requested by the job fit the available quota.&lt;/p>
&lt;p>In the above example, the Job tolerates spot resources. If there are previously admitted Jobs consuming all existing on-demand quota but
not all of spot’s, Kueue admits the Job using the spot quota. Kueue does this by issuing a single update to the Job object that:&lt;/p>
&lt;ul>
&lt;li>Changes the &lt;code>.spec.suspend&lt;/code> flag to false&lt;/li>
&lt;li>Adds the term &lt;code>instance-type: spot&lt;/code> to the job's &lt;code>.spec.template.spec.nodeSelector&lt;/code> so that when the pods are created by the job controller, those pods can only schedule
onto spot nodes.&lt;/li>
&lt;/ul>
&lt;p>Finally, if there are available empty nodes with matching node selector terms, then kube-scheduler will directly schedule the pods. If not, then
kube-scheduler will initially mark the pods as unschedulable, which will trigger the cluster-autoscaler to provision new nodes.&lt;/p>
&lt;h2 id="future-work-and-getting-involved">Future work and getting involved&lt;/h2>
&lt;p>The example above offers a glimpse of some of Kueue's features including support for quota, resource flexibility, and integration with cluster
autoscaler. Kueue also supports fair-sharing, job priorities, and different queueing strategies. Take a look at the
&lt;a href="https://github.com/kubernetes-sigs/kueue/tree/main/docs">Kueue documentation&lt;/a> to learn more about those features and how to use Kueue.&lt;/p>
&lt;p>We have a number of features that we plan to add to Kueue, such as hierarchical quota, budgets, and support for dynamically sized jobs. In
the more immediate future, we are focused on adding support for job preemption.&lt;/p>
&lt;p>The latest &lt;a href="https://github.com/kubernetes-sigs/kueue/releases">Kueue release&lt;/a> is available on Github;
try it out if you run batch workloads on Kubernetes (requires v1.22 or newer).
We are in the early stages of this project and we are seeking feedback of all levels, major or minor, so please don’t hesitate to reach out. We’re
also open to additional contributors, whether it is to fix or report bugs, or help add new features or write documentation. You can get in touch with
us via our &lt;a href="http://sigs.k8s.io/kueue">repo&lt;/a>, &lt;a href="https://groups.google.com/a/kubernetes.io/g/wg-batch">mailing list&lt;/a> or on
&lt;a href="https://kubernetes.slack.com/messages/wg-batch">Slack&lt;/a>.&lt;/p>
&lt;p>Last but not least, thanks to all &lt;a href="https://github.com/kubernetes-sigs/kueue/graphs/contributors">our contributors&lt;/a> who made this project possible!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: alpha support for running Pods with user namespaces</title><link>https://kubernetes.io/blog/2022/10/03/userns-alpha/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/03/userns-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Rodrigo Campos (Microsoft), Giuseppe Scrivano (Red Hat)&lt;/p>
&lt;p>Kubernetes v1.25 introduces the support for user namespaces.&lt;/p>
&lt;p>This is a major improvement for running secure workloads in
Kubernetes. Each pod will have access only to a limited subset of the
available UIDs and GIDs on the system, thus adding a new security
layer to protect from other pods running on the same system.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>A process running on Linux can use up to 4294967296 different UIDs and
GIDs.&lt;/p>
&lt;p>User namespaces is a Linux feature that allows mapping a set of users
in the container to different users in the host, thus restricting what
IDs a process can effectively use.
Furthermore, the capabilities granted in a new user namespace do not
apply in the host initial namespaces.&lt;/p>
&lt;h2 id="why-is-it-important">Why is it important?&lt;/h2>
&lt;p>There are mainly two reasons why user namespaces are important:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>improve security since they restrict the IDs a pod can use, so each
pod can run in its own separate environment with unique IDs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>enable running workloads as root in a safer manner.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In a user namespace we can map the root user inside the pod to a
non-zero ID outside the container, containers believe in running as
root while they are a regular unprivileged ID from the host point of
view.&lt;/p>
&lt;p>The process can keep capabilities that are usually restricted to
privileged pods and do it in a safe way since the capabilities granted
in a new user namespace do not apply in the host initial namespaces.&lt;/p>
&lt;h2 id="how-do-i-enable-user-namespaces">How do I enable user namespaces?&lt;/h2>
&lt;p>At the moment, user namespaces support is opt-in, so you must enable
it for a pod setting &lt;code>hostUsers&lt;/code> to &lt;code>false&lt;/code> under the pod spec stanza:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
spec:
hostUsers: false
containers:
- name: nginx
image: docker.io/nginx
&lt;/code>&lt;/pre>&lt;p>The feature is behind a feature gate, so make sure to enable
the &lt;code>UserNamespacesStatelessPodsSupport&lt;/code> gate before you can use
the new feature.&lt;/p>
&lt;p>The runtime must also support user namespaces:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>containerd: support is planned for the 1.7 release. See containerd
issue &lt;a href="https://github.com/containerd/containerd/issues/7063">#7063&lt;/a> for more details.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CRI-O: v1.25 has support for user namespaces.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Support for this in &lt;code>cri-dockerd&lt;/code> is &lt;a href="https://github.com/Mirantis/cri-dockerd/issues/74">not planned&lt;/a> yet.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>You can reach SIG Node by several means:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>You can also contact us directly:&lt;/p>
&lt;ul>
&lt;li>GitHub / Slack: @rata @giuseppe&lt;/li>
&lt;/ul></description></item><item><title>Blog: Enforce CRD Immutability with CEL Transition Rules</title><link>https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/</link><pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://github.com/alexzielenski">Alexander Zielenski&lt;/a> (Google)&lt;/p>
&lt;p>Immutable fields can be found in a few places in the built-in Kubernetes types.
For example, you can't change the &lt;code>.metadata.name&lt;/code> of an object. Specific objects
have fields where changes to existing objects are constrained; for example, the
&lt;code>.spec.selector&lt;/code> of a Deployment.&lt;/p>
&lt;p>Aside from simple immutability, there are other common design patterns such as
lists which are append-only, or a map with mutable values and immutable keys.&lt;/p>
&lt;p>Until recently the best way to restrict field mutability for CustomResourceDefinitions
has been to create a validating
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission webhook&lt;/a>:
this means a lot of complexity for the common case of making a field immutable.&lt;/p>
&lt;p>Beta since Kubernetes 1.25, CEL Validation Rules allow CRD authors to express
validation constraints on their fields using a rich expression language,
&lt;a href="https://github.com/google/cel-spec">CEL&lt;/a>. This article explores how you can
use validation rules to implement a few common immutability patterns directly in
the manifest for a CRD.&lt;/p>
&lt;h2 id="basics-of-validation-rules">Basics of validation rules&lt;/h2>
&lt;p>The new support for CEL validation rules in Kubernetes allows CRD authors to add
complicated admission logic for their resources without writing any code!&lt;/p>
&lt;p>For example, A CEL rule to constrain a field &lt;code>maximumSize&lt;/code> to be greater than a
&lt;code>minimumSize&lt;/code> for a CRD might look like the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44;font-style:italic"> &lt;/span>&lt;span style="color:#bbb"> &lt;/span>self.maximumSize &amp;gt; self.minimumSize&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;Maximum size must be greater than minimum size.&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The rule field contains an expression written in CEL. &lt;code>self&lt;/code> is a special keyword
in CEL which refers to the object whose type contains the rule.&lt;/p>
&lt;p>The message field is an error message which will be sent to Kubernetes clients
whenever this particular rule is not satisfied.&lt;/p>
&lt;p>For more details about the capabilities and limitations of Validation Rules using
CEL, please refer to
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules&lt;/a>.
The &lt;a href="https://github.com/google/cel-spec">CEL specification&lt;/a> is also a good
reference for information specifically related to the language.&lt;/p>
&lt;h2 id="immutability-patterns-with-cel-validation-rules">Immutability patterns with CEL validation rules&lt;/h2>
&lt;p>This section implements several common use cases for immutability in Kubernetes
CustomResourceDefinitions, using validation rules expressed as
&lt;a href="https://book.kubebuilder.io/reference/markers/crd.html">kubebuilder marker comments&lt;/a>.
Resultant OpenAPI generated by the kubebuilder marker comments will also be
included so that if you are writing your CRD manifests by hand you can still
follow along.&lt;/p>
&lt;h2 id="project-setup">Project setup&lt;/h2>
&lt;p>To use CEL rules with kubebuilder comments, you first need to set up a Golang
project structure with the CRD defined in Go.&lt;/p>
&lt;p>You may skip this step if you are not using kubebuilder or are only interested
in the resultant OpenAPI extensions.&lt;/p>
&lt;p>Begin with a folder structure of a Go module set up like the following. If
you have your own project already set up feel free to adapt this tutorial to your liking:&lt;/p>
&lt;figure>
&lt;div class="mermaid">
graph LR
. --> generate.go
. --> pkg --> apis --> stable.example.com --> v1
v1 --> doc.go
v1 --> types.go
. --> tools.go
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;p>This is the typical folder structure used by Kubernetes projects for defining new API resources.&lt;/p>
&lt;p>&lt;code>doc.go&lt;/code> contains package-level metadata such as the group and the version:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +groupName=stable.example.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +versionName=v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> v1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>types.go&lt;/code> contains all type definitions in stable.example.com/v1&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1 &lt;span style="color:#b44">&amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// An empty CRD as an example of defining a type using controller tools
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:storageversion
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:subresource:status
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> TestCRD &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Spec TestCRDSpec &lt;span style="color:#b44">`json:&amp;#34;spec,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status TestCRDStatus &lt;span style="color:#b44">`json:&amp;#34;status,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> TestCRDStatus &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> TestCRDSpec &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// You will fill this in as you go along
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>tools.go&lt;/code> contains a dependency on &lt;a href="https://book.kubebuilder.io/reference/generating-crd.html#generating-crds">controller-gen&lt;/a> which will be used to generate the CRD definition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">//go:build tools
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> celimmutabilitytutorial
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// Force direct dependency on code-generator so that it may be executed with go run
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _ &lt;span style="color:#b44">&amp;#34;sigs.k8s.io/controller-tools/cmd/controller-gen&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, &lt;code>generate.go&lt;/code>contains a &lt;code>go:generate&lt;/code> directive to make use of
&lt;code>controller-gen&lt;/code>. &lt;code>controller-gen&lt;/code> parses our &lt;code>types.go&lt;/code> and creates generates
CRD yaml files into a &lt;code>crd&lt;/code> folder:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> celimmutabilitytutorial
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">//go:generate go run sigs.k8s.io/controller-tools/cmd/controller-gen crd paths=./pkg/apis/... output:dir=./crds
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You may now want to add dependencies for our definitions and test the code generation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f">cd&lt;/span> cel-immutability-tutorial
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go mod init &amp;lt;your-org&amp;gt;/&amp;lt;your-module-name&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go mod tidy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After running these commands you now have completed the basic project structure.
Your folder tree should look like the following:&lt;/p>
&lt;figure>
&lt;div class="mermaid">
graph LR
. --> crds --> stable.example.com_testcrds.yaml
. --> generate.go
. --> go.mod
. --> go.sum
. --> pkg --> apis --> stable.example.com --> v1
v1 --> doc.go
v1 --> types.go
. --> tools.go
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;p>The manifest for the example CRD is now available in &lt;code>crds/stable.example.com_testcrds.yaml&lt;/code>.&lt;/p>
&lt;h2 id="immutablility-after-first-modification">Immutablility after first modification&lt;/h2>
&lt;p>A common immutability design pattern is to make the field immutable once it has
been first set. This example will throw a validation error if the field after
changes after being first initialized.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;!has(oldSelf.value) || has(self.value)&amp;#34;, message=&amp;#34;Value is required once set&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ImmutableSinceFirstWrite &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;self == oldSelf&amp;#34;,message=&amp;#34;Value is immutable&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxLength=512
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Value &lt;span style="color:#0b0;font-weight:bold">string&lt;/span> &lt;span style="color:#b44">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>+kubebuilder&lt;/code> directives in the comments inform controller-gen how to
annotate the generated OpenAPI. The &lt;code>XValidation&lt;/code> rule causes the rule to appear
among the &lt;code>x-kubernetes-validations&lt;/code> OpenAPI extension. Kubernetes then
respects the OpenAPI spec to enforce our constraints.&lt;/p>
&lt;p>To enforce a field's immutability after its first write, you need to apply the following constraints:&lt;/p>
&lt;ol>
&lt;li>Field must be allowed to be initially unset &lt;code>+kubebuilder:validation:Optional&lt;/code>&lt;/li>
&lt;li>Once set, field must not be allowed to be removed: &lt;code>!has(oldSelf.value) | has(self.value)&lt;/code> (type-scoped rule)&lt;/li>
&lt;li>Once set, field must not be allowed to change value &lt;code>self == oldSelf&lt;/code> (field-scoped rule)&lt;/li>
&lt;/ol>
&lt;p>Also note the additional directive &lt;code>+kubebuilder:validation:MaxLength&lt;/code>. CEL
requires that all strings have attached max length so that it may estimate the
computation cost of the rule. Rules that are too expensive will be rejected.
For more information on CEL cost budgeting, check out the other tutorial.&lt;/p>
&lt;h3 id="example-usage">Example usage&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_immutablesincefirstwrites.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/immutablesincefirstwrites.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating initial empty object with no &lt;code>value&lt;/code> is permitted since &lt;code>value&lt;/code> is &lt;code>optional&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">immutablesincefirstwrite.stable.example.com/test1 created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The initial modification of &lt;code>value&lt;/code> succeeds:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">immutablesincefirstwrite.stable.example.com/test1 configured
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>An attempt to change &lt;code>value&lt;/code> is blocked by the field-level validation rule. Note
the error message shown to the user comes from the validation rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, new world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceFirstWrite &amp;#34;test1&amp;#34; is invalid: value: Invalid value: &amp;#34;string&amp;#34;: Value is immutable
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>An attempt to remove the &lt;code>value&lt;/code> field altogether is blocked by the other validation rule
on the type. The error message also comes from the rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceFirstWrite &amp;#34;test1&amp;#34; is invalid: &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;object&amp;#34;: Value is required once set
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema">Generated schema&lt;/h3>
&lt;p>Note that in the generated schema there are two separate rule locations.
One is directly attached to the property &lt;code>immutable_since_first_write&lt;/code>.
The other rule is associated with the crd type itself.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxLength&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">512&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is immutable&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>self == oldSelf&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is required once set&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;!has(oldSelf.value) || has(self.value)&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="immutability-upon-object-creation">Immutability upon object creation&lt;/h2>
&lt;p>A field which is immutable upon creation time is implemented similarly to the
earlier example. The difference is that that field is marked required, and the
type-scoped rule is no longer necessary.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ImmutableSinceCreation &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Required
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;self == oldSelf&amp;#34;,message=&amp;#34;Value is immutable&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxLength=512
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Value &lt;span style="color:#0b0;font-weight:bold">string&lt;/span> &lt;span style="color:#b44">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This field will be required when the object is created, and after that point will
not be allowed to be modified. Our CEL Validation Rule &lt;code>self == oldSelf&lt;/code>&lt;/p>
&lt;h3 id="usage-example">Usage example&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_immutablesincecreations.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/immutablesincecreations.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Applying an object without the required field should fail:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceCreation &amp;#34;test1&amp;#34; is invalid:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* value: Required value
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;null&amp;#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that the field has been added, the operation is permitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">immutablesincecreation.stable.example.com/test1 created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you attempt to change the &lt;code>value&lt;/code>, the operation is blocked due to the
validation rules in the CRD. Note that the error message is as it was defined
in the validation rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, new world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceCreation &amp;#34;test1&amp;#34; is invalid: value: Invalid value: &amp;#34;string&amp;#34;: Value is immutable
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Also if you attempted to remove &lt;code>value&lt;/code> altogether after adding it, you will
see an error as expected:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceCreation &amp;#34;test1&amp;#34; is invalid:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* value: Required value
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;null&amp;#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema-1">Generated schema&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxLength&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">512&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is immutable&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>self == oldSelf&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">required&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- value&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="append-only-list-of-containers">Append-only list of containers&lt;/h2>
&lt;p>In the case of ephemeral containers on Pods, Kubernetes enforces that the
elements in the list are immutable, and can’t be removed. The following example
shows how you could use CEL to achieve the same behavior.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;!has(oldSelf.value) || has(self.value)&amp;#34;, message=&amp;#34;Value is required once set&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> AppendOnlyList &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxItems=100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;oldSelf.all(x, x in self)&amp;#34;,message=&amp;#34;Values may only be added&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Values []v1.EphemeralContainer &lt;span style="color:#b44">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Once set, field must not be deleted: &lt;code>!has(oldSelf.value) || has(self.value)&lt;/code> (type-scoped)&lt;/li>
&lt;li>Once a value is added it is not removed: &lt;code>oldSelf.all(x, x in self)&lt;/code> (field-scoped)&lt;/li>
&lt;li>Value may be initially unset: &lt;code>+kubebuilder:validation:Optional&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>Note that for cost-budgeting purposes, &lt;code>MaxItems&lt;/code> is also required to be specified.&lt;/p>
&lt;h3 id="example-usage-1">Example usage&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_appendonlylists.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/appendonlylists.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating an initial list with one element inside should succeed without problem:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: nginx/nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">appendonlylist.stable.example.com/testlist created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adding an element to the list should also proceed without issue as expected:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: nginx/nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: mongodb/mongodb
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">appendonlylist.stable.example.com/testlist configured
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But if you now attempt to remove an element, the error from the validation rule
is triggered:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: nginx/nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The AppendOnlyList &amp;#34;testlist&amp;#34; is invalid: value: Invalid value: &amp;#34;array&amp;#34;: Values may only be added
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Additionally, to attempt to remove the field once it has been set is also disallowed
by the type-scoped validation rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The AppendOnlyList &amp;#34;testlist&amp;#34; is invalid: &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;object&amp;#34;: Value is required once set
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema-2">Generated schema&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">items&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxItems&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>array&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Values may only be added&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>oldSelf.all(x, x in self)&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is required once set&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;!has(oldSelf.value) || has(self.value)&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="map-with-append-only-keys-immutable-values">Map with append-only keys, immutable values&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// A map which does not allow keys to be removed or their values changed once set. New keys may be added, however.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;!has(oldSelf.values) || has(self.values)&amp;#34;, message=&amp;#34;Value is required once set&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> MapAppendOnlyKeys &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxProperties=10
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;oldSelf.all(key, key in self &amp;amp;&amp;amp; self[key] == oldSelf[key])&amp;#34;,message=&amp;#34;Keys may not be removed and their values must stay the same&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Values &lt;span style="color:#a2f;font-weight:bold">map&lt;/span>[&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>]&lt;span style="color:#0b0;font-weight:bold">string&lt;/span> &lt;span style="color:#b44">`json:&amp;#34;values,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Once set, field must not be deleted: &lt;code>!has(oldSelf.values) || has(self.values)&lt;/code> (type-scoped)&lt;/li>
&lt;li>Once a key is added it is not removed nor is its value modified: &lt;code>oldSelf.all(key, key in self &amp;amp;&amp;amp; self[key] == oldSelf[key])&lt;/code> (field-scoped)&lt;/li>
&lt;li>Value may be initially unset: &lt;code>+kubebuilder:validation:Optional&lt;/code>&lt;/li>
&lt;/ol>
&lt;h3 id="example-usage-2">Example usage&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_mapappendonlykeys.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/mapappendonlykeys.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating an initial object with one key within &lt;code>values&lt;/code> should be permitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">values:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key1: value1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">mapappendonlykeys.stable.example.com/testmap created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adding new keys to the map should also be permitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">values:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key1: value1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key2: value2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">mapappendonlykeys.stable.example.com/testmap configured
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But if a key is removed, the error messagr from the validation rule should be
returned:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">values:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key1: value1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The MapAppendOnlyKeys &amp;#34;testmap&amp;#34; is invalid: values: Invalid value: &amp;#34;object&amp;#34;: Keys may not be removed and their values must stay the same
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the entire field is removed, the other validation rule is triggered and the
operation is prevented. Note that the error message for the validation rule is
shown to the user.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The MapAppendOnlyKeys &amp;#34;testmap&amp;#34; is invalid: &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;object&amp;#34;: Value is required once set
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema-3">Generated schema&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">description&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>A map which does not allow keys to be removed or their values&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>changed once set. New keys may be added, however.&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">additionalProperties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxProperties&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Keys may not be removed and their values must stay the same&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>oldSelf.all(key, key in self &amp;amp;&amp;amp; self[key] == oldSelf[key])&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is required once set&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;!has(oldSelf.values) || has(self.values)&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="going-further">Going further&lt;/h1>
&lt;p>The above examples showed how CEL rules can be added to kubebuilder types.
The same rules can be added directly to OpenAPI if writing a manifest for a CRD by hand.&lt;/p>
&lt;p>For native types, the same behavior can be achieved using kube-openapi’s marker
&lt;a href="https://github.com/kubernetes/kube-openapi/blob/923526ac052c59656d41710b45bbcb03748aa9d6/pkg/generators/extension.go#L69">&lt;code>+validations&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Usage of CEL within Kubernetes Validation Rules is so much more powerful than
what has been shown in this article. For more information please check out
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules&lt;/a>
in the Kubernetes documentation and &lt;a href="https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/">CRD Validation Rules Beta&lt;/a> blog post.&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Kubernetes In-Tree to CSI Volume Migration Status Update</title><link>https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/</link><pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jiawei Wang (Google)&lt;/p>
&lt;p>The Kubernetes in-tree storage plugin to &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI)&lt;/a> migration infrastructure has already been &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta&lt;/a> since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for CSI Migration feature to go GA.&lt;/p>
&lt;p>SIG Storage is excited to announce that the core CSI Migration feature is &lt;strong>generally available&lt;/strong> in Kubernetes v1.25 release!&lt;/p>
&lt;p>SIG Storage wrote a blog post in v1.23 for &lt;a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/">CSI Migration status update&lt;/a> which discussed the CSI migration status for each storage driver. It has been a while and this article is intended to give a latest status update on each storage driver for their CSI Migration status in Kubernetes v1.25.&lt;/p>
&lt;h2 id="quick-recap-what-is-csi-migration-and-why-migrate">Quick recap: What is CSI Migration, and why migrate?&lt;/h2>
&lt;p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.
Kubernetes support for the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">Container Storage Interface&lt;/a> has been
&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">generally available&lt;/a> since Kubernetes v1.13.
Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).&lt;/p>
&lt;p>As more CSI Drivers were created and became production ready, SIG Storage wanted all Kubernetes users to benefit from the CSI model. However, we could not break API compatibility with the existing storage API types due to k8s architecture conventions. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.&lt;/p>
&lt;p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code>kubernetes.io/gce-pd&lt;/code> or &lt;code>kubernetes.io/aws-ebs&lt;/code> with a corresponding &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI driver&lt;/a> from the storage backend.
If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. Existing &lt;code>StorageClass&lt;/code>, &lt;code>PersistentVolume&lt;/code> and &lt;code>PersistentVolumeClaim&lt;/code> objects should continue to work.
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.
However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p>
&lt;p>For example, suppose you are a &lt;code>kubernetes.io/gce-pd&lt;/code> user; after CSI migration, you can still use &lt;code>kubernetes.io/gce-pd&lt;/code> to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing APIs and Interface will still function correctly. However, the underlying function calls are all going through the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI driver&lt;/a> instead of the in-tree Kubernetes function.&lt;/p>
&lt;p>This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.&lt;/p>
&lt;h2 id="timeline-and-status">What is the timeline / status?&lt;/h2>
&lt;p>The current and targeted releases for each individual driver is shown in the table below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Beta (in-tree deprecated)&lt;/th>
&lt;th>Beta (on-by-default)&lt;/th>
&lt;th>GA&lt;/th>
&lt;th>Target &amp;quot;in-tree plugin&amp;quot; removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.28 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph FS&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph RBD&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;td>1.28 (Target)&lt;/td>
&lt;td>1.30 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Portworx&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;td>1.29 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.28 (Target)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The following storage drivers will not have CSI migration support.
The &lt;code>scaleio&lt;/code>, &lt;code>flocker&lt;/code>, &lt;code>quobyte&lt;/code> and &lt;code>storageos&lt;/code> drivers were removed; the others are deprecated and will be removed from core Kubernetes in the coming releases.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Deprecated&lt;/th>
&lt;th>Code Removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Flocker&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GlusterFS&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Quobyte&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ScaleIO&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>StorageOS&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="what-does-it-mean-for-the-core-csi-migration-feature-to-go-ga">What does it mean for the core CSI Migration feature to go GA?&lt;/h2>
&lt;p>Core CSI Migration goes to GA means that the general framework, core library and API for CSI migration is
stable for Kubernetes v1.25 and will be part of future Kubernetes releases as well.&lt;/p>
&lt;ul>
&lt;li>If you are a Kubernetes distribution maintainer, this means if you disabled &lt;code>CSIMigration&lt;/code> feature gate previously, you are no longer allowed to do so because the feature gate has been locked.&lt;/li>
&lt;li>If you are a Kubernetes storage driver developer, this means you can expect no backwards incompatibility changes in the CSI migration library.&lt;/li>
&lt;li>If you are a Kubernetes maintainer, expect nothing changes from your day to day development flows.&lt;/li>
&lt;li>If you are a Kubernetes user, expect nothing to change from your day-to-day usage flows. If you encounter any storage related issues, contact the people who operate your cluster (if that's you, contact the provider of your Kubernetes distribution, or get help from the &lt;a href="https://kubernetes.io/community/#discuss">community&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="what-does-it-mean-for-the-storage-driver-csi-migration-to-go-ga">What does it mean for the storage driver CSI migration to go GA?&lt;/h2>
&lt;p>Storage Driver CSI Migration goes to GA means that the specific storage driver supports CSI Migration. Expect feature parity between the in-tree plugin with the CSI driver.&lt;/p>
&lt;ul>
&lt;li>If you are a Kubernetes distribution maintainer, make sure you install the corresponding
CSI driver on the distribution. And make sure you are not disabling the specific &lt;code>CSIMigration{provider}&lt;/code> flag, as they are locked.&lt;/li>
&lt;li>If you are a Kubernetes storage driver maintainer, make sure the CSI driver can ensure feature parity if it supports CSI migration.&lt;/li>
&lt;li>If you are a Kubernetes maintainer/developer, expect nothing to change from your day-to-day development flows.&lt;/li>
&lt;li>If you are a Kubernetes user, the CSI Migration feature should be completely transparent
to you, the only requirement is to install the corresponding CSI driver.&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What's next?&lt;/h2>
&lt;p>We are expecting cloud provider in-tree storage plugins code removal to start to happen as part of the v1.26 and v1.27 releases of Kubernetes. More and more drivers that support CSI migration will go GA in the upcoming releases.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The Kubernetes Slack channel &lt;a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration&lt;/a> along with any of the standard &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage communication channels&lt;/a> are great ways to reach out to the SIG Storage and migration working group teams.&lt;/p>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:&lt;/p>
&lt;ul>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;/ul>
&lt;p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:&lt;/p>
&lt;ul>
&lt;li>Andy Zhang (andyzhangz)&lt;/li>
&lt;li>Divyen Patel (divyenpatel)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>Ismail Alidzhikov (ialidzhikov)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Matthew Cary (mattcary)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;li>Neha Arora (nearora-msft)&lt;/li>
&lt;li>Oksana Naumov (trierra)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;/ul>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: CustomResourceDefinition Validation Rules Graduate to Beta</title><link>https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/</link><pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Joe Betz (Google), Cici Huang (Google), Kermit Alexander (Google)&lt;/p>
&lt;p>In Kubernetes 1.25, &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation rules for CustomResourceDefinitions&lt;/a> (CRDs) have graduated to Beta!&lt;/p>
&lt;p>Validation rules make it possible to declare how custom resources are validated using the &lt;a href="https://github.com/google/cel-spec">Common Expression Language&lt;/a> (CEL). For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiextensions.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CustomResourceDefinition&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;self.minReplicas &amp;lt;= self.replicas &amp;amp;&amp;amp; self.replicas &amp;lt;= self.maxReplicas&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;replicas should be in the range minReplicas..maxReplicas.&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>integer&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Validation rules support a wide range of use cases. To get a sense of some of the capabilities, let's look at a few examples:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Validation Rule&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>self.minReplicas &amp;lt;= self.replicas&lt;/code>&lt;/td>
&lt;td>Validate an integer field is less than or equal to another integer field&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>'Available' in self.stateCounts&lt;/code>&lt;/td>
&lt;td>Validate an entry with the 'Available' key exists in a map&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.set1.all(e, !(e in self.set2))&lt;/code>&lt;/td>
&lt;td>Validate that the elements of two sets are disjoint&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self == oldSelf&lt;/code>&lt;/td>
&lt;td>Validate that a required field is immutable once it is set&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.created + self.ttl &amp;lt; self.expired&lt;/code>&lt;/td>
&lt;td>Validate that 'expired' date is after a 'create' date plus a 'ttl' duration&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Validation rules are expressive and flexible. See the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation Rules documentation&lt;/a> to learn more about what validation rules are capable of.&lt;/p>
&lt;h2 id="why-cel">Why CEL?&lt;/h2>
&lt;p>CEL was chosen as the language for validation rules for a couple reasons:&lt;/p>
&lt;ul>
&lt;li>CEL expressions can easily be inlined into CRD schemas. They are sufficiently expressive to replace the vast majority of CRD validation checks currently implemented in admission webhooks. This results in CRDs that are self-contained and are easier to understand.&lt;/li>
&lt;li>CEL expressions are compiled and type checked against a CRD's schema &amp;quot;ahead-of-time&amp;quot; (when CRDs are created and updated) allowing them to be evaluated efficiently and safely &amp;quot;runtime&amp;quot; (when custom resources are validated). Even regex string literals in CEL are validated and pre-compiled when CRDs are created or updated.&lt;/li>
&lt;/ul>
&lt;h2 id="why-not-use-validation-webhooks">Why not use validation webhooks?&lt;/h2>
&lt;p>Benefits of using validation rules when compared with validation webhooks:&lt;/p>
&lt;ul>
&lt;li>CRD authors benefit from a simpler workflow since validation rules eliminate the need to develop and maintain a webhook.&lt;/li>
&lt;li>Cluster administrators benefit by no longer having to install, upgrade and operate webhooks for the purposes of CRD validation.&lt;/li>
&lt;li>Cluster operability improves because CRD validation no longer requires a remote call to a webhook endpoint, eliminating a potential point of failure in the request-serving-path of the Kubernetes API server. This allows clusters to retain high availability while scaling to larger amounts of installed CRD extensions, since expected control plane availability would otherwise decrease with each additional webhook installed.&lt;/li>
&lt;/ul>
&lt;h2 id="getting-started-with-validation-rules">Getting started with validation rules&lt;/h2>
&lt;h3 id="writing-validation-rules-in-openapiv3-schemas">Writing validation rules in OpenAPIv3 schemas&lt;/h3>
&lt;p>You can define validation rules for any level of a CRD's OpenAPIv3 schema. Validation rules are automatically scoped to their location in the schema where they are declared.&lt;/p>
&lt;p>Good practices for CRD validation rules:&lt;/p>
&lt;ul>
&lt;li>Scope validation rules as close as possible to the fields(s) they validate.&lt;/li>
&lt;li>Use multiple rules when validating independent constraints.&lt;/li>
&lt;li>Do not use validation rules for validations already&lt;/li>
&lt;li>Use OpenAPIv3 &lt;a href="https://swagger.io/specification/#properties">value validations&lt;/a> (&lt;code>maxLength&lt;/code>, &lt;code>maxItems&lt;/code>, &lt;code>maxProperties&lt;/code>, &lt;code>required&lt;/code>, &lt;code>enum&lt;/code>, &lt;code>minimum&lt;/code>, &lt;code>maximum&lt;/code>, ..) and &lt;a href="https://swagger.io/docs/specification/data-models/data-types/#format">string formats&lt;/a> where available.&lt;/li>
&lt;li>Use &lt;code>x-kubernetes-int-or-string&lt;/code>, &lt;code>x-kubernetes-embedded-type&lt;/code> and &lt;code>x-kubernetes-list-type=(set|map)&lt;/code> were appropriate.&lt;/li>
&lt;/ul>
&lt;p>Examples of good practice:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Validation&lt;/th>
&lt;th>Best Practice&lt;/th>
&lt;th>Example(s)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Validate an integer is between 0 and 100.&lt;/td>
&lt;td>Use OpenAPIv3 value validations.&lt;/td>
&lt;td>&lt;pre>type: integer&lt;br>minimum: 0&lt;br>maximum: 100&lt;/pre>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Constraint the max size limits on maps (objects with additionalProperties), arrays and string.&lt;/td>
&lt;td>Use OpenAPIv3 value validations. Recommended for all maps, arrays and strings. This best practice is essential for rule cost estimation (explained below).&lt;/td>
&lt;td>&lt;pre>type:&lt;br>maxItems: 100&lt;/pre>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Require a date-time be more recent than a particular timestamp.&lt;/td>
&lt;td>Use OpenAPIv3 string formats to declare that the field is a date-time. Use validation rules to compare it to a particular timestamp.&lt;/td>
&lt;td>&lt;pre>type: string&lt;br>format: date-time&lt;br>x-kubernetes-validations:&lt;br> - rule: &amp;quot;self &amp;gt;= timestamp('2000-01-01T00:00:00.000Z')&amp;quot;&lt;/pre>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Require two sets to be disjoint.&lt;/td>
&lt;td>Use x-kubernetes-list-type to validate that the arrays are sets. &lt;br>Use validation rules to validate the sets are disjoint.&lt;/td>
&lt;td>&lt;pre>type: object&lt;br>properties:&lt;br> set1:&lt;br> type: array&lt;br> x-kubernetes-list-type: set&lt;br> set2: ...&lt;br> x-kubernetes-validations:&lt;br> - rule: &amp;quot;!self.set1.all(e, !(e in self.set2))&amp;quot;&lt;/pre>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="crd-transition-rules">CRD transition rules&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#transition-rules">Transition Rules&lt;/a> make it possible to compare the new state against the old state of a resource in validation rules. You use transition rules to make sure that the cluster's API server does not accept invalid state transitions. A transition rule is a validation rule that references 'oldSelf'. The API server only evaluates transition rules when both an old value and new value exist.&lt;/p>
&lt;p>Transition rule examples:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Transition Rule&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>self == oldSelf&lt;/code>&lt;/td>
&lt;td>For a required field, make that field immutable once it is set. For an optional field, only allow transitioning from unset to set, or from set to unset.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(on parent of field) &lt;code>has(self.field) == has(oldSelf.field)&lt;/code>&lt;br>on field: &lt;code>self == oldSelf&lt;/code>&lt;/td>
&lt;td>Make a field immutable: validate that a field, even if optional, never changes after the resource is created (for a required field, the previous rule is simpler).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.all(x, x in oldSelf)&lt;/code>&lt;/td>
&lt;td>Only allow adding items to a field that represents a set (prevent removals).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self &amp;gt;= oldSelf&lt;/code>&lt;/td>
&lt;td>Validate that a number is monotonically increasing.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="using-the-functions-libraries">Using the Functions Libraries&lt;/h2>
&lt;p>Validation rules have access to a couple different function libraries:&lt;/p>
&lt;ul>
&lt;li>CEL standard functions, defined in the &lt;a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#list-of-standard-definitions">list of standard definitions&lt;/a>&lt;/li>
&lt;li>CEL standard &lt;a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#macros">macros&lt;/a>&lt;/li>
&lt;li>CEL &lt;a href="https://pkg.go.dev/github.com/google/cel-go/ext#Strings">extended string function library&lt;/a>&lt;/li>
&lt;li>Kubernetes &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions">CEL extension library&lt;/a> which includes supplemental functions for &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions">lists&lt;/a>, &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex">regex&lt;/a>, and &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex">URLs&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Examples of function libraries in use:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Validation Rule&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>!(self.getDayOfWeek() in [0, 6])&lt;/code>&lt;/td>
&lt;td>Validate that a date is not a Sunday or Saturday.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>isUrl(self) &amp;amp;&amp;amp; url(self).getHostname() in [a.example.com', 'b.example.com']&lt;/code>&lt;/td>
&lt;td>Validate that a URL has an allowed hostname.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.map(x, x.weight).sum() == 1&lt;/code>&lt;/td>
&lt;td>Validate that the weights of a list of objects sum to 1.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>int(self.find('^[0-9]*')) &amp;lt; 100&lt;/code>&lt;/td>
&lt;td>Validate that a string starts with a number less than 100.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.isSorted()&lt;/code>&lt;/td>
&lt;td>Validates that a list is sorted.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="resource-use-and-limits">Resource use and limits&lt;/h2>
&lt;p>To prevent CEL evaluation from consuming excessive compute resources, validation rules impose some limits. These limits are based on CEL &lt;em>cost units&lt;/em>, a platform and machine independent measure of execution cost. As a result, the limits are the same regardless of where they are enforced.&lt;/p>
&lt;h3 id="estimated-cost-limit">Estimated cost limit&lt;/h3>
&lt;p>CEL is, by design, non-Turing-complete in such a way that the halting problem isn’t a concern. CEL takes advantage of this design choice to include an &amp;quot;estimated cost&amp;quot; subsystem that can statically compute the worst case run time cost of any CEL expression. Validation rules are &lt;a href="o/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#resource-use-by-validation-functions">integrated with the estimated cost system&lt;/a> and disallow CEL expressions from being included in CRDs if they have a sufficiently poor (high) estimated cost. The estimated cost limit is set quite high and typically requires an O(n^2) or worse operation, across something of unbounded size, to be exceeded. Fortunately the fix is usually quite simple: because the cost system is aware of size limits declared in the CRD's schema, CRD authors can add size limits to the CRD's schema (&lt;code>maxItems&lt;/code> for arrays, &lt;code>maxProperties&lt;/code> for maps, &lt;code>maxLength&lt;/code> for strings) to reduce the estimated cost.&lt;/p>
&lt;p>Good practice:&lt;/p>
&lt;p>Set &lt;code>maxItems&lt;/code>, &lt;code>maxProperties&lt;/code> and &lt;code>maxLength&lt;/code> on all array, map (&lt;code>object&lt;/code> with &lt;code>additionalProperties&lt;/code>) and string types in CRD schemas! This results in lower and more accurate estimated costs and generally makes a CRD safer to use.&lt;/p>
&lt;h3 id="runtime-cost-limits-for-crd-validation-rules">Runtime cost limits for CRD validation rules&lt;/h3>
&lt;p>In addition to the estimated cost limit, CEL keeps track of actual cost while evaluating a CEL expression and will halt execution of the expression if a limit is exceeded.&lt;/p>
&lt;p>With the estimated cost limit already in place, the runtime cost limit is rarely encountered. But it is possible. For example, it might be encountered for a large resource composed entirely of a single large list and a validation rule that is either evaluated on each element in the list, or traverses the entire list.&lt;/p>
&lt;p>CRD authors can ensure the runtime cost limit will not be exceeded in much the same way the estimated cost limit is avoided: by setting &lt;code>maxItems&lt;/code>, &lt;code>maxProperties&lt;/code> and &lt;code>maxLength&lt;/code> on array, map and string types.&lt;/p>
&lt;h2 id="future-work">Future work&lt;/h2>
&lt;p>We look forward to working with the community on the adoption of CRD Validation Rules, and hope to see this feature promoted to general availability in an upcoming Kubernetes release!&lt;/p>
&lt;p>There is a growing community of Kubernetes contributors thinking about how to make it possible to write extensible admission controllers using CEL as a substitute for admission webhooks for policy enforcement use cases. Anyone interested should reach out to us on the usual &lt;a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery&lt;/a> channels or via slack at &lt;a href="https://kubernetes.slack.com/archives/C02TTBG6LF4">#sig-api-machinery-cel-dev&lt;/a>.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Special thanks to Cici Huang, Ben Luddy, Jordan Liggitt, David Eads, Daniel Smith, Dr. Stefan Schimanski, Leila Jalali and everyone who contributed to Validation Rules!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Use Secrets for Node-Driven Expansion of CSI Volumes</title><link>https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/</link><pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Humble Chirammal (Red Hat), Louis Koo (deeproute.ai)&lt;/p>
&lt;p>Kubernetes v1.25, released earlier this month, introduced a new feature
that lets your cluster expand storage volumes, even when access to those
volumes requires a secret (for example: a credential for accessing a SAN fabric)
to perform node expand operation. This new behavior is in alpha and you
must enable a feature gate (&lt;code>CSINodeExpandSecret&lt;/code>) to make use of it.
You must also be using &lt;a href="https://kubernetes-csi.github.io/docs/">CSI&lt;/a>
storage; this change isn't relevant to storage drivers that are built in to Kubernetes.&lt;/p>
&lt;p>To turn on this new, alpha feature, you enable the &lt;code>CSINodeExpandSecret&lt;/code> feature
gate for the kube-apiserver and kubelet, which turns on a mechanism to send &lt;code>secretRef&lt;/code>
configuration as part of NodeExpansion by the CSI drivers thus make use of
the same to perform node side expansion operation with the underlying
storage system.&lt;/p>
&lt;h2 id="what-is-this-all-about">What is this all about?&lt;/h2>
&lt;p>Before Kubernetes v1.24, you were able to define a cluster-level StorageClass
that made use of &lt;a href="https://kubernetes-csi.github.io/docs/secrets-and-credentials-storage-class.html">StorageClass Secrets&lt;/a>,
but you didn't have any mechanism to specify the credentials that would be used for
operations that take place when the storage was mounted onto a node and when
the volume has to be expanded at node side.&lt;/p>
&lt;p>The Kubernetes CSI already implemented a similar mechanism specific kinds of
volume resizes; namely, resizes of PersistentVolumes where the resizes take place
independently from any node referred as Controller Expansion. In that case, you
associate a PersistentVolume with a Secret that contains credentials for volume resize
actions, so that controller expansion can take place. CSI also supports a &lt;code>nodeExpandVolume&lt;/code>
operation which CSI drivers can make use independent of Controller Expansion or along with
Controller Expansion on which, where the resize is driven from a node in your cluster where
the volume is attached. Please read &lt;a href="https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/">Kubernetes 1.24: Volume Expansion Now A Stable Feature&lt;/a>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>At times, the CSI driver needs to check the actual size of the backend block storage (or image)
before proceeding with a node-level filesystem expand operation. This avoids false positive returns
from the backend storage cluster during filesystem expands.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When a PersistentVolume represents encrypted block storage (for example using LUKS)
you need to provide a passphrase in order to expand the device, and also to make it possible
to grow the filesystem on that device.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For various validations at time of node expansion, the CSI driver has to be connected
to the backend storage cluster. If the &lt;code>nodeExpandVolume&lt;/code> request includes a &lt;code>secretRef&lt;/code>
then the CSI driver can make use of the same and connect to the storage cluster to
perform the cluster operations.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>To enable this functionality from this version of Kubernetes, SIG Storage have introduced
a new feature gate called &lt;code>CSINodeExpandSecret&lt;/code>. Once the feature gate is enabled
in the cluster, NodeExpandVolume requests can include a &lt;code>secretRef&lt;/code> field. The NodeExpandVolume request
is part of CSI; for example, in a request which has been sent from the Kubernetes
control plane to the CSI driver.&lt;/p>
&lt;p>As a cluster operator, you admin can specify these secrets as an opaque parameter in a StorageClass,
the same way that you can already specify other CSI secret data. The StorageClass needs to have some
CSI-specific parameters set. Here's an example of those parameters:&lt;/p>
&lt;pre tabindex="0">&lt;code>csi.storage.k8s.io/node-expand-secret-name: test-secret
csi.storage.k8s.io/node-expand-secret-namespace: default
&lt;/code>&lt;/pre>&lt;p>If feature gates are enabled and storage class carries the above secret configuration,
the CSI provisioner receives the credentials from the Secret as part of the NodeExpansion request.&lt;/p>
&lt;p>CSI volumes that require secrets for online expansion will have NodeExpandSecretRef
field set. If not set, the NodeExpandVolume CSI RPC call will be made without a secret.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Enable the &lt;code>CSINodeExpandSecret&lt;/code> feature gate (please refer to
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">Feature Gates&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a Secret, and then a StorageClass that uses that Secret.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Here's an example manifest for a Secret that holds credentials:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">data&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">stringData&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">username&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admin&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">password&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>t0p-Secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here's an example manifest for a StorageClass that refers to those credentials:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-blockstorage-sc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/node-expand-secret-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-secret &lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># the name of the Secret&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/node-expand-secret-namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default &lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># the namespace that the Secret is in&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">reclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeBindingMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Immediate&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example-output">Example output&lt;/h2>
&lt;p>If the PersistentVolumeClaim (PVC) was created successfully, you can see that
configuration within the &lt;code>spec.csi&lt;/code> field of the PersistentVolume (look for
&lt;code>spec.csi.nodeExpandSecretRef&lt;/code>).
Check that it worked by running &lt;code>kubectl get persistentvolume &amp;lt;pv_name&amp;gt; -o yaml&lt;/code>.
You should see something like.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolume&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">pv.kubernetes.io/provisioned-by&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2022-08-26T15:14:07Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">finalizers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- kubernetes.io/pv-protection&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;420263&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">uid&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>6fa824d7-8a06-4e0c-b722-d3f897dcbd65&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">capacity&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>6Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;419862&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">uid&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>95eb531a-d675-49f6-940b-9bc3fde83eb0&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeExpandSecretRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeAttributes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage.kubernetes.io/csiProvisionerIdentity&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1648042783218-8081&lt;/span>-blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>e21c7809-aabb-11ec-917a-2e2e254eb4cf&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeAffinity&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">required&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeSelectorTerms&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>topology.hostpath.csi/node&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>In&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- racknode01&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-blockstorage-sc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">phase&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Bound&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you then trigger online storage expansion, the kubelet passes the appropriate credentials
to the CSI driver, by loading that Secret and passing the data to the storage driver.&lt;/p>
&lt;p>Here's an example debug log:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0330 03:29:51.966241 1 server.go:101] GRPC call: /csi.v1.Node/NodeExpandVolume
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0330 03:29:51.966261 1 server.go:105] GRPC request: {&amp;#34;capacity_range&amp;#34;:{&amp;#34;required_bytes&amp;#34;:7516192768},&amp;#34;secrets&amp;#34;:&amp;#34;***stripped***&amp;#34;,&amp;#34;staging_target_path&amp;#34;:&amp;#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&amp;#34;,&amp;#34;volume_capability&amp;#34;:{&amp;#34;AccessType&amp;#34;:{&amp;#34;Mount&amp;#34;:{}},&amp;#34;access_mode&amp;#34;:{&amp;#34;mode&amp;#34;:7}},&amp;#34;volume_id&amp;#34;:&amp;#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&amp;#34;,&amp;#34;volume_path&amp;#34;:&amp;#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&amp;#34;}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0330 03:29:51.966360 1 nodeserver.go:459] req:volume_id:&amp;#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&amp;#34; volume_path:&amp;#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&amp;#34; capacity_range:&amp;lt;required_bytes:7516192768 &amp;gt; staging_target_path:&amp;#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&amp;#34; volume_capability:&amp;lt;mount:&amp;lt;&amp;gt; access_mode:&amp;lt;mode:SINGLE_NODE_MULTI_WRITER &amp;gt; &amp;gt; secrets:&amp;lt;key:&amp;#34;XXXXXX&amp;#34; value:&amp;#34;XXXXX&amp;#34; &amp;gt; secrets:&amp;lt;key:&amp;#34;XXXXX&amp;#34; value:&amp;#34;XXXXXX&amp;#34; &amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-future">The future&lt;/h2>
&lt;p>As this feature is still in alpha, Kubernetes Storage SIG expect to update or get feedback from CSI driver
authors with more tests and implementation. The community plans to eventually
promote the feature to Beta in upcoming releases.&lt;/p>
&lt;h2 id="get-involved-or-learn-more">Get involved or learn more?&lt;/h2>
&lt;p>The enhancement proposal includes lots of detail about the history and technical
implementation of this feature.&lt;/p>
&lt;p>To learn more about StorageClass based dynamic provisioning in Kubernetes, please refer to
&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes&lt;/a> and
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes&lt;/a>.&lt;/p>
&lt;p>Please get involved by joining the Kubernetes
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md">Storage SIG&lt;/a>
(Special Interest Group) to help us enhance this feature.
There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Local Storage Capacity Isolation Reaches GA</title><link>https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/</link><pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jing Xu (Google)&lt;/p>
&lt;p>Local ephemeral storage capacity isolation was introduced as a alpha feature in Kubernetes 1.7 and it went beta in 1.9. With Kubernetes 1.25 we are excited to announce general availability(GA) of this feature.&lt;/p>
&lt;p>Pods use ephemeral local storage for scratch space, caching, and logs. The lifetime of local ephemeral storage does not extend beyond the life of the individual pod. It is exposed to pods using the container’s writable layer, logs directory, and &lt;code>EmptyDir&lt;/code> volumes. Before this feature was introduced, there were issues related to the lack of local storage accounting and isolation, such as Pods not knowing how much local storage is available and being unable to request guaranteed local storage. Local storage is a best-effort resource and pods can be evicted due to other pods filling the local storage.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local storage capacity isolation feature&lt;/a> allows users to manage local ephemeral storage in the same way as managing CPU and memory. It provides support for capacity isolation of shared storage between pods, such that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of shared storage exceeds that limit. It also allows setting ephemeral storage requests for resource reservation. The limits and requests for shared &lt;code>ephemeral-storage&lt;/code> are similar to those for memory and CPU consumption.&lt;/p>
&lt;h3 id="how-to-use-local-storage-capacity-isolation">How to use local storage capacity isolation&lt;/h3>
&lt;p>A typical configuration for local ephemeral storage is to place all different kinds of ephemeral local data (emptyDir volumes, writeable layers, container images, logs) into one filesystem. Typically, both /var/lib/kubelet and /var/log are on the system's root filesystem. If users configure the local storage in different ways, kubelet might not be able to correctly measure disk usage and use this feature.&lt;/p>
&lt;h4 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage&lt;/h4>
&lt;p>You can specify &lt;code>ephemeral-storage&lt;/code> for managing local ephemeral storage. Each container of a Pod can specify either or both of the following:&lt;/p>
&lt;ul>
&lt;li>&lt;code>spec.containers[].resources.limits.ephemeral-storage&lt;/code>&lt;/li>
&lt;li>&lt;code>spec.containers[].resources.requests.ephemeral-storage&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>In the following example, the Pod has two containers. The first container has a request of 8GiB of local ephemeral storage and a limit of 12GiB. The second container requests 2GiB of local storage, but no limit setting. Therefore, the Pod requests a total of 10GiB (8GiB+2GiB) of local ephemeral storage and enforces a limit of 12GiB of local ephemeral storage. It also sets emptyDir sizeLimit to 5GiB. With this setting in pod spec, it will affect how the scheduler makes a decision on scheduling pods and also how kubelet evict pods.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>frontend&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>app&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>images.my-company.example/app:v4&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;8Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;12Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ephemeral&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/tmp&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>log-aggregator&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>images.my-company.example/log-aggregator:v6&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ephemeral&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/tmp&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ephemeral&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">emptyDir&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{}&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sizeLimit&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>5Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>First of all, the scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node. In this case, the pod can be assigned to a node only if its available ephemeral storage (allocatable resource) has more than 10GiB.&lt;/p>
&lt;p>Secondly, at container level, since one of the container sets resource limit, kubelet eviction manager will measure the disk usage of this container and evict the pod if the storage usage of the first container exceeds its limit (12GiB). At pod level, kubelet works out an overall Pod storage limit by
adding up the limits of all the containers in that Pod. In this case, the total storage usage at pod level is the sum of the disk usage from all containers plus the Pod's &lt;code>emptyDir&lt;/code> volumes. If this total usage exceeds the overall Pod storage limit (12GiB), then the kubelet also marks the Pod for eviction.&lt;/p>
&lt;p>Last, in this example, emptyDir volume sets its sizeLimit to 5Gi. It means that if this pod's emptyDir used up more local storage than 5GiB, the pod will be evicted from the node.&lt;/p>
&lt;h4 id="setting-resource-quota-and-limitrange-for-local-ephemeral-storage">Setting resource quota and limitRange for local ephemeral storage&lt;/h4>
&lt;p>This feature adds two more resource quotas for storage. The request and limit set constraints on the total requests/limits of all containers’ in a namespace.&lt;/p>
&lt;ul>
&lt;li>&lt;code>requests.ephemeral-storage&lt;/code>&lt;/li>
&lt;li>&lt;code>limits.ephemeral-storage&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceQuota&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage-resources&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hard&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests.ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits.ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;20Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Similar to CPU and memory, admin could use LimitRange to set default container’s local storage request/limit, and/or minimum/maximum resource constraints for a namespace.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LimitRange&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage-limit-range&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">default&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">defaultRequest&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>5Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Container&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Also, ephemeral-storage may be specified to reserve for kubelet or system. example, &lt;code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=10Gi][,][pid=1000] --kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=5Gi][,][pid=1000]&lt;/code>. If your cluster node root disk capacity is 100Gi, after setting system-reserved and kube-reserved value, the available allocatable ephemeral storage would become 85Gi. The schedule will use this information to assign pods based on request and allocatable resources from each node. The eviction manager will also use allocatable resource to determine pod eviction. See more details from &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons&lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p>
&lt;p>We offer a huge thank you to all the contributors in &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage SIG&lt;/a> and CSI community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p>&lt;ul>&lt;li>Benjamin Elder (&lt;a href=https://github.com/BenTheElder>BenTheElder&lt;/a>)&lt;/li>&lt;li>Michelle Au (&lt;a href=https://github.com/msau42>msau42&lt;/a>)&lt;/li>&lt;li>Tim Hockin (&lt;a href=https://github.com/thockin>thockin&lt;/a>)&lt;/li>&lt;li>Jordan Liggitt (&lt;a href=https://github.com/liggitt>liggitt&lt;/a>)&lt;/li>&lt;li>Xing Yang (&lt;a href=https://github.com/xing-yang>xing-yang&lt;/a>)&lt;/li>&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Two Features for Apps Rollouts Graduate to Stable</title><link>https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/</link><pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ravi Gudimetla (Apple), Filip Křepinský (Red Hat), Maciej Szulik (Red Hat)&lt;/p>
&lt;p>This blog describes the two features namely &lt;code>minReadySeconds&lt;/code> for StatefulSets and &lt;code>maxSurge&lt;/code> for DaemonSets that SIG Apps is happy to graduate to stable in Kubernetes 1.25.&lt;/p>
&lt;p>Specifying &lt;code>minReadySeconds&lt;/code> slows down a rollout of a StatefulSet, when using a &lt;code>RollingUpdate&lt;/code> value in &lt;code>.spec.updateStrategy&lt;/code> field, by waiting for each pod for a desired time.
This time can be used for initializing the pod (e.g. warming up the cache) or as a delay before acknowledging the pod.&lt;/p>
&lt;p>&lt;code>maxSurge&lt;/code> allows a DaemonSet workload to run multiple instances of the same pod on a node during a rollout when using a &lt;code>RollingUpdate&lt;/code> value in &lt;code>.spec.updateStrategy&lt;/code> field.
This helps to minimize the downtime of the DaemonSet for consumers.&lt;/p>
&lt;p>These features were already available in a Deployment and other workloads. This graduation helps to align this functionality across the workloads.&lt;/p>
&lt;h2 id="what-problems-do-these-features-solve">What problems do these features solve?&lt;/h2>
&lt;h3 id="solved-problem-statefulset-minreadyseconds">minReadySeconds for StatefulSets&lt;/h3>
&lt;p>&lt;code>minReadySeconds&lt;/code> ensures that the StatefulSet workload is &lt;code>Ready&lt;/code> for the given number of seconds before reporting the
pod as &lt;code>Available&lt;/code>. The notion of being &lt;code>Ready&lt;/code> and &lt;code>Available&lt;/code> is quite important for workloads. For example, some workloads, like Prometheus with multiple instances of Alertmanager, should be considered &lt;code>Available&lt;/code> only when the Alertmanager's state transfer is complete. &lt;code>minReadySeconds&lt;/code> also helps when using loadbalancers with cloud providers. Since the pod should be &lt;code>Ready&lt;/code> for the given number of seconds, it provides buffer time to prevent killing pods in rotation before new pods show up.&lt;/p>
&lt;h3 id="how-use-daemonset-maxsurge">maxSurge for DaemonSets&lt;/h3>
&lt;p>Kubernetes system-level components like CNI, CSI are typically run as DaemonSets. These components can have impact on the availability of the workloads if those DaemonSets go down momentarily during the upgrades. The feature allows DaemonSet pods to temporarily increase their number, thereby ensuring zero-downtime for the DaemonSets.&lt;/p>
&lt;p>Please note that the usage of &lt;code>hostPort&lt;/code> in conjunction with &lt;code>maxSurge&lt;/code> in DaemonSets is not allowed as DaemonSet pods are tied to a single node and two active pods cannot share the same port on the same node.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;h3 id="how-does-statefulset-minreadyseconds-work">minReadySeconds for StatefulSets&lt;/h3>
&lt;p>The StatefulSet controller watches for the StatefulSet pods and counts how long a particular pod has been in the &lt;code>Running&lt;/code> state, if this value is greater than or equal to the time specified in &lt;code>.spec.minReadySeconds&lt;/code> field of the StatefulSet, the StatefulSet controller updates the &lt;code>AvailableReplicas&lt;/code> field in the StatefulSet's status.&lt;/p>
&lt;h3 id="how-does-daemonset-maxsurge-work">maxSurge for DaemonSets&lt;/h3>
&lt;p>The DaemonSet controller creates the additional pods (above the desired number resulting from DaemonSet spec) based on the value given in &lt;code>.spec.strategy.rollingUpdate.maxSurge&lt;/code>. The additional pods would run on the same node where the old DaemonSet pod is running till the old pod gets killed.&lt;/p>
&lt;ul>
&lt;li>The default value is 0.&lt;/li>
&lt;li>The value cannot be &lt;code>0&lt;/code> when &lt;code>MaxUnavailable&lt;/code> is 0.&lt;/li>
&lt;li>The value can be specified either as an absolute number of pods, or a percentage (rounded up) of desired pods.&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;h3 id="how-use-statefulset-minreadyseconds">minReadySeconds for StatefulSets&lt;/h3>
&lt;p>Specify a value for &lt;code>minReadySeconds&lt;/code> for any StatefulSet and check if pods are available or not by inspecting
&lt;code>AvailableReplicas&lt;/code> field using:&lt;/p>
&lt;p>&lt;code>kubectl get statefulset/&amp;lt;name_of_the_statefulset&amp;gt; -o yaml&lt;/code>&lt;/p>
&lt;p>Please note that the default value of &lt;code>minReadySeconds&lt;/code> is 0.&lt;/p>
&lt;h3 id="how-use-daemonset-maxsurge">maxSurge for DaemonSets&lt;/h3>
&lt;p>Specify a value for &lt;code>.spec.updateStrategy.rollingUpdate.maxSurge&lt;/code> and set &lt;code>.spec.updateStrategy.rollingUpdate.maxUnavailable&lt;/code> to &lt;code>0&lt;/code>.&lt;/p>
&lt;p>Then observe a faster rollout and higher number of pods running at the same time in the next rollout.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl rollout restart daemonset &amp;lt;name_of_the_daemonset&amp;gt;
kubectl get pods -w
&lt;/code>&lt;/pre>&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;h3 id="learn-more-statefulset-minreadyseconds">minReadySeconds for StatefulSets&lt;/h3>
&lt;ul>
&lt;li>Documentation: &lt;a href="https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds">https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds&lt;/a>&lt;/li>
&lt;li>KEP: &lt;a href="https://github.com/kubernetes/enhancements/issues/2599">https://github.com/kubernetes/enhancements/issues/2599&lt;/a>&lt;/li>
&lt;li>API Changes: &lt;a href="https://github.com/kubernetes/kubernetes/pull/100842">https://github.com/kubernetes/kubernetes/pull/100842&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="learn-more-daemonset-maxsurge">maxSurge for DaemonSets&lt;/h3>
&lt;ul>
&lt;li>Documentation: &lt;a href="https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/">https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/&lt;/a>&lt;/li>
&lt;li>KEP: &lt;a href="https://github.com/kubernetes/enhancements/issues/1591">https://github.com/kubernetes/enhancements/issues/1591&lt;/a>&lt;/li>
&lt;li>API Changes: &lt;a href="https://github.com/kubernetes/kubernetes/pull/96375">https://github.com/kubernetes/kubernetes/pull/96375&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Please reach out to us on &lt;a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps&lt;/a> channel on Slack, or through the SIG Apps mailing list &lt;a href="https://groups.google.com/g/kubernetes-sig-apps">kubernetes-sig-apps@googlegroups.com&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: PodHasNetwork Condition for Pods</title><link>https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/</link><pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong>
Deep Debroy (Apple)&lt;/p>
&lt;p>Kubernetes 1.25 introduces Alpha support for a new kubelet-managed pod condition
in the status field of a pod: &lt;code>PodHasNetwork&lt;/code>. The kubelet, for a worker node,
will use the &lt;code>PodHasNetwork&lt;/code> condition to accurately surface the initialization
state of a pod from the perspective of pod sandbox creation and network
configuration by a container runtime (typically in coordination with CNI
plugins). The kubelet starts to pull container images and start individual
containers (including init containers) after the status of the &lt;code>PodHasNetwork&lt;/code>
condition is set to &lt;code>&amp;quot;True&amp;quot;&lt;/code>. Metrics collection services that report latency of
pod initialization from a cluster infrastructural perspective (i.e. agnostic of
per container characteristics like image size or payload) can utilize the
&lt;code>PodHasNetwork&lt;/code> condition to accurately generate Service Level Indicators
(SLIs). Certain operators or controllers that manage underlying pods may utilize
the &lt;code>PodHasNetwork&lt;/code> condition to optimize the set of actions performed when pods
repeatedly fail to come up.&lt;/p>
&lt;h3 id="how-is-this-different-from-the-existing-initialized-condition-reported-for-pods">How is this different from the existing Initialized condition reported for pods?&lt;/h3>
&lt;p>The kubelet sets the status of the existing &lt;code>Initialized&lt;/code> condition reported in
the status field of a pod depending on the presence of init containers in a pod.&lt;/p>
&lt;p>If a pod specifies init containers, the status of the &lt;code>Initialized&lt;/code> condition in
the pod status will not be set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> until all init containers for the pod
have succeeded. However, init containers, configured by users, may have errors
(payload crashing, invalid image, etc) and the number of init containers
configured in a pod may vary across different workloads. Therefore,
cluster-wide, infrastructural SLIs around pod initialization cannot depend on
the &lt;code>Initialized&lt;/code> condition of pods.&lt;/p>
&lt;p>If a pod does not specify init containers, the status of the &lt;code>Initialized&lt;/code>
condition in the pod status is set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> very early in the lifecycle of
the pod. This occurs before the kubelet initiates any pod runtime sandbox
creation and network configuration steps. As a result, a pod without init
containers will report the status of the &lt;code>Initialized&lt;/code> condition as &lt;code>&amp;quot;True&amp;quot;&lt;/code>
even if the container runtime is not able to successfully initialize the pod
sandbox environment.&lt;/p>
&lt;p>Relative to either situation above, the &lt;code>PodHasNetwork&lt;/code> condition surfaces more
accurate data around when the pod runtime sandbox was initialized with
networking configured so that the kubelet can proceed to launch user-configured
containers (including init containers) in the pod.&lt;/p>
&lt;h3 id="special-cases">Special Cases&lt;/h3>
&lt;p>If a pod specifies &lt;code>hostNetwork&lt;/code> as &lt;code>&amp;quot;True&amp;quot;&lt;/code>, the &lt;code>PodHasNetwork&lt;/code> condition is
set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> based on successful creation of the pod sandbox while the
network configuration state of the pod sandbox is ignored. This is because the
CRI implementation typically skips any pod sandbox network configuration when
&lt;code>hostNetwork&lt;/code> is set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> for a pod.&lt;/p>
&lt;p>A node agent may dynamically re-configure network interface(s) for a pod by
watching changes in pod annotations that specify additional networking
configuration (e.g. &lt;code>k8s.v1.cni.cncf.io/networks&lt;/code>). Dynamic updates of pod
networking configuration after the pod sandbox is initialized by Kubelet (in
coordination with a container runtime) are not reflected by the &lt;code>PodHasNetwork&lt;/code>
condition.&lt;/p>
&lt;h3 id="try-out-the-podhasnetwork-condition-for-pods">Try out the PodHasNetwork condition for pods&lt;/h3>
&lt;p>In order to have the kubelet report the &lt;code>PodHasNetwork&lt;/code> condition in the status
field of a pod, please enable the &lt;code>PodHasNetworkCondition&lt;/code> feature gate on the
kubelet.&lt;/p>
&lt;p>For a pod whose runtime sandbox has been successfully created and has networking
configured, the kubelet will report the &lt;code>PodHasNetwork&lt;/code> condition with status set to &lt;code>&amp;quot;True&amp;quot;&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl describe pod nginx1
Name: nginx1
Namespace: default
...
Conditions:
Type Status
PodHasNetwork True
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;p>For a pod whose runtime sandbox has not been created yet (and networking not
configured either), the kubelet will report the &lt;code>PodHasNetwork&lt;/code> condition with
status set to &lt;code>&amp;quot;False&amp;quot;&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl describe pod nginx2
Name: nginx2
Namespace: default
...
Conditions:
Type Status
PodHasNetwork False
Initialized True
Ready False
ContainersReady False
PodScheduled True
&lt;/code>&lt;/pre>&lt;h3 id="what-s-next">What’s next?&lt;/h3>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the
reporting of the &lt;code>PodHasNetwork&lt;/code> condition to Beta in 1.26 or 1.27.&lt;/p>
&lt;h3 id="how-can-i-learn-more">How can I learn more?&lt;/h3>
&lt;p>Please check out the
&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">documentation&lt;/a> for the
&lt;code>PodHasNetwork&lt;/code> condition to learn more about it and how it fits in relation to
other pod conditions.&lt;/p>
&lt;h3 id="how-to-get-involved">How to get involved?&lt;/h3>
&lt;p>This feature is driven by the SIG Node community. Please join us to connect with
the community and share your ideas and feedback around the above feature and
beyond. We look forward to hearing from you!&lt;/p>
&lt;h3 id="acknowledgements">Acknowledgements&lt;/h3>
&lt;p>We want to thank the following people for their insightful and helpful reviews
of the KEP and PRs around this feature: Derek Carr (@derekwaynecarr), Mrunal
Patel (@mrunalp), Dawn Chen (@dchen1107), Qiutong Song (@qiutongs), Ruiwen Zhao
(@ruiwen-zhao), Tim Bannister (@sftim), Danielle Lancashire (@endocrimes) and
Agam Dua (@agamdua).&lt;/p></description></item><item><title>Blog: Announcing the Auto-refreshing Official Kubernetes CVE Feed</title><link>https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Pushkar Joglekar (VMware)&lt;/p>
&lt;p>A long-standing request from the Kubernetes community has been to have a
programmatic way for end users to keep track of Kubernetes security issues
(also called &amp;quot;CVEs&amp;quot;, after the database that tracks public security issues across
different products and vendors). Accompanying the release of Kubernetes v1.25,
we are excited to announce availability of such
a &lt;a href="https://kubernetes.io/docs/reference/issues-security/official-cve-feed/">feed&lt;/a> as an &lt;code>alpha&lt;/code>
feature. This blog will cover the background and scope of this new service.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>With the growing number of eyes on Kubernetes, the number of CVEs related to
Kubernetes have increased. Although most CVEs that directly, indirectly, or
transitively impact Kubernetes are regularly fixed, there is no single place for
the end users of Kubernetes to programmatically subscribe or pull the data of
fixed CVEs. Current options are either broken or incomplete.&lt;/p>
&lt;h2 id="scope">Scope&lt;/h2>
&lt;h3 id="what-this-does">What This Does&lt;/h3>
&lt;p>Create a periodically auto-refreshing, human and machine-readable list of
official Kubernetes CVEs&lt;/p>
&lt;h3 id="what-this-doesn-t-do">What This Doesn't Do&lt;/h3>
&lt;ul>
&lt;li>Triage and vulnerability disclosure will continue to be done by SRC (Security
Response Committee).&lt;/li>
&lt;li>Listing CVEs that are identified in build time dependencies and container
images are out of scope.&lt;/li>
&lt;li>Only official CVEs announced by the Kubernetes SRC will be published in the
feed.&lt;/li>
&lt;/ul>
&lt;h3 id="who-it-s-for">Who It's For&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>End Users&lt;/strong>: Persons or teams who &lt;em>use&lt;/em> Kubernetes to deploy applications
they own&lt;/li>
&lt;li>&lt;strong>Platform Providers&lt;/strong>: Persons or teams who &lt;em>manage&lt;/em> Kubernetes clusters&lt;/li>
&lt;li>&lt;strong>Maintainers&lt;/strong>: Persons or teams who &lt;em>create&lt;/em> and &lt;em>support&lt;/em> Kubernetes
releases through their work in Kubernetes Community - via various Special
Interest Groups and Committees.&lt;/li>
&lt;/ul>
&lt;h2 id="implementation-details">Implementation Details&lt;/h2>
&lt;p>A supporting
&lt;a href="https://kubernetes.dev/blog/2022/09/12/k8s-cve-feed-alpha/">contributor blog&lt;/a>
was published that describes in depth on how this CVE feed was implemented to
ensure the feed was reasonably protected against tampering and was automatically
updated after a new CVE was announced.&lt;/p>
&lt;h2 id="what-s-next">What's Next?&lt;/h2>
&lt;p>In order to graduate this feature, SIG Security
is gathering feedback from end users who are using this alpha feed.&lt;/p>
&lt;p>So in order to improve the feed in future Kubernetes Releases, if you have any
feedback, please let us know by adding a comment to
this &lt;a href="https://github.com/kubernetes/sig-security/issues/1">tracking issue&lt;/a> or
let us know on
&lt;a href="https://kubernetes.slack.com/archives/C01CUSVMHPY">#sig-security-tooling&lt;/a>
Kubernetes Slack channel.
(Join &lt;a href="https://slack.k8s.io">Kubernetes Slack here&lt;/a>)&lt;/p>
&lt;p>&lt;em>A special shout out and massive thanks to Neha Lohia
&lt;a href="https://github.com/nehalohia27">(@nehalohia27)&lt;/a> and Tim
Bannister &lt;a href="https://github.com/sftim">(@sftim)&lt;/a> for their stellar collaboration
for many months from &amp;quot;ideation to implementation&amp;quot; of this feature.&lt;/em>&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: KMS V2 Improvements</title><link>https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Anish Ramasekar, Rita Zhang, Mo Khan, and Xander Grzywinski (Microsoft)&lt;/p>
&lt;p>With Kubernetes v1.25, SIG Auth is introducing a new &lt;code>v2alpha1&lt;/code> version of the Key Management Service (KMS) API. There are a lot of improvements in the works, and we're excited to be able to start down the path of a new and improved KMS!&lt;/p>
&lt;h2 id="what-is-kms">What is KMS?&lt;/h2>
&lt;p>One of the first things to consider when securing a Kubernetes cluster is encrypting persisted API data at rest. KMS provides an interface for a provider to utilize a key stored in an external key service to perform this encryption.&lt;/p>
&lt;p>Encryption at rest using KMS v1 has been a feature of Kubernetes since version v1.10, and is currently in beta as of version v1.12.&lt;/p>
&lt;h2 id="what-s-new-in-v2alpha1">What’s new in &lt;code>v2alpha1&lt;/code>?&lt;/h2>
&lt;p>While the original v1 implementation has been successful in helping Kubernetes users encrypt etcd data, it did fall short in a few key ways:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Performance:&lt;/strong> When starting a cluster, all resources are serially fetched and decrypted to fill the &lt;code>kube-apiserver&lt;/code> cache. When using a KMS plugin, this can cause slow startup times due to the large number of requests made to the remote vault. In addition, there is the potential to hit API rate limits on external key services depending on how many encrypted resources exist in the cluster.&lt;/li>
&lt;li>&lt;strong>Key Rotation:&lt;/strong> With KMS v1, rotation of a key-encrypting key is a manual and error-prone process. It can be difficult to determine what encryption keys are in-use on a cluster.&lt;/li>
&lt;li>&lt;strong>Health Check &amp;amp; Status:&lt;/strong> Before the KMS v2 API, the &lt;code>kube-apiserver&lt;/code> was forced to make encrypt and decrypt calls as a proxy to determine if the KMS plugin is healthy. With cloud services these operations usually cost actual money with cloud service. Whatever the cost, those operations on their own do not provide a holistic view of the service's health.&lt;/li>
&lt;li>&lt;strong>Observability:&lt;/strong> Without some kind of trace ID, it's has been difficult to correlate events found in the various logs across &lt;code>kube-apiserver&lt;/code>, KMS, and KMS plugins.&lt;/li>
&lt;/ol>
&lt;p>The KMS v2 enhancement attempts to address all of these shortcomings, though not all planned features are implemented in the initial alpha release. Here are the improvements that arrived in Kubernetes v1.25:&lt;/p>
&lt;ol>
&lt;li>Support for KMS plugins that use a key hierarchy to reduce network requests made to the remote vault. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#key-hierachy">design details for how a KMS plugin can leverage key hierarchy&lt;/a>.&lt;/li>
&lt;li>Extra metadata is now tracked to allow a KMS plugin to communicate what key it is currently using with the &lt;code>kube-apiserver&lt;/code>, allowing for rotation without API server restart. Data stored in etcd follows a more standard proto format to allow external tools to observe its state. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#metadata">details for metadata&lt;/a>.&lt;/li>
&lt;li>A dedicated status API is used to communicate the health of the KMS plugin with the API server. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#status-api">details for status API&lt;/a>.&lt;/li>
&lt;li>To improve observability, a new &lt;code>UID&lt;/code> field is included in &lt;code>EncryptRequest&lt;/code> and &lt;code>DecryptRequest&lt;/code> of the v2 API. The UID is generated for each envelope operation. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#Observability">details for observability&lt;/a>.&lt;/li>
&lt;/ol>
&lt;h3 id="sequence-diagram">Sequence Diagram&lt;/h3>
&lt;h4 id="encrypt-request">Encrypt Request&lt;/h4>
&lt;!-- source - https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O)](https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O -->
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/images/blog/2022-09-09-kubernetes-1.25-kms-v2-improvements/kubernetes-1.25-encryption.svg"
alt="Sequence diagram for KMSv2 Encrypt"/>
&lt;/figure>
&lt;h4 id="decrypt-request">Decrypt Request&lt;/h4>
&lt;!-- source - https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O)](https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O ](https://mermaid.ink/img/pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk)](https://mermaid.live/edit#pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk -->
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/images/blog/2022-09-09-kubernetes-1.25-kms-v2-improvements/kubernetes-1.25-decryption.svg"
alt="Sequence diagram for KMSv2 Decrypt"/>
&lt;/figure>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>For Kubernetes v1.26, we expect to ship another alpha version. As of right now, the alpha API will be ready to be used by KMS plugin authors. We hope to include a reference plugin implementation with the next release, and you'll be able to try out the feature at that time.&lt;/p>
&lt;p>You can learn more about KMS v2 by reading &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption&lt;/a>. You can also follow along on the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/3299-kms-v2-improvements/#readme">KEP&lt;/a> to track progress across the coming Kubernetes releases.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved&lt;/h2>
&lt;p>If you are interested in getting involved in the development of this feature or would like to share feedback, please reach out on the &lt;a href="https://kubernetes.slack.com/archives/C03035EH4VB">#sig-auth-kms-dev&lt;/a> channel on Kubernetes Slack.&lt;/p>
&lt;p>You are also welcome to join the bi-weekly &lt;a href="https://github.com/kubernetes/community/blob/master/sig-auth/README.md#meetings">SIG Auth meetings&lt;/a>, held every-other Wednesday.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>This feature has been an effort driven by contributors from several different companies. We would like to extend a huge thank you to everyone that contributed their time and effort to help make this possible.&lt;/p></description></item><item><title>Blog: Kubernetes’s IPTables Chains Are Not API</title><link>https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Dan Winship (Red Hat)&lt;/p>
&lt;p>Some Kubernetes components (such as kubelet and kube-proxy) create
iptables chains and rules as part of their operation. These chains
were never intended to be part of any Kubernetes API/ABI guarantees,
but some external components nonetheless make use of some of them (in
particular, using &lt;code>KUBE-MARK-MASQ&lt;/code> to mark packets as needing to be
masqueraded).&lt;/p>
&lt;p>As a part of the v1.25 release, SIG Network made this declaration
explicit: that (with one exception), the iptables chains that
Kubernetes creates are intended only for Kubernetes’s own internal
use, and third-party components should not assume that Kubernetes will
create any specific iptables chains, or that those chains will contain
any specific rules if they do exist.&lt;/p>
&lt;p>Then, in future releases, as part of &lt;a href="https://github.com/kubernetes/enhancements/issues/3178">KEP-3178&lt;/a>, we will begin phasing
out certain chains that Kubernetes itself no longer needs. Components
outside of Kubernetes itself that make use of &lt;code>KUBE-MARK-MASQ&lt;/code>,
&lt;code>KUBE-MARK-DROP&lt;/code>, or other Kubernetes-generated iptables chains should
start migrating away from them now.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>In addition to various service-specific iptables chains, kube-proxy
creates certain general-purpose iptables chains that it uses as part
of service proxying. In the past, kubelet also used iptables for a few
features (such as setting up &lt;code>hostPort&lt;/code> mapping for pods) and so it
also redundantly created some of the same chains.&lt;/p>
&lt;p>However, with &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">the removal of dockershim&lt;/a> in Kubernetes in 1.24,
kubelet now no longer ever uses any iptables rules for its own
purposes; the things that it used to use iptables for are now always
the responsibility of the container runtime or the network plugin, and
there is no reason for kubelet to be creating any iptables rules.&lt;/p>
&lt;p>Meanwhile, although &lt;code>iptables&lt;/code> is still the default kube-proxy backend
on Linux, it is unlikely to remain the default forever, since the
associated command-line tools and kernel APIs are essentially
deprecated, and no longer receiving improvements. (RHEL 9
&lt;a href="https://access.redhat.com/solutions/6739041">logs a warning&lt;/a> if you use the iptables API, even via
&lt;code>iptables-nft&lt;/code>.)&lt;/p>
&lt;p>Although as of Kubernetes 1.25 iptables kube-proxy remains popular,
and kubelet continues to create the iptables rules that it
historically created (despite no longer &lt;em>using&lt;/em> them), third party
software cannot assume that core Kubernetes components will keep
creating these rules in the future.&lt;/p>
&lt;h2 id="upcoming-changes">Upcoming changes&lt;/h2>
&lt;p>Starting a few releases from now, kubelet will no longer create the
following iptables chains in the &lt;code>nat&lt;/code> table:&lt;/p>
&lt;ul>
&lt;li>&lt;code>KUBE-MARK-DROP&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-MARK-MASQ&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-POSTROUTING&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Additionally, the &lt;code>KUBE-FIREWALL&lt;/code> chain in the &lt;code>filter&lt;/code> table will no
longer have the functionality currently associated with
&lt;code>KUBE-MARK-DROP&lt;/code> (and it may eventually go away entirely).&lt;/p>
&lt;p>This change will be phased in via the &lt;code>IPTablesOwnershipCleanup&lt;/code>
feature gate. That feature gate is available and can be manually
enabled for testing in Kubernetes 1.25. The current plan is that it
will become enabled-by-default in Kubernetes 1.27, though this may be
delayed to a later release. (It will not happen sooner than Kubernetes
1.27.)&lt;/p>
&lt;h2 id="what-to-do-if-you-use-kubernetes-s-iptables-chains">What to do if you use Kubernetes’s iptables chains&lt;/h2>
&lt;p>(Although the discussion below focuses on short-term fixes that are
still based on iptables, you should probably also start thinking about
eventually migrating to nftables or another API).&lt;/p>
&lt;h3 id="use-case-kube-mark-masq">If you use &lt;code>KUBE-MARK-MASQ&lt;/code>...&lt;/h3>
&lt;p>If you are making use of the &lt;code>KUBE-MARK-MASQ&lt;/code> chain to cause packets
to be masqueraded, you have two options: (1) rewrite your rules to use
&lt;code>-j MASQUERADE&lt;/code> directly, (2) create your own alternative “mark for
masquerade” chain.&lt;/p>
&lt;p>The reason kube-proxy uses &lt;code>KUBE-MARK-MASQ&lt;/code> is because there are lots
of cases where it needs to call both &lt;code>-j DNAT&lt;/code> and &lt;code>-j MASQUERADE&lt;/code> on
a packet, but it’s not possible to do both of those at the same time
in iptables; &lt;code>DNAT&lt;/code> must be called from the &lt;code>PREROUTING&lt;/code> (or &lt;code>OUTPUT&lt;/code>)
chain (because it potentially changes where the packet will be routed
to) while &lt;code>MASQUERADE&lt;/code> must be called from &lt;code>POSTROUTING&lt;/code> (because the
masqueraded source IP that it picks depends on what the final routing
decision was).&lt;/p>
&lt;p>In theory, kube-proxy could have one set of rules to match packets in
&lt;code>PREROUTING&lt;/code>/&lt;code>OUTPUT&lt;/code> and call &lt;code>-j DNAT&lt;/code>, and then have a second set
of rules to match the same packets in &lt;code>POSTROUTING&lt;/code> and call &lt;code>-j MASQUERADE&lt;/code>. But instead, for efficiency, it only matches them once,
during &lt;code>PREROUTING&lt;/code>/&lt;code>OUTPUT&lt;/code>, at which point it calls &lt;code>-j DNAT&lt;/code> and
then calls &lt;code>-j KUBE-MARK-MASQ&lt;/code> to set a bit on the kernel packet mark
as a reminder to itself. Then later, during &lt;code>POSTROUTING&lt;/code>, it has a
single rule that matches all previously-marked packets, and calls &lt;code>-j MASQUERADE&lt;/code> on them.&lt;/p>
&lt;p>If you have &lt;em>a lot&lt;/em> of rules where you need to apply both DNAT and
masquerading to the same packets like kube-proxy does, then you may
want a similar arrangement. But in many cases, components that use
&lt;code>KUBE-MARK-MASQ&lt;/code> are only doing it because they copied kube-proxy’s
behavior without understanding why kube-proxy was doing it that way.
Many of these components could easily be rewritten to just use
separate DNAT and masquerade rules. (In cases where no DNAT is
occurring then there is even less point to using &lt;code>KUBE-MARK-MASQ&lt;/code>;
just move your rules from &lt;code>PREROUTING&lt;/code> to &lt;code>POSTROUTING&lt;/code> and call &lt;code>-j MASQUERADE&lt;/code> directly.)&lt;/p>
&lt;h3 id="use-case-kube-mark-drop">If you use &lt;code>KUBE-MARK-DROP&lt;/code>...&lt;/h3>
&lt;p>The rationale for &lt;code>KUBE-MARK-DROP&lt;/code> is similar to the rationale for
&lt;code>KUBE-MARK-MASQ&lt;/code>: kube-proxy wanted to make packet-dropping decisions
alongside other decisions in the &lt;code>nat&lt;/code> &lt;code>KUBE-SERVICES&lt;/code> chain, but you
can only call &lt;code>-j DROP&lt;/code> from the &lt;code>filter&lt;/code> table. So instead, it uses
&lt;code>KUBE-MARK-DROP&lt;/code> to mark packets to be dropped later on.&lt;/p>
&lt;p>In general, the approach for removing a dependency on &lt;code>KUBE-MARK-DROP&lt;/code>
is the same as for removing a dependency on &lt;code>KUBE-MARK-MASQ&lt;/code>. In
kube-proxy’s case, it is actually quite easy to replace the usage of
&lt;code>KUBE-MARK-DROP&lt;/code> in the &lt;code>nat&lt;/code> table with direct calls to &lt;code>DROP&lt;/code> in the
&lt;code>filter&lt;/code> table, because there are no complicated interactions between
DNAT rules and drop rules, and so the drop rules can simply be moved
from &lt;code>nat&lt;/code> to &lt;code>filter&lt;/code>.&lt;/p>
&lt;p>In more complicated cases, it might be necessary to “re-match” the
same packets in both &lt;code>nat&lt;/code> and &lt;code>filter&lt;/code>.&lt;/p>
&lt;h3 id="use-case-iptables-mode">If you use Kubelet’s iptables rules to figure out &lt;code>iptables-legacy&lt;/code> vs &lt;code>iptables-nft&lt;/code>...&lt;/h3>
&lt;p>Components that manipulate host-network-namespace iptables rules from
inside a container need some way to figure out whether the host is
using the old &lt;code>iptables-legacy&lt;/code> binaries or the newer &lt;code>iptables-nft&lt;/code>
binaries (which talk to a different kernel API underneath).&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/iptables-wrappers/">&lt;code>iptables-wrappers&lt;/code>&lt;/a> module provides a way for such components to
autodetect the system iptables mode, but in the past it did this by
assuming that Kubelet will have created “a bunch” of iptables rules
before any containers start, and so it can guess which mode the
iptables binaries in the host filesystem are using by seeing which
mode has more rules defined.&lt;/p>
&lt;p>In future releases, Kubelet will no longer create many iptables rules,
so heuristics based on counting the number of rules present may fail.&lt;/p>
&lt;p>However, as of 1.24, Kubelet always creates a chain named
&lt;code>KUBE-IPTABLES-HINT&lt;/code> in the &lt;code>mangle&lt;/code> table of whichever iptables
subsystem it is using. Components can now look for this specific chain
to know which iptables subsystem Kubelet (and thus, presumably, the
rest of the system) is using.&lt;/p>
&lt;p>(Additionally, since Kubernetes 1.17, kubelet has created a chain
called &lt;code>KUBE-KUBELET-CANARY&lt;/code> in the &lt;code>mangle&lt;/code> table. While this chain
may go away in the future, it will of course still be there in older
releases, so in any recent version of Kubernetes, at least one of
&lt;code>KUBE-IPTABLES-HINT&lt;/code> or &lt;code>KUBE-KUBELET-CANARY&lt;/code> will be present.)&lt;/p>
&lt;p>The &lt;code>iptables-wrappers&lt;/code> package has &lt;a href="https://github.com/kubernetes-sigs/iptables-wrappers/pull/3">already been updated&lt;/a> with this new
heuristic, so if you were previously using that, you can rebuild your
container images with an updated version of that.&lt;/p>
&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>The project to clean up iptables chain ownership and deprecate the old
chains is tracked by &lt;a href="https://github.com/kubernetes/enhancements/issues/3178">KEP-3178&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing COSI: Object Storage Management using Kubernetes APIs</title><link>https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/</link><pubDate>Fri, 02 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sidhartha Mani (&lt;a href="https://min.io">Minio, Inc&lt;/a>)&lt;/p>
&lt;p>This article introduces the Container Object Storage Interface (COSI), a standard for provisioning and consuming object storage in Kubernetes. It is an alpha feature in Kubernetes v1.25.&lt;/p>
&lt;p>File and block storage are treated as first class citizens in the Kubernetes ecosystem via &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface&lt;/a> (CSI). Workloads using CSI volumes enjoy the benefits of portability across vendors and across Kubernetes clusters without the need to change application manifests. An equivalent standard does not exist for Object storage.&lt;/p>
&lt;p>Object storage has been rising in popularity in recent years as an alternative form of storage to filesystems and block devices. Object storage paradigm promotes disaggregation of compute and storage. This is done by making data available over the network, rather than locally. Disaggregated architectures allow compute workloads to be stateless, which consequently makes them easier to manage, scale and automate.&lt;/p>
&lt;h2 id="cosi">COSI&lt;/h2>
&lt;p>COSI aims to standardize consumption of object storage to provide the following benefits:&lt;/p>
&lt;ul>
&lt;li>Kubernetes Native - Use the Kubernetes API to provision, configure and manage buckets&lt;/li>
&lt;li>Self Service - A clear delineation between administration and operations (DevOps) to enable self-service capability for DevOps personnel&lt;/li>
&lt;li>Portability - Vendor neutrality enabled through portability across Kubernetes Clusters and across Object Storage vendors&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Portability across vendors is only possible when both vendors support a common datapath-API. Eg. it is possible to port from AWS S3 to Ceph, or AWS S3 to MinIO and back as they all use S3 API. In contrast, it is not possible to port from AWS S3 and Google Cloud’s GCS or vice versa.&lt;/em>&lt;/p>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;p>COSI is made up of three components:&lt;/p>
&lt;ul>
&lt;li>COSI Controller Manager&lt;/li>
&lt;li>COSI Sidecar&lt;/li>
&lt;li>COSI Driver&lt;/li>
&lt;/ul>
&lt;p>The COSI Controller Manager acts as the main controller that processes changes to COSI API objects. It is responsible for fielding requests for bucket creation, updates, deletion and access management. One instance of the controller manager is required per kubernetes cluster. Only one is needed even if multiple object storage providers are used in the cluster.&lt;/p>
&lt;p>The COSI Sidecar acts as a translator between COSI API requests and vendor-specific COSI Drivers. This component uses a standardized gRPC protocol that vendor drivers are expected to satisfy.&lt;/p>
&lt;p>The COSI Driver is the vendor specific component that receives requests from the sidecar and calls the appropriate vendor APIs to create buckets, manage their lifecycle and manage access to them.&lt;/p>
&lt;h2 id="api">API&lt;/h2>
&lt;p>The COSI API is centered around buckets, since bucket is the unit abstraction for object storage. COSI defines three Kubernetes APIs aimed at managing them&lt;/p>
&lt;ul>
&lt;li>Bucket&lt;/li>
&lt;li>BucketClass&lt;/li>
&lt;li>BucketClaim&lt;/li>
&lt;/ul>
&lt;p>In addition, two more APIs for managing access to buckets are also defined:&lt;/p>
&lt;ul>
&lt;li>BucketAccess&lt;/li>
&lt;li>BucketAccessClass&lt;/li>
&lt;/ul>
&lt;p>In a nutshell, Bucket and BucketClaim can be considered to be similar to PersistentVolume and PersistentVolumeClaim respectively. The BucketClass’ counterpart in the file/block device world is StorageClass.&lt;/p>
&lt;p>Since Object Storage is always authenticated, and over the network, access credentials are required to access buckets. The two APIs, namely, BucketAccess and BucketAccessClass are used to denote access credentials and policies for authentication. More info about these APIs can be found in the official COSI proposal - &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support">https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support&lt;/a>&lt;/p>
&lt;h2 id="self-service">Self-Service&lt;/h2>
&lt;p>Other than providing kubernetes-API driven bucket management, COSI also aims to empower DevOps personnel to provision and manage buckets on their own, without admin intervention. This, further enabling dev teams to realize faster turn-around times and faster time-to-market.&lt;/p>
&lt;p>COSI achieves this by dividing bucket provisioning steps among two different stakeholders, namely the administrator (admin), and the cluster operator. The administrator will be responsible for setting broad policies and limits on how buckets are provisioned, and how access is obtained for them. The cluster operator will be free to create and utilize buckets within the limits set by the admin.&lt;/p>
&lt;p>For example, a cluster operator could use an admin policy could be used to restrict maximum provisioned capacity to 100GB, and developers would be allowed to create buckets and store data upto that limit. Similarly for access credentials, admins would be able to restrict who can access which buckets, and developers would be able to access all the buckets available to them.&lt;/p>
&lt;h2 id="portability">Portability&lt;/h2>
&lt;p>The third goal of COSI is to achieve vendor neutrality for bucket management. COSI enables two kinds of portability:&lt;/p>
&lt;ul>
&lt;li>Cross Cluster&lt;/li>
&lt;li>Cross Provider&lt;/li>
&lt;/ul>
&lt;p>Cross Cluster portability is allowing buckets provisioned in one cluster to be available in another cluster. This is only valid when the object storage backend itself is accessible from both clusters.&lt;/p>
&lt;p>Cross-provider portability is about allowing organizations or teams to move from one object storage provider to another seamlessly, and without requiring changes to application definitions (PodTemplates, StatefulSets, Deployment and so on). This is only possible if the source and destination providers use the same data.&lt;/p>
&lt;p>&lt;em>COSI does not handle data migration as it is outside of its scope. In case porting between providers requires data to be migrated as well, then other measures need to be taken to ensure data availability.&lt;/em>&lt;/p>
&lt;h2 id="what-s-next">What’s next&lt;/h2>
&lt;p>The amazing sig-storage-cosi community has worked hard to bring the COSI standard to alpha status. We are looking forward to onboarding a lot of vendors to write COSI drivers and become COSI compatible!&lt;/p>
&lt;p>We want to add more authentication mechanisms for COSI buckets, we are designing advanced bucket sharing primitives, multi-cluster bucket management and much more. Lots of great ideas and opportunities ahead!&lt;/p>
&lt;p>Stay tuned for what comes next, and if you have any questions, comments or suggestions&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="https://kubernetes.slack.com/archives/C017EGC1C6N">Slack:#sig-storage-cosi&lt;/a>&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/614261834?pwd=Sk1USmtjR2t0MUdjTGVZeVVEV1BPQT09">Zoom meeting&lt;/a>, every Thursday at 10:00 Pacific Time&lt;/li>
&lt;li>Participate in the &lt;a href="https://github.com/kubernetes/enhancements/pull/2813">bucket API proposal PR&lt;/a> to add your ideas, suggestions and more.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.25: cgroup v2 graduates to GA</title><link>https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong>: David Porter (Google), Mrunal Patel (Red Hat)&lt;/p>
&lt;p>Kubernetes 1.25 brings cgroup v2 to GA (general availability), letting the
&lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">kubelet&lt;/a> use the latest container resource
management capabilities.&lt;/p>
&lt;h2 id="what-are-cgroups">What are cgroups?&lt;/h2>
&lt;p>Effective &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">resource management&lt;/a> is a
critical aspect of Kubernetes. This involves managing the finite resources in
your nodes, such as CPU, memory, and storage.&lt;/p>
&lt;p>&lt;em>cgroups&lt;/em> are a Linux kernel capability that establish resource management
functionality like limiting CPU usage or setting memory limits for running
processes.&lt;/p>
&lt;p>When you use the resource management capabilities in Kubernetes, such as configuring
&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">requests and limits for Pods and containers&lt;/a>,
Kubernetes uses cgroups to enforce your resource requests and limits.&lt;/p>
&lt;p>The Linux kernel offers two versions of cgroups: cgroup v1 and cgroup v2.&lt;/p>
&lt;h2 id="what-is-cgroup-v2">What is cgroup v2?&lt;/h2>
&lt;p>cgroup v2 is the latest version of the Linux cgroup API. cgroup v2 provides a
unified control system with enhanced resource management capabilities.&lt;/p>
&lt;p>cgroup v2 has been in development in the Linux Kernel since 2016 and in recent
years has matured across the container ecosystem. With Kubernetes 1.25, cgroup
v2 support has graduated to general availability.&lt;/p>
&lt;p>Many recent releases of Linux distributions have switched over to cgroup v2 by
default so it's important that Kubernetes continues to work well on these new
updated distros.&lt;/p>
&lt;p>cgroup v2 offers several improvements over cgroup v1, such as the following:&lt;/p>
&lt;ul>
&lt;li>Single unified hierarchy design in API&lt;/li>
&lt;li>Safer sub-tree delegation to containers&lt;/li>
&lt;li>Newer features like &lt;a href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information&lt;/a>&lt;/li>
&lt;li>Enhanced resource allocation management and isolation across multiple resources
&lt;ul>
&lt;li>Unified accounting for different types of memory allocations (network and kernel memory, etc)&lt;/li>
&lt;li>Accounting for non-immediate resource changes such as page cache write backs&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Some Kubernetes features exclusively use cgroup v2 for enhanced resource
management and isolation. For example,
the &lt;a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/">MemoryQoS feature&lt;/a> improves
memory utilization and relies on cgroup v2 functionality to enable it. New
resource management features in the kubelet will also take advantage of the new
cgroup v2 features moving forward.&lt;/p>
&lt;h2 id="how-do-you-use-cgroup-v2">How do you use cgroup v2?&lt;/h2>
&lt;p>Many Linux distributions are switching to cgroup v2 by default; you might start
using it the next time you update the Linux version of your control plane and
nodes!&lt;/p>
&lt;p>Using a Linux distribution that uses cgroup v2 by default is the recommended
method. Some of the popular Linux distributions that use cgroup v2 include the
following:&lt;/p>
&lt;ul>
&lt;li>Container Optimized OS (since M97)&lt;/li>
&lt;li>Ubuntu (since 21.10)&lt;/li>
&lt;li>Debian GNU/Linux (since Debian 11 Bullseye)&lt;/li>
&lt;li>Fedora (since 31)&lt;/li>
&lt;li>Arch Linux (since April 2021)&lt;/li>
&lt;li>RHEL and RHEL-like distributions (since 9)&lt;/li>
&lt;/ul>
&lt;p>To check if your distribution uses cgroup v2 by default,
refer to &lt;a href="https://kubernetes.io/docs/concepts/architecture/cgroups/#check-cgroup-version">Check your cgroup version&lt;/a> or
consult your distribution's documentation.&lt;/p>
&lt;p>If you're using a managed Kubernetes offering, consult your provider to
determine how they're adopting cgroup v2, and whether you need to take action.&lt;/p>
&lt;p>To use cgroup v2 with Kubernetes, you must meet the following requirements:&lt;/p>
&lt;ul>
&lt;li>Your Linux distribution enables cgroup v2 on kernel version 5.8 or later&lt;/li>
&lt;li>Your container runtime supports cgroup v2. For example:
&lt;ul>
&lt;li>&lt;a href="https://containerd.io/">containerd&lt;/a> v1.4 or later&lt;/li>
&lt;li>&lt;a href="https://cri-o.io/">cri-o&lt;/a> v1.20 or later&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The kubelet and the container runtime are configured to use the &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The kubelet and container runtime use a &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#cgroup-drivers">cgroup driver&lt;/a>
to set cgroup parameters. When using cgroup v2, it's strongly recommended that both
the kubelet and your container runtime use the
&lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver&lt;/a>,
so that there's a single cgroup manager on the system. To configure the kubelet
and the container runtime to use the driver, refer to the
&lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver documentation&lt;/a>.&lt;/p>
&lt;h2 id="migrate-to-cgroup-v2">Migrate to cgroup v2&lt;/h2>
&lt;p>When you run Kubernetes with a Linux distribution that enables cgroup v2, the
kubelet should automatically adapt without any additional configuration
required, as long as you meet the requirements.&lt;/p>
&lt;p>In most cases, you won't see a difference in the user experience when you
switch to using cgroup v2 unless your users access the cgroup file system
directly.&lt;/p>
&lt;p>If you have applications that access the cgroup file system directly, either on
the node or from inside a container, you must update the applications to use
the cgroup v2 API instead of the cgroup v1 API.&lt;/p>
&lt;p>Scenarios in which you might need to update to cgroup v2 include the following:&lt;/p>
&lt;ul>
&lt;li>If you run third-party monitoring and security agents that depend on the cgroup file system, update the
agents to versions that support cgroup v2.&lt;/li>
&lt;li>If you run &lt;a href="https://github.com/google/cadvisor">cAdvisor&lt;/a> as a stand-alone
DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.&lt;/li>
&lt;li>If you deploy Java applications with the JDK, prefer to use JDK 11.0.16 and
later or JDK 15 and later, which &lt;a href="https://bugs.openjdk.org/browse/JDK-8230305">fully support cgroup v2&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="learn-more">Learn more&lt;/h2>
&lt;ul>
&lt;li>Read the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">Kubernetes cgroup v2 documentation&lt;/a>&lt;/li>
&lt;li>Read the enhancement proposal, &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2254-cgroup-v2/README.md">KEP 2254&lt;/a>&lt;/li>
&lt;li>Learn more about
&lt;a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups&lt;/a> on Linux Manual Pages
and &lt;a href="https://docs.kernel.org/admin-guide/cgroup-v2.html">cgroup v2&lt;/a> on the Linux Kernel documentation&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved">Get involved&lt;/h2>
&lt;p>Your feedback is always welcome! SIG Node meets regularly and are available in
the &lt;code>#sig-node&lt;/code> channel in the Kubernetes &lt;a href="https://slack.k8s.io/">Slack&lt;/a>, or
using the SIG &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">mailing list&lt;/a>.&lt;/p>
&lt;p>cgroup v2 has had a long journey and is a great example of open source
community collaboration across the industry because it required work across the
stack, from the Linux Kernel to systemd to various container runtimes, and (of
course) Kubernetes.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We would like to thank &lt;a href="https://github.com/giuseppe">Giuseppe Scrivano&lt;/a> who
initiated cgroup v2 support in Kubernetes, and reviews and leadership from the
SIG Node community including chairs &lt;a href="https://github.com/dchen1107">Dawn Chen&lt;/a>
and &lt;a href="https://github.com/derekwaynecarr">Derek Carr&lt;/a>.&lt;/p>
&lt;p>We'd also like to thank the maintainers of container runtimes like Docker,
containerd and CRI-O, and the maintainers of components like
&lt;a href="https://github.com/google/cadvisor">cAdvisor&lt;/a>
and &lt;a href="https://github.com/opencontainers/runc">runc, libcontainer&lt;/a>,
which underpin many container runtimes. Finally, this wouldn't have been
possible without support from systemd and upstream Linux Kernel maintainers.&lt;/p>
&lt;p>It's a team effort!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: CSI Inline Volumes have graduated to GA</title><link>https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jonathan Dobson (Red Hat)&lt;/p>
&lt;p>CSI Inline Volumes were introduced as an alpha feature in Kubernetes 1.15 and have been beta since 1.16. We are happy to announce that this feature has graduated to General Availability (GA) status in Kubernetes 1.25.&lt;/p>
&lt;p>CSI Inline Volumes are similar to other ephemeral volume types, such as &lt;code>configMap&lt;/code>, &lt;code>downwardAPI&lt;/code> and &lt;code>secret&lt;/code>. The important difference is that the storage is provided by a CSI driver, which allows the use of ephemeral storage provided by third-party vendors. The volume is defined as part of the pod spec and follows the lifecycle of the pod, meaning the volume is created once the pod is scheduled and destroyed when the pod is destroyed.&lt;/p>
&lt;h2 id="what-s-new-in-1-25">What's new in 1.25?&lt;/h2>
&lt;p>There are a couple of new bug fixes related to this feature in 1.25, and the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">CSIInlineVolume feature gate&lt;/a> has been locked to &lt;code>True&lt;/code> with the graduation to GA. There are no new API changes, so users of this feature during beta should not notice any significant changes aside from these bug fixes.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/89290">#89290 - CSI inline volumes should support fsGroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/79980">#79980 - CSI volume reconstruction does not work for ephemeral volumes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="when-to-use-this-feature">When to use this feature&lt;/h2>
&lt;p>CSI inline volumes are meant for simple local volumes that should follow the lifecycle of the pod. They may be useful for providing secrets, configuration data, or other special-purpose storage to the pod from a CSI driver.&lt;/p>
&lt;p>A CSI driver is not suitable for inline use when:&lt;/p>
&lt;ul>
&lt;li>The volume needs to persist longer than the lifecycle of a pod&lt;/li>
&lt;li>Volume snapshots, cloning, or volume expansion are required&lt;/li>
&lt;li>The CSI driver requires &lt;code>volumeAttributes&lt;/code> that should be restricted to an administrator&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use-this-feature">How to use this feature&lt;/h2>
&lt;p>In order to use this feature, the &lt;code>CSIDriver&lt;/code> spec must explicitly list &lt;code>Ephemeral&lt;/code> as one of the supported &lt;code>volumeLifecycleModes&lt;/code>. Here is a simple example from the &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver&lt;/a>.&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
name: secrets-store.csi.k8s.io
spec:
podInfoOnMount: true
attachRequired: false
volumeLifecycleModes:
- Ephemeral
&lt;/code>&lt;/pre>&lt;p>Any pod spec may then reference that CSI driver to create an inline volume, as in this example.&lt;/p>
&lt;pre tabindex="0">&lt;code>kind: Pod
apiVersion: v1
metadata:
name: my-csi-app-inline
spec:
containers:
- name: my-frontend
image: busybox
volumeMounts:
- name: secrets-store-inline
mountPath: &amp;#34;/mnt/secrets-store&amp;#34;
readOnly: true
command: [ &amp;#34;sleep&amp;#34;, &amp;#34;1000000&amp;#34; ]
volumes:
- name: secrets-store-inline
csi:
driver: secrets-store.csi.k8s.io
readOnly: true
volumeAttributes:
secretProviderClass: &amp;#34;my-provider&amp;#34;
&lt;/code>&lt;/pre>&lt;p>If the driver supports any volume attributes, you can provide these as part of the &lt;code>spec&lt;/code> for the Pod as well:&lt;/p>
&lt;pre tabindex="0">&lt;code> csi:
driver: block.csi.vendor.example
volumeAttributes:
foo: bar
&lt;/code>&lt;/pre>&lt;h2 id="example-use-cases">Example Use Cases&lt;/h2>
&lt;p>Two existing CSI drivers that support the &lt;code>Ephemeral&lt;/code> volume lifecycle mode are the Secrets Store CSI Driver and the Cert-Manager CSI Driver.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver&lt;/a> allows users to mount secrets from external secret stores into a pod as an inline volume. This can be useful when the secrets are stored in an external managed service or Vault instance.&lt;/p>
&lt;p>The &lt;a href="https://github.com/cert-manager/csi-driver">Cert-Manager CSI Driver&lt;/a> works along with &lt;a href="https://cert-manager.io/">cert-manager&lt;/a> to seamlessly request and mount certificate key pairs into a pod. This allows the certificates to be renewed and updated in the application pod automatically.&lt;/p>
&lt;h2 id="security-considerations">Security Considerations&lt;/h2>
&lt;p>Special consideration should be given to which CSI drivers may be used as inline volumes. &lt;code>volumeAttributes&lt;/code> are typically controlled through the &lt;code>StorageClass&lt;/code>, and may contain attributes that should remain restricted to the cluster administrator. Allowing a CSI driver to be used for inline ephmeral volumes means that any user with permission to create pods may also provide &lt;code>volumeAttributes&lt;/code> to the driver through a pod spec.&lt;/p>
&lt;p>Cluster administrators may choose to omit (or remove) &lt;code>Ephemeral&lt;/code> from &lt;code>volumeLifecycleModes&lt;/code> in the CSIDriver spec to prevent the driver from being used as an inline ephemeral volume, or use an &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook&lt;/a> to restrict how the driver is used.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>For more information on this feature, see:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">Kubernetes documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">CSI documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/596-csi-inline-volumes/README.md">KEP-596&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">Beta blog post for CSI Inline Volumes&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes v1.25: Pod Security Admission Controller in Stable</title><link>https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/</link><pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Allclair (Google), Sam Stoelinga (Google)&lt;/p>
&lt;p>The release of Kubernetes v1.25 marks a major milestone for Kubernetes out-of-the-box pod security
controls: Pod Security admission (PSA) graduated to stable, and Pod Security Policy (PSP) has been
removed.
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PSP was deprecated in Kubernetes v1.21&lt;/a>,
and no longer functions in Kubernetes v1.25 and later.&lt;/p>
&lt;p>The Pod Security admission controller replaces PodSecurityPolicy, making it easier to enforce predefined
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> by
simply adding a label to a namespace. The Pod Security Standards are maintained by the K8s
community, which means you automatically get updated security policies whenever new
security-impacting Kubernetes features are introduced.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since Beta?&lt;/h2>
&lt;p>Pod Security Admission hasn’t changed much since the Beta in Kubernetes v1.23. The focus has been on
improving the user experience, while continuing to maintain a high quality bar.&lt;/p>
&lt;h3 id="improved-violation-messages">Improved violation messages&lt;/h3>
&lt;p>We improved violation messages so that you get
&lt;a href="https://github.com/kubernetes/kubernetes/pull/107698">fewer duplicate messages&lt;/a>. For example,
instead of the following message when the Baseline and Restricted policies check the same
capability:&lt;/p>
&lt;pre tabindex="0">&lt;code>pods &amp;#34;admin-pod&amp;#34; is forbidden: violates PodSecurity &amp;#34;restricted:latest&amp;#34;: non-default capabilities (container &amp;#34;admin&amp;#34; must not include &amp;#34;SYS_ADMIN&amp;#34; in securityContext.capabilities.add), unrestricted capabilities (container &amp;#34;admin&amp;#34; must not include &amp;#34;SYS_ADMIN&amp;#34; in securityContext.capabilities.add)
&lt;/code>&lt;/pre>&lt;p>You get this message:&lt;/p>
&lt;pre tabindex="0">&lt;code>pods &amp;#34;admin-pod&amp;#34; is forbidden: violates PodSecurity &amp;#34;restricted:latest&amp;#34;: unrestricted capabilities (container &amp;#34;admin&amp;#34; must not include &amp;#34;SYS_ADMIN&amp;#34; in securityContext.capabilities.add)
&lt;/code>&lt;/pre>&lt;h3 id="improved-namespace-warnings">Improved namespace warnings&lt;/h3>
&lt;p>When you modify the &lt;code>enforce&lt;/code> Pod Security labels on a namespace, the Pod Security
admission controller checks all existing pods for
violations and surfaces a &lt;a href="https://kubernetes.io/blog/2020/09/03/warnings/">warning&lt;/a> if any are out of compliance. These
&lt;a href="https://github.com/kubernetes/kubernetes/pull/105889">warnings are now aggregated&lt;/a> for pods with
identical violations, making large namespaces with many replicas much more manageable. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>Warning: frontend-h23gf2: allowPrivilegeEscalation != false
Warning: myjob-g342hj (and 6 other pods): host namespaces, allowPrivilegeEscalation != false Warning: backend-j23h42 (and 1 other pod): non-default capabilities, unrestricted capabilities
&lt;/code>&lt;/pre>&lt;p>Additionally, when you apply a non-privileged label to a namespace that has been
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#exemptions">configured to be exempt&lt;/a>,
you will now get a warning alerting you to this fact:&lt;/p>
&lt;pre tabindex="0">&lt;code>Warning: namespace &amp;#39;kube-system&amp;#39; is exempt from Pod Security, and the policy (enforce=baseline:latest) will be ignored
&lt;/code>&lt;/pre>&lt;h3 id="changes-to-the-pod-security-standards">Changes to the Pod Security Standards&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>,
which Pod Security admission enforces, have been updated with support for the new Pod OS
field. In v1.25 and later, if you use the Restricted policy, the following Linux-specific restrictions will no
longer be required if you explicitly set the pod's &lt;code>.spec.os.name&lt;/code> field to &lt;code>windows&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Seccomp - The &lt;code>seccompProfile.type&lt;/code> field for Pod and container security contexts&lt;/li>
&lt;li>Privilege escalation - The &lt;code>allowPrivilegeEscalation&lt;/code> field on container security contexts&lt;/li>
&lt;li>Capabilities - The requirement to drop &lt;code>ALL&lt;/code> capabilities in the &lt;code>capabilities&lt;/code> field on containers&lt;/li>
&lt;/ul>
&lt;p>In Kubernetes v1.23 and earlier, the kubelet didn't enforce the Pod OS field.
If your cluster includes nodes running a v1.23 or older kubelet, you should explicitly
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces">pin Restricted policies&lt;/a>
to a version prior to v1.25.&lt;/p>
&lt;h2 id="migrating-from-podsecuritypolicy-to-the-pod-security-admission-controller">Migrating from PodSecurityPolicy to the Pod Security admission controller&lt;/h2>
&lt;p>For instructions to migrate from PodSecurityPolicy to the Pod Security admission controller, and
for help choosing a migration strategy, refer to the
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration guide&lt;/a>.
We're also developing a tool called
&lt;a href="https://github.com/kubernetes-sigs/pspmigrator">pspmigrator&lt;/a> to automate parts
of the migration process.&lt;/p>
&lt;p>We'll be talking about PSP migration in more detail at our upcoming KubeCon 2022 NA talk,
&lt;a href="https://sched.co/182Jx">&lt;em>Migrating from Pod Security Policy&lt;/em>&lt;/a>. Use the
&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/">KubeCon NA schedule&lt;/a>
to learn more.&lt;/p></description></item><item><title>Blog: PodSecurityPolicy: The Historical Context</title><link>https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/</link><pubDate>Tue, 23 Aug 2022 15:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Mahé Tardy (Quarkslab)&lt;/p>
&lt;p>The PodSecurityPolicy (PSP) admission controller has been removed, as of
Kubernetes v1.25. Its deprecation was announced and detailed in the blog post
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>,
published for the Kubernetes v1.21 release.&lt;/p>
&lt;p>This article aims to provide historical context on the birth and evolution of
PSP, explain why the feature never made it to stable, and show why it was
removed and replaced by Pod Security admission control.&lt;/p>
&lt;p>PodSecurityPolicy, like other specialized admission control plugins, provided
fine-grained permissions on specific fields concerning the pod security settings
as a built-in policy API. It acknowledged that cluster administrators and
cluster users are usually not the same people, and that creating workloads in
the form of a Pod or any resource that will create a Pod should not equal being
&amp;quot;root on the cluster&amp;quot;. It could also encourage best practices by configuring
more secure defaults through mutation and decoupling low-level Linux security
decisions from the deployment process.&lt;/p>
&lt;h2 id="the-birth-of-podsecuritypolicy">The birth of PodSecurityPolicy&lt;/h2>
&lt;p>PodSecurityPolicy originated from OpenShift's SecurityContextConstraints
(SCC) that were in the very first release of the Red Hat OpenShift Container Platform,
even before Kubernetes 1.0. PSP was a stripped-down version of the SCC.&lt;/p>
&lt;p>The origin of the creation of PodSecurityPolicy is difficult to track, notably
because it was mainly added before Kubernetes Enhancements Proposal (KEP)
process, when design proposals were still a thing. Indeed, the archive of the final
&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/auth/pod-security-policy.md">design proposal&lt;/a>
is still available. Nevertheless, a &lt;a href="https://github.com/kubernetes/enhancements/issues/5">KEP issue number five&lt;/a>
was created after the first pull requests were merged.&lt;/p>
&lt;p>Before adding the first piece of code that created PSP, two main pull
requests were merged into Kubernetes, a &lt;a href="https://github.com/kubernetes/kubernetes/pull/7343">&lt;code>SecurityContext&lt;/code> subresource&lt;/a>
that defined new fields on pods' containers, and the first iteration of the &lt;a href="https://github.com/kubernetes/kubernetes/pull/7101">ServiceAccount&lt;/a>
API.&lt;/p>
&lt;p>Kubernetes 1.0 was released on 10 July 2015 without any mechanism to restrict the
security context and sensitive options of workloads, other than an alpha-quality
SecurityContextDeny admission plugin (then known as &lt;code>scdeny&lt;/code>).
The &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny">SecurityContextDeny plugin&lt;/a>
is still in Kubernetes today (as an alpha feature) and creates an admission controller that
prevents the usage of some fields in the security context.&lt;/p>
&lt;p>The roots of the PodSecurityPolicy were added with
&lt;a href="https://github.com/kubernetes/kubernetes/pull/7893">the very first pull request on security policy&lt;/a>,
which added the design proposal with the new PSP object, based on the SCC (Security Context Constraints). It
was a long discussion of nine months, with back and forth from OpenShift's SCC,
many rebases, and the rename to PodSecurityPolicy that finally made it to
upstream Kubernetes in February 2016. Now that the PSP object
had been created, the next step was to add an admission controller that could enforce
these policies. The first step was to add the admission
&lt;a href="https://github.com/kubernetes/kubernetes/pull/7893#issuecomment-180410539">without taking into account the users or groups&lt;/a>.
A specific &lt;a href="https://github.com/kubernetes/kubernetes/issues/23217">issue to bring PodSecurityPolicy to a usable state&lt;/a>
was added to keep track of the progress and a first version of the admission
controller was merged in &lt;a href="https://github.com/kubernetes/kubernetes/pull/24600">pull request named PSP admission&lt;/a>
in May 2016. Then around two months later, Kubernetes 1.3 was released.&lt;/p>
&lt;p>Here is a timeline that recaps the main pull requests of the birth of the
PodSecurityPolicy and its admission controller with 1.0 and 1.3 releases as
reference points.&lt;/p>
&lt;figure>
&lt;img src="./timeline.svg"
alt="Timeline of the PodSecurityPolicy creation pull requests"/>
&lt;/figure>
&lt;p>After that, the PSP admission controller was enhanced by adding what was initially
left aside. &lt;a href="https://github.com/kubernetes/kubernetes/pull/33080">The authorization mechanism&lt;/a>,
merged in early November 2016 allowed administrators to use multiple policies
in a cluster to grant different levels of access for different types of users.
Later, a &lt;a href="https://github.com/kubernetes/kubernetes/pull/52849">pull request&lt;/a>
merged in October 2017 fixed &lt;a href="https://github.com/kubernetes/kubernetes/issues/36184">a design issue&lt;/a>
on ordering PodSecurityPolicies between mutating and alphabetical order, and continued to
build the PSP admission as we know it. After that, many improvements and fixes
followed to build the PodSecurityPolicy feature of recent Kubernetes releases.&lt;/p>
&lt;h2 id="the-rise-of-pod-security-admission">The rise of Pod Security Admission&lt;/h2>
&lt;p>Despite the crucial issue it was trying to solve, PodSecurityPolicy presented
some major flaws:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flawed authorization model&lt;/strong> - users can create a pod if they have the
&lt;strong>use&lt;/strong> verb on the PSP that allows that pod or the pod's service account has
the &lt;strong>use&lt;/strong> permission on the allowing PSP.&lt;/li>
&lt;li>&lt;strong>Difficult to roll out&lt;/strong> - PSP fail-closed. That is, in the absence of a policy,
all pods are denied. It mostly means that it cannot be enabled by default and
that users have to add PSPs for all workloads before enabling the feature,
thus providing no audit mode to discover which pods would not be allowed by
the new policy. The opt-in model also leads to insufficient test coverage and
frequent breakage due to cross-feature incompatibility. And unlike RBAC,
there was no strong culture of shipping PSP manifests with projects.&lt;/li>
&lt;li>&lt;strong>Inconsistent unbounded API&lt;/strong> - the API has grown with lots of
inconsistencies notably because of many requests for niche use cases: e.g.
labels, scheduling, fine-grained volume controls, etc. It has poor
composability with a weak prioritization model, leading to unexpected
mutation priority. It made it really difficult to combine PSP with other
third-party admission controllers.&lt;/li>
&lt;li>&lt;strong>Require security knowledge&lt;/strong> - effective usage still requires an
understanding of Linux security primitives. e.g. MustRunAsNonRoot +
AllowPrivilegeEscalation.&lt;/li>
&lt;/ul>
&lt;p>The experience with PodSecurityPolicy concluded that most users care for two or three
policies, which led to the creation of the &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>,
that define three policies:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Privileged&lt;/strong> - unrestricted policy.&lt;/li>
&lt;li>&lt;strong>Baseline&lt;/strong> - minimally restrictive policy, allowing the default pod
configuration.&lt;/li>
&lt;li>&lt;strong>Restricted&lt;/strong> - security best practice policy.&lt;/li>
&lt;/ul>
&lt;p>The replacement for PSP, the new &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>
is an in-tree, stable for Kubernetes v1.25, admission plugin to enforce these
standards at the namespace level. It makes it easier to enforce basic pod
security without deep security knowledge. For more sophisticated use cases, you
might need a third-party solution that can be easily combined with Pod Security
Admission.&lt;/p>
&lt;h2 id="what-s-next">What's next&lt;/h2>
&lt;p>For further details on the SIG Auth processes, covering PodSecurityPolicy removal and
creation of Pod Security admission, the
&lt;a href="https://www.youtube.com/watch?v=SFtHRmPuhEw">SIG auth update at KubeCon NA 2019&lt;/a>
and the &lt;a href="https://www.youtube.com/watch?v=HsRRmlTJpls">PodSecurityPolicy Replacement: Past, Present, and Future&lt;/a>
presentation at KubeCon NA 2021 records are available.&lt;/p>
&lt;p>Particularly on the PSP removal, the
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>
blog post is still accurate.&lt;/p>
&lt;p>And for the new Pod Security admission,
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">documentation is available&lt;/a>.
In addition, the blog post
&lt;a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">Kubernetes 1.23: Pod Security Graduates to Beta&lt;/a>
along with the KubeCon EU 2022 presentation
&lt;a href="https://www.youtube.com/watch?v=gcz5VsvOYmI">The Hitchhiker's Guide to Pod Security&lt;/a>
give great hands-on tutorials to learn.&lt;/p></description></item><item><title>Blog: Kubernetes v1.25: Combiner</title><link>https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">Kubernetes 1.25 Release Team&lt;/a>&lt;/p>
&lt;p>Announcing the release of Kubernetes v1.25!&lt;/p>
&lt;p>This release includes a total of 40 enhancements. Fifteen of those enhancements are entering Alpha, ten are graduating to Beta, and thirteen are graduating to Stable. We also have two features being deprecated or removed.&lt;/p>
&lt;h2 id="release-theme-and-logo">Release theme and logo&lt;/h2>
&lt;p>&lt;strong>Kubernetes 1.25: Combiner&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-08-23-kubernetes-1.25-release/kubernetes-1.25.png"
alt="Combiner logo"/>
&lt;/figure>
&lt;p>The theme for Kubernetes v1.25 is &lt;em>Combiner&lt;/em>.&lt;/p>
&lt;p>The Kubernetes project itself is made up of many, many individual components that, when combined, take the form of the project you see today. It is also built and maintained by many individuals, all of them with different skills, experiences, histories, and interests, who join forces not just as the release team but as the many SIGs that support the project and the community year-round.&lt;/p>
&lt;p>With this release, we wish to honor the collaborative, open spirit that takes us from isolated developers, writers, and users spread around the globe to a combined force capable of changing the world. Kubernetes v1.25 includes a staggering 40 enhancements, none of which would exist without the incredible power we have when we work together.&lt;/p>
&lt;p>Inspired by our release lead's son, Albert Song, Kubernetes v1.25 is named for each and every one of you, no matter how you choose to contribute your unique power to the combined force that becomes Kubernetes.&lt;/p>
&lt;h2 id="what-s-new-major-themes">What's New (Major Themes)&lt;/h2>
&lt;h3 id="pod-security-changes">PodSecurityPolicy is removed; Pod Security Admission graduates to Stable&lt;/h3>
&lt;p>PodSecurityPolicy was initially &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">deprecated in v1.21&lt;/a>, and with the release of v1.25, it has been removed. The updates required to improve its usability would have introduced breaking changes, so it became necessary to remove it in favor of a more friendly replacement. That replacement is &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>, which graduates to Stable with this release. If you are currently relying on PodSecurityPolicy, please follow the instructions for &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration to Pod Security Admission&lt;/a>.&lt;/p>
&lt;h3 id="ephemeral-containers-graduate-to-stable">Ephemeral Containers Graduate to Stable&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers&lt;/a> are containers that exist for only a limited time within an existing pod. This is particularly useful for troubleshooting when you need to examine another container but cannot use &lt;code>kubectl exec&lt;/code> because that container has crashed or its image lacks debugging utilities. Ephemeral containers graduated to Beta in Kubernetes v1.23, and with this release, the feature graduates to Stable.&lt;/p>
&lt;h3 id="support-for-cgroups-v2-graduates-to-stable">Support for cgroups v2 Graduates to Stable&lt;/h3>
&lt;p>It has been more than two years since the Linux kernel cgroups v2 API was declared stable. With some distributions now defaulting to this API, Kubernetes must support it to continue operating on those distributions. cgroups v2 offers several improvements over cgroups v1, for more information see the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">cgroups v2&lt;/a> documentation. While cgroups v1 will continue to be supported, this enhancement puts us in a position to be ready for its eventual deprecation and replacement.&lt;/p>
&lt;h3 id="improved-windows-support">Improved Windows support&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019">Performance dashboards&lt;/a> added support for Windows&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/51540">Unit tests&lt;/a> added support for Windows&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/108592">Conformance tests&lt;/a> added support for Windows&lt;/li>
&lt;li>New GitHub repository created for &lt;a href="https://github.com/kubernetes-sigs/windows-operational-readiness">Windows Operational Readiness&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="moved-container-registry-service-from-k8s-gcr-io-to-registry-k8s-io">Moved container registry service from k8s.gcr.io to registry.k8s.io&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/pull/109938">Moving container registry from k8s.gcr.io to registry.k8s.io&lt;/a> got merged. For more details, see the &lt;a href="https://github.com/kubernetes/k8s.io/wiki/New-Registry-url-for-Kubernetes-(registry.k8s.io)">wiki page&lt;/a>, &lt;a href="https://groups.google.com/a/kubernetes.io/g/dev/c/DYZYNQ_A6_c/m/oD9_Q8Q9AAAJ">announcement&lt;/a> was sent to the kubernetes development mailing list.&lt;/p>
&lt;h3 id="promoted-seccompdefault-to-beta">Promoted SeccompDefault to Beta&lt;/h3>
&lt;p>SeccompDefault promoted to beta, see the tutorial &lt;a href="https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">Restrict a Container's Syscalls with seccomp&lt;/a> for more details.&lt;/p>
&lt;h3 id="promoted-endport-in-network-policy-to-stable">Promoted endPort in Network Policy to Stable&lt;/h3>
&lt;p>Promoted &lt;code>endPort&lt;/code> in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports">Network Policy&lt;/a> to GA. Network Policy providers that support &lt;code>endPort&lt;/code> field now can use it to specify a range of ports to apply a Network Policy. Previously, each Network Policy could only target a single port.&lt;/p>
&lt;p>Please be aware that &lt;code>endPort&lt;/code> field &lt;strong>must be supported&lt;/strong> by the Network Policy provider. If your provider does not support &lt;code>endPort&lt;/code>, and this field is specified in a Network Policy, the Network Policy will be created covering only the port field (single port).&lt;/p>
&lt;h3 id="promoted-local-ephemeral-storage-capacity-isolation-to-stable">Promoted Local Ephemeral Storage Capacity Isolation to Stable&lt;/h3>
&lt;p>The &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/361-local-ephemeral-storage-isolation">Local Ephemeral Storage Capacity Isolation&lt;/a> feature moved to GA. This was introduced as alpha in 1.8, moved to beta in 1.10, and it is now a stable feature. It provides support for capacity isolation of local ephemeral storage between pods, such as &lt;code>EmptyDir&lt;/code>, so that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of local ephemeral storage exceeds that limit.&lt;/p>
&lt;h3 id="promoted-core-csi-migration-to-stable">Promoted core CSI Migration to Stable&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#quick-recap-what-is-csi-migration-and-why-migrate">CSI Migration&lt;/a> is an ongoing effort that SIG Storage has been working on for a few releases. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. The &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">core CSI Migration&lt;/a> feature moved to GA. CSI Migration for GCE PD and AWS EBS also moved to GA. CSI Migration for vSphere remains in beta (but is on by default). CSI Migration for Portworx moved to Beta (but is off-by-default).&lt;/p>
&lt;h3 id="promoted-csi-ephemeral-volume-to-stable">Promoted CSI Ephemeral Volume to Stable&lt;/h3>
&lt;p>The &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/596-csi-inline-volumes">CSI Ephemeral Volume&lt;/a> feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it moved to GA. This feature is used by some CSI drivers such as the &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">secret-store CSI driver&lt;/a>.&lt;/p>
&lt;h3 id="promoted-crd-validation-expression-language-to-beta">Promoted CRD Validation Expression Language to Beta&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md">CRD Validation Expression Language&lt;/a> is promoted to beta, which makes it possible to declare how custom resources are validated using the &lt;a href="https://github.com/google/cel-spec">Common Expression Language (CEL)&lt;/a>. Please see the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules&lt;/a> guide.&lt;/p>
&lt;h3 id="promoted-server-side-unknown-field-validation-to-beta">Promoted Server Side Unknown Field Validation to Beta&lt;/h3>
&lt;p>Promoted the &lt;code>ServerSideFieldValidation&lt;/code> feature gate to beta (on by default). This allows optionally triggering schema validation on the API server that errors when unknown fields are detected. This allows the removal of client-side validation from kubectl while maintaining the same core functionality of erroring out on requests that contain unknown or invalid fields.&lt;/p>
&lt;h3 id="introduced-kms-v2-api">Introduced KMS v2 API&lt;/h3>
&lt;p>Introduce KMS v2alpha1 API to add performance, rotation, and observability improvements. Encrypt data at rest (ie Kubernetes &lt;code>Secrets&lt;/code>) with DEK using AES-GCM instead of AES-CBC for kms data encryption. No user action is required. Reads with AES-GCM and AES-CBC will continue to be allowed. See the guide &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption&lt;/a> for more information.&lt;/p>
&lt;h3 id="kube-proxy-images-are-now-based-on-distroless-images">Kube-proxy images are now based on distroless images&lt;/h3>
&lt;p>In previous releases, kube-proxy container images were built using Debian as the base image. Starting with this release, the images are now built using &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a>. This change reduced image size by almost 50% and decreased the number of installed packages and files to only those strictly required for kube-proxy to do its job.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduations-to-stable">Graduations to Stable&lt;/h3>
&lt;p>This release includes a total of thirteen enhancements promoted to stable:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/277">Ephemeral Containers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/361">Local Ephemeral Storage Resource Management&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/596">CSI Ephemeral Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/625">CSI Migration - Core&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/785">Graduate the kube-scheduler ComponentConfig to GA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1487">CSI Migration - AWS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1488">CSI Migration - GCE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1591">DaemonSets Support MaxSurge&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2079">NetworkPolicy Port Range&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2254">cgroups v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">Pod Security Admission&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2599">Add &lt;code>minReadySeconds&lt;/code> to Statefulsets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2802">Identify Windows pods at API admission level authoritatively&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations-and-removals">Deprecations and Removals&lt;/h3>
&lt;p>Two features were &lt;a href="https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/">deprecated or removed&lt;/a> from Kubernetes with this release.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/5">PodSecurityPolicy is removed&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS plugin deprecated from available in-tree drivers&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>The complete details of the Kubernetes v1.25 release are available in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes v1.25 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.25.0">GitHub&lt;/a>.
To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local
Kubernetes clusters using containers as “nodes”, with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>.
You can also easily install 1.25 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that, when combined, make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.&lt;/p>
&lt;p>We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.25 release for our community. Every one of you had a part to play in building this, and you all executed beautifully. We would like to extend special thanks to our fearless release lead, Cici Huang, for all she did to guarantee we had what we needed to succeed.&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>Finleap Connect operates in a highly regulated environment. &lt;a href="https://www.cncf.io/case-studies/finleap-connect/">In 2019, they had five months to implement mutual TLS (mTLS) across all services in their clusters for their business code to comply with the new European PSD2 payment directive&lt;/a>.&lt;/li>
&lt;li>PNC sought to develop a way to ensure new code would meet security standards and audit compliance requirements automatically—replacing the cumbersome 30-day manual process they had in place. Using Knative, &lt;a href="https://www.cncf.io/case-studies/pnc-bank/">PNC developed internal tools to automatically check new code and changes to existing code&lt;/a>.&lt;/li>
&lt;li>Nexxiot needed highly-reliable, secure, performant, and cost efficient Kubernetes clusters. &lt;a href="https://www.cncf.io/case-studies/nexxiot/">They turned to Cilium as the CNI to lock down their clusters and enable resilient networking with reliable day two operations&lt;/a>.&lt;/li>
&lt;li>Because the process of creating cyber insurance policies is a complicated multi-step process, At-Bay sought to improve operations by using asynchronous message-based communication patterns/facilities. &lt;a href="https://www.cncf.io/case-studies/at-bay/">They determined that Dapr fulfilled its desired list of requirements and much more&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon North America 2022 will take place in Detroit, Michigan from 24 – 28 October 2022! You can find more information about the conference and registration on the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">event site&lt;/a>.&lt;/li>
&lt;li>KubeDay event series kicks off with KubeDay Japan on December 7! Register or submit a proposal on the &lt;a href="https://events.linuxfoundation.org/kubeday-japan/">event site&lt;/a>&lt;/li>
&lt;li>In the &lt;a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 Cloud Native Survey&lt;/a>, the CNCF saw record Kubernetes and container adoption. Take a look at the &lt;a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">results of the survey&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> project
aggregates a number of interesting data points related to the velocity of Kubernetes and various
sub-projects. This includes everything from individual contributions to the number of companies that
are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.25 release cycle, which &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">ran for 14 weeks&lt;/a> (May 23 to August 23), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;amp;var-metric=contributions">1065 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">1620 individuals&lt;/a>.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h2>
&lt;p>Join members of the Kubernetes v1.25 release team on Thursday September 22, 2022 10am – 11am PT to learn about
the major features of this release, as well as deprecations and removals to help plan for upgrades.
For more information and registration, visit the &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v125-release/">event page&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests.
Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> website&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for the latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault&lt;/a>.&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Spotlight on SIG Storage</title><link>https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Frederico Muñoz (SAS)&lt;/p>
&lt;p>Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes.&lt;/p>
&lt;p>Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage's umbrella.&lt;/p>
&lt;p>In this SIG Storage spotlight, &lt;a href="https://twitter.com/fredericomunoz">Frederico Muñoz&lt;/a> (Cloud &amp;amp; Architecture Lead at SAS) talked with &lt;a href="https://twitter.com/2000xyang">Xing Yang&lt;/a>, Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.&lt;/p>
&lt;h2 id="about-sig-storage">About SIG Storage&lt;/h2>
&lt;p>&lt;strong>Frederico (FSM)&lt;/strong>: Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage.&lt;/p>
&lt;p>&lt;strong>Xing Yang (XY)&lt;/strong>: I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">VolumeSnapshot&lt;/a> project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage. It needed contributors. So I volunteered to help. Then I worked with other community members to bring VolumeSnapshot to Alpha in K8s 1.12 release in 2018, Beta in K8s 1.17 in 2019, and eventually GA in 1.20 in 2020.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Reading the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/charter.md">SIG Storage charter&lt;/a> alone it’s clear that SIG Storage covers a lot of ground, could you describe how the SIG is organised?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: In SIG Storage, there are two Co-Chairs and two Tech Leads. Saad Ali from Google and myself are Co-Chairs. Michelle Au from Google and Jan Šafránek from Red Hat are Tech Leads.&lt;/p>
&lt;p>We have bi-weekly meetings where we go through features we are working on for each particular release, getting the statuses, making sure each feature has dev owners and reviewers working on it, and reminding people about the release deadlines, etc. More information on the SIG is on the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">community page&lt;/a>. People can also add PRs that need attention, design proposals that need discussion, and other topics to the meeting agenda doc. We will go over them after project tracking is done.&lt;/p>
&lt;p>We also have other regular meetings, i.e., CSI Implementation meeting, Object Bucket API design meeting, and one-off meetings for specific topics if needed. There is also a &lt;a href="https://github.com/kubernetes/community/blob/master/wg-data-protection/README.md">K8s Data Protection Workgroup&lt;/a> that is sponsored by SIG Storage and SIG Apps. SIG Storage owns or co-owns features that are being discussed at the Data Protection WG.&lt;/p>
&lt;h2 id="storage-and-kubernetes">Storage and Kubernetes&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>: Storage is such a foundational component in so many things, not least in Kubernetes: what do you think are the Kubernetes-specific challenges in terms of storage management?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: In Kubernetes, there are multiple components involved for a volume operation. For example, creating a Pod to use a PVC has multiple components involved. There are the Attach Detach Controller and the external-attacher working on attaching the PVC to the pod. There’s the Kubelet that works on mounting the PVC to the pod. Of course the CSI driver is involved as well. There could be race conditions sometimes when coordinating between multiple components.&lt;/p>
&lt;p>Another challenge is regarding core vs &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions&lt;/a> (CRD), not really storage specific. CRD is a great way to extend Kubernetes capabilities while not adding too much code to the Kubernetes core itself. However, this also means there are many external components that are needed when running a Kubernetes cluster.&lt;/p>
&lt;p>From the SIG Storage side, one most notable example is Volume Snapshot. Volume Snapshot APIs are defined as CRDs. API definitions and controllers are out-of-tree. There is a common snapshot controller and a snapshot validation webhook that should be deployed on the control plane, similar to how kube-controller-manager is deployed. Although Volume Snapshot is a CRD, it is a core feature of SIG Storage. It is recommended for the K8s cluster distros to deploy Volume Snapshot CRDs, the snapshot controller, and the snapshot validation webhook, however, most of the time we don’t see distros deploy them. So this becomes a problem for the storage vendors: now it becomes their responsibility to deploy these non-driver specific common components. This could cause conflicts if a customer wants to use more than one storage system and deploy more than one CSI driver.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Not only the complexity of a single storage system, you have to consider how they will be used together in Kubernetes?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Yes, there are many different storage systems that can provide storage to containers in Kubernetes. They don’t work the same way. It is challenging to find a solution that works for everyone.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Storage in Kubernetes also involves interacting with external solutions, perhaps more so than other parts of Kubernetes. Is this interaction with vendors and external providers challenging? Has it evolved with time in any way?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Yes, it is definitely challenging. Initially Kubernetes storage had in-tree volume plugin interfaces. Multiple storage vendors implemented in-tree interfaces and have volume plugins in the Kubernetes core code base. This caused lots of problems. If there is a bug in a volume plugin, it affects the entire Kubernetes code base. All volume plugins must be released together with Kubernetes. There was no flexibility if storage vendors need to fix a bug in their plugin or want to align with their own product release.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: That’s where CSI enters the game?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Exactly, then there comes &lt;a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface&lt;/a> (CSI). This is an industry standard trying to design common storage interfaces so that a storage vendor can write one plugin and have it work across a range of container orchestration systems (CO). Now Kubernetes is the main CO, but back when CSI just started, there were Docker, Mesos, Cloud Foundry, in addition to Kubernetes. CSI drivers are out-of-tree so bug fixes and releases can happen at their own pace.&lt;/p>
&lt;p>CSI is definitely a big improvement compared to in-tree volume plugins. Kubernetes implementation of CSI has been GA &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">since the 1.13 release&lt;/a>. It has come a long way. SIG Storage has been working on moving in-tree volume plugins to out-of-tree CSI drivers for several releases now.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Moving drivers away from the Kubernetes main tree and into CSI was an important improvement.&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: CSI interface is an improvement over the in-tree volume plugin interface, however, there are still challenges. There are lots of storage systems. Currently &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">there are more than 100 CSI drivers listed in CSI driver docs&lt;/a>. These storage systems are also very diverse. So it is difficult to design a common API that works for all. We introduced capabilities at CSI driver level, but we also have challenges when volumes provisioned by the same driver have different behaviors. The other day we just had a meeting discussing Per Volume CSI Driver Capabilities. We have a problem differentiating some CSI driver capabilities when the same driver supports both block and file volumes. We are going to have follow up meetings to discuss this problem.&lt;/p>
&lt;h2 id="ongoing-challenges">Ongoing challenges&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>: Specifically for the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">1.25 release&lt;/a> we can see that there are a relevant number of storage-related &lt;a href="https://bit.ly/k8s125-enhancements">KEPs&lt;/a> in the pipeline, would you say that this release is particularly important for the SIG?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: I wouldn’t say one release is more important than other releases. In any given release, we are working on a few very important things.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Indeed, but are there any 1.25 specific specificities and highlights you would like to point out though?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Yes. For the 1.25 release, I want to highlight the following:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI Migration&lt;/a> is an on-going effort that SIG Storage has been working on for a few releases now. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. There are 7 KEPs that we are targeting in 1.25 are related to CSI migration. There is one core KEP for the general CSI Migration feature. That is targeting GA in 1.25. CSI Migration for GCE PD and AWS EBS are targeting GA. CSI Migration for vSphere is targeting to have the feature gate on by default while staying in 1.25 that are in Beta. Ceph RBD and PortWorx are targeting Beta, with feature gate off by default. Ceph FS is targeting Alpha.&lt;/li>
&lt;li>The second one I want to highlight is &lt;a href="https://github.com/kubernetes-sigs/container-object-storage-interface-spec">COSI, the Container Object Storage Interface&lt;/a>. This is a sub-project under SIG Storage. COSI proposes object storage Kubernetes APIs to support orchestration of object store operations for Kubernetes workloads. It also introduces gRPC interfaces for object storage providers to write drivers to provision buckets. The COSI team has been working on this project for more than two years now. The COSI feature is targeting Alpha in 1.25. The KEP just got merged. The COSI team is working on updating the implementation based on the updated KEP.&lt;/li>
&lt;li>Another feature I want to mention is &lt;a href="https://github.com/kubernetes/enhancements/issues/596">CSI Ephemeral Volume&lt;/a> support. This feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it is now targeting GA in 1.25.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>FSM&lt;/strong>: If you had to single something out, what would be the most pressing areas the SIG is working on?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: CSI migration is definitely one area that the SIG has put in lots of effort and it has been on-going for multiple releases now. It involves work from multiple cloud providers and storage vendors as well.&lt;/p>
&lt;h2 id="community-involvement">Community involvement&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>: Kubernetes is a community-driven project. Any recommendation for anyone looking into getting involved in SIG Storage work? Where should they start?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Take a look at the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG Storage community page&lt;/a>, it has lots of information on how to get started. There are &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/annual-report-2021.md">SIG annual reports&lt;/a> that tell you what we did each year. Take a look at the Contributing guide. It has links to presentations that can help you get familiar with Kubernetes storage concepts.&lt;/p>
&lt;p>Join our &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">bi-weekly meetings on Thursdays&lt;/a>. Learn how the SIG operates and what we are working on for each release. Find a project that you are interested in and help out. As I mentioned earlier, I got started in SIG Storage by contributing to the Volume Snapshot project.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Any closing thoughts you would like to add?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: SIG Storage always welcomes new contributors. We need contributors to help with building new features, fixing bugs, doing code reviews, writing tests, monitoring test grid health, and improving documentation, etc.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Thank you so much for your time and insights into the workings of SIG Storage!&lt;/p></description></item><item><title>Blog: Stargazing, solutions and staycations: the Kubernetes 1.24 release interview</title><link>https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/</link><pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The Kubernetes project has participants from all around the globe. Some are friends, some are colleagues, and some are strangers. The one thing that unifies them, no matter their differences, are that they all have an interesting story. It is my pleasure to be the documentarian for the stories of the Kubernetes community in the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>. With every new Kubernetes release comes an interview with the release team lead, telling the story of that release, but also their own personal story.&lt;/p>
&lt;p>With 1.25 around the corner, &lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">the tradition continues&lt;/a> with a look back at the story of 1.24. That release was led by &lt;a href="https://twitter.com/jameslaverack">James Laverack&lt;/a> of Jetstack. &lt;a href="https://kubernetespodcast.com/episode/178-kubernetes-1.24/">James was on the podcast&lt;/a> in May, and while you can read his story below, if you can, please do listen to it in his own voice.&lt;/p>
&lt;p>Make sure you &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts&lt;/a>, so you hear all our stories from the cloud native community, including the story of 1.25 next week.&lt;/p>
&lt;p>&lt;em>This transcript has been lightly edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: Your journey to Kubernetes went through the financial technology (fintech) industry. Tell me a little bit about how you came to software?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I took a pretty traditional path to software engineering. I went through school and then I did a computer science degree at the University of Bristol, and then I just ended up taking a software engineer job from there. Somewhat rather by accident, I ended up doing fintech work, which is pretty interesting, pretty engaging.&lt;/p>
&lt;p>But in my most recent fintech job before I joined &lt;a href="https://www.jetstack.io/">Jetstack&lt;/a>, I ended up working on a software project. We needed Kubernetes to solve a technical problem. So we implemented Kubernetes, and as often happens, I ended up as the one person of a team that understood the infrastructure, while everyone else was doing all of the application development.&lt;/p>
&lt;p>I ended up enjoying the infrastructure side so much that I decided to move and do that full time. So I looked around and I found Jetstack, whose offices were literally across the road. I could see them out of our office window. And so I decided to just hop across the road and join them, and do all of this Kubernetes stuff more.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What's the tech scene like in Bristol? You went there for school and never left?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Pretty much. It's happened to a lot of people I know and a lot of my friends, is that you go to University somewhere and you're just kind of stuck there forever, so to speak. It's been known for being quite hot in the area in terms of that part of the UK. It has a lot of tech companies, obviously, it was a fintech company I worked at before. I think some larger companies have offices there. For &amp;quot;not London&amp;quot;, it's not doing too bad, I don't think.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When you say hot, though, that's tech industry, not weather, I'm assuming.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, weather is the usual UK. It's kind of a nice overcast and rainy, which I quite like. I'm quite fond of it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Public transport good?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Buses are all right. We've got a new bus installed recently, which everyone hated while it was being built. And now it's complete, everyone loves. So, standard I think.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That is the way. As someone who lived in London for a long time, it's very easy for me to say &amp;quot;well, London's kind of like Singapore. It's its own little city-state.&amp;quot; But whenever we did go out to that part of the world, Bath especially, a very lovely town&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Oh, Bath's lovely. I've been a couple of times.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Have you been to Box?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: To where, sorry?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's &lt;a href="https://en.wikipedia.org/wiki/Box,_Wiltshire">a town called Box&lt;/a> just outside Bath. I had my picture taken outside all the buildings. Proclaimed myself the mayor.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Oh, no, I don't think I have.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Well, look it up if you're ever in the region, everybody. Let's get back to Jetstack, though. They were across the road. Great company, the &lt;a href="https://www.jetstack.io/about/mattbarker/">two&lt;/a> &lt;a href="https://www.jetstack.io/about/mattbates/">Matts&lt;/a>, the co-founders there. What was the interview process like for you?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It was pretty relaxed. One lunchtime, I just walked down the road and went to a coffee shop with Matt and we had this lovely conversation talking about my background and Jetstack and what I was looking to achieve in a new role and all this. And I'd applied to be a software engineer. And then they kind of at the end of it, he looked over at me and was like, &amp;quot;well, how about being a solutions engineer instead?&amp;quot; And I was like, what's that?&lt;/p>
&lt;p>And he's like, &amp;quot;well, you know, it's just effectively being a software consultant. You go, you help companies implement Kubernetes, users, saying all that stuff you enjoy. But you do it full time.&amp;quot; I was like, &amp;quot;well, maybe.&amp;quot; And in the end he convinced me. I ended up joining as a solutions engineer with the idea of if I didn't like it, I could transfer to be a software engineer again.&lt;/p>
&lt;p>Nearly three years later, I've never taken them up on the offer. I've just &lt;a href="https://www.jetstack.io/blog/life-as-a-solutions-engineer/">stayed as a solutions engineer&lt;/a> the entire time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: At the company you were working at, I guess you were effectively the consultant between the people writing the software and the deployment in Kubernetes. Did it make sense then for you to carry on in that role, as you moved to Jetstack?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think so. I think it's something that I enjoyed. Not that I didn't enjoy writing software applications. I always enjoyed it, and we had a really interesting product and a really fun team. But I just found that more interesting. And it was becoming increasingly difficult to justify spending time on it when we had an application to write.&lt;/p>
&lt;p>Which was just completely fine, and that made sense for the needs of the team at the time. But it's not what I wanted to do.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you think that talks to the split between Kubernetes being for developers or for operators? Do you think there's always going to be the need to have a different set of people who are maintaining the running infrastructure versus the people who are writing the code that run on it?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think to some extent, yes, whether or not that's a separate platform team or whether or not that is because the people running it are consultants of some kind. Or whether or not this has been abstracted away from you in some of the more batteries-included versions of Kubernetes — some of the cloud-hosted ones, especially, somewhat remove that need. So I don't think it's absolutely necessary to employ a platform team. But I think someone needs to do it or you need to implicitly or explicitly pay for someone to do it in some way.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: In the three years you have been at Jetstack now, how different are the jobs that you do for the customers? Is this just a case of learning one thing and rolling it out to multiple people, or is there always a different challenge with everyone you come across?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think there's always a different challenge. My role has varied drastically. For example, a long time ago, I did an Istio install. But it was a relatively complicated, single mesh, multi-cluster install. And that was before multi-cluster support was really as readily available as it is now. Conversely, I've worked building custom orchestration platforms on top of Kubernetes for specific customer use cases.&lt;/p>
&lt;p>It's all varied and every single customer engagement is different. That is an element I really like about the job, that variability in how things are and how things go.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When the platform catches up and does things like makes it easier to manage multi-cluster environments, do you go back to the customers and bring them up to date with the newest methods?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It depends. Most of our engagements are to solve a specific problem. And once we've solved that problem, they may have us back. But typically speaking, in my line of work, it's not an ongoing engagement. There are some within Jetstack that do that, but not so much in my team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your bio suggests that you were once called &amp;quot;the reason any corporate policy evolves.&amp;quot; What's the story there?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [CHUCKLES] I think I just couldn't leave things well enough alone. I was talking to our operations director inside of Jetstack, and he once said to me that whenever he's thinking of a new corporate policy, he asks will it pass the James Laverack test. That is, will I look at it and find some horrendous loophole?&lt;/p>
&lt;p>For example when I first joined, I took a look at our acceptable use policy for company equipment. And it stated that you're not allowed to have copyrighted material on your laptop. And of course, this makes sense, as you know, you don't want people doing software piracy or anything. But as written, that would imply you're not allowed to have anything that is copyrighted by anyone on your machine.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Such as perhaps the operating system that comes installed on it?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Such as perhaps the operating system, or anything. And you know, this clearly didn't make any sense. So he adjusted that, and I've kind of been fiddling with that sort of policy ever since.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release team is often seen as an administrative role versus a pure coding role. Does that speak to the kind of change you've had in career in previously being a software developer and now being more of a consultant, or was there something else that attracted you to get involved in that particular part of the community?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I wouldn't really consider it less technical. I mean, yes, you do much less coding. This is something that constantly surprises my friends and some of my colleagues, when I tell them more detail about my role. There's not really any coding involved.&lt;/p>
&lt;p>I don't think my role has really changed to have less coding. In fact, one of my more recent projects at Jetstack, a client project, involved a lot of coding. But I think that what attracted me to this role within Kubernetes is really the community. I found it really rewarding to engage with SIG Release and to engage with the release team. So I've always just enjoyed doing it, even though there is, as you say, not all that much coding involved.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Indeed; your wife said to you, &lt;a href="https://twitter.com/JamesLaverack/status/1483201645286678529">&amp;quot;I don't think your job is to code anymore. You just talk to people all day.&amp;quot;&lt;/a> How did that make you feel?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Ahh, annoyed, because she was right. This was kind of a couple of months ago when I was in the middle of it with all of the Kubernetes meetings. Also, my client project at the time involved a lot of technical discussion. I was in three or four hours of calls every day. And I don't mind that. But I would come out, in part because of course you're working from home, so she sees me all the time. So I'd come out, I'd grab a coffee and be like, &amp;quot;oh, I've got a meeting, I've got to go.&amp;quot; And she'd be like, &amp;quot;do you ever code anymore?&amp;quot;
I think it was in fact just after Christmas when she asked me, &amp;quot;when was the last time you programmed anything?&amp;quot; And I had to think about it. Then I realized that perhaps there was a problem there. Well, not a problem, but I realized that perhaps I don't code as much as I used to.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are you the kind of person who will pick up a hobby project to try and fix that?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Absolutely. I've recently started writing &lt;a href="https://github.com/JamesLaverack/kubernetes-minecraft-operator">a Kubernetes operator for my Minecraft server&lt;/a>. That probably tells you about the state I'm in.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If it's got Kubernetes in it, it doesn't sound that much of a hobby.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHING] Do you not consider Kubernetes to be a hobby?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It depends.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think I do.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think by now.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: In some extents.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned observing the release team in process before you decided to get involved. Was that as part of working with customers and looking to see whether a particular feature would make it into a release, or was there some other reason that that was how you saw the Kubernetes community?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Just after I joined Jetstack, I got the opportunity to go to KubeCon San Diego. I think we actually met there.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We did.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: We had dinner, didn't we? So when I went, I'd only been at Jetstack for a few months. I really wasn't involved in the community in any serious way at all. As a result, I just ended up following around my then colleague, James Munnelly. James is lovely. And, you know, I just kind of went around with him, because he knew everyone.&lt;/p>
&lt;p>I ended up in this hotel bar with a bunch of Kubernetes people, including Stephen Augustus, the co-chair of SIG Release and holder of a bunch of other roles within the community. I happened to ask him, I want to get involved. What is a good way to get involved with the Kubernetes community, if I've never been involved before? And he said, oh, you should join the release team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So it's all down to where you end up in the bar with someone.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, pretty much.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If I'd got to you sooner, you could have been working on Istio.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, I could've been working on Istio, I could have ended up in some other SIG doing something. I just happened to be talking to Stephen. And Stephen suggested it, and I gave it a go. And here I am three years later.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think I remember at the time you were working on an etcd operator?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, that's correct. That was part of a client project, which they, thankfully &lt;a href="https://github.com/improbable-eng/etcd-cluster-operator">let us open source&lt;/a>. This was an operator for etcd, where they had a requirement to run it in Kubernetes, which of course is the opposite way around to how you'd normally want to run it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And I remember having you up at the time, like I'm pretty sure those things exist already, and asking what the need was for there to be something different.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It was that they needed something very specific. The ones that existed already were all designed to run clusters that couldn't be shut down. As long as one replica stayed up, you could keep running etcd. But they needed to be able to suspend and restart the entire cluster, which means it needs disk-persistence support, which it turns out is quite complicated.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's easier if you just throw all the data away.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It's much easier to throw all the data away. We needed to be a little bit careful about how we managed it. We thought about forking and changing an existing one. But we realized it would probably just be as easy to start from scratch, so we did that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've been a member of every release team since that point, since Kubernetes 1.18 in 2020, in a wide range of roles. Which set of roles have you been through?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I started out as a release notes shadow, and did that for a couple of releases, in 1.18 and 1.19. In 1.20, I was the release notes lead. And then in 1.21, I moved into being a shadow again as an enhancement shadow, before in 1.22 becoming an enhancements lead, but in 1.23 a release lead shadow, and finally in 1.24, release lead as a whole.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That's quite a long time to be with the release team. You're obviously going to move into an emeritus role after this release. Do you see yourself still remaining involved? Is it something that you're clearly very passionate about?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think I'm going to be around in SIG Release for as long as people want me there. I find it a really interesting part of the community. And I find the people super-interesting and super-inviting.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk then about &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/">Kubernetes 1.24&lt;/a>. First, as always, congratulations on the release.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This release consists of 46 enhancements. 14 have graduated to stable, 15 have moved to beta, and 13 are in alpha. 2 are deprecated and 2 have been removed. How is that versus other releases recently? Is that an average number? That seems like a lot of stable enhancements, especially.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think it's pretty similar. Most of the recent releases have been quite similar in the number of enhancements they have and in what categories. For example, in 1.23, the previous release, there were 47. I think 1.22, before that, had 53, so slightly more. But it's around about that number.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You didn't want to sneak in two extra so you could say you were one more than the last one?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: No, I don't think so. I think we had enough going on.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release team is obviously beholden to what features the SIGs are developing and what their plans are. Is there ever any coordination between the release process and the SIGs in terms of things like saying, this release is going to be a catch-up release, like the old Snow Leopard releases for macOS, for example, where we say we don't want as many new features, but we really want more stabilization, and could you please work on those kind of things?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Not really. The cornerstone of a Kubernetes organization is the SIGs themselves, so the special interest groups that make up the organization. It's really up to them what they want to do. We don't do any particular coordination on the style of thing that should be implemented. A lot of SIGs have roadmaps that are looking over multiple releases to try to get features that they think are important in.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk about some of the new features in 1.24. We have been hearing for many releases now about the impending doom which is the removal of Dockershim. &lt;a href="https://github.com/kubernetes/enhancements/issues/2221">It is gone in 1.24&lt;/a>. Do we worry?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I don't think we worry. This is something that the community has been preparing for for a long time. &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">We've&lt;/a> &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">published&lt;/a> a &lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">lot&lt;/a> of &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">documentation&lt;/a> &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">about&lt;/a> &lt;a href="https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/">how&lt;/a> you need to approach this. The honest truth is that most users, most application developers in Kubernetes, will simply not notice a difference or have to worry about it.&lt;/p>
&lt;p>It's only really platform teams that administer Kubernetes clusters and people in very specific circumstances that are using Docker directly, not through the Kubernetes API, that are going to experience any issue at all.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And I see that Mirantis and Docker have developed a CRI plugin for Docker anyway, so you can just switch over to that and everything continues.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, absolutely, or you can use one of the many other CRI implementations. There are two in the CNCF, &lt;a href="https://containerd.io/">containerd&lt;/a>, and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Having gone through the process of communicating this change over several releases, what has the team learnt in terms of how we will communicate a message like this in future?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think that this has been really interesting from the perspective that this is the biggest removal that the Kubernetes project has had to date. We've removed features before. In fact, we're removing another one in this release as well. But this is one of the most user-visible changes we've made.&lt;/p>
&lt;p>I think there are very good reasons for doing it. But I think we've learned a lot about how and when to communicate, and the importance of having migration guides, the importance of having official documentation that really clarifies the thing. I think that's the real, it's an area in which the Kubernetes project has matured a lot since I've been on the team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What is the other feature that's being removed?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: The other feature that we're removing is dynamic Kubelet configuration. This is a feature that was in beta for a while. But I believe we decided that it just wasn't being used enough to justify keeping it. So we're removing it. We deprecated it back in 1.22 and we're removing it this release.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There was a change in policy a few releases ago that talked about features not being allowed to stay in beta forever. Have there been any features that were at risk of being removed due to lack of maintenance, or are all the SIGs pretty good now at keeping their features on track?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think the SIGs are getting pretty good at it. We had a spate of a long time when a lot of features were kind of perpetually in beta. As you remember, Ingress was in beta for a long, long time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I choose to believe it still is.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHTER] I think it's really good that we're moving towards that stability approach with things like Kubernetes. I think it's a very positive change.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The fact that Ingress was in beta for so long, along with things like the main workload controllers, for example, did lead people to believing that beta APIs were stable and production ready, and could and should be used. Something that's changing in this release is that &lt;a href="https://github.com/kubernetes/enhancements/issues/3136">beta APIs are going to be off by default&lt;/a>. Why that change?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This is really about encouraging the use of stable APIs. There was a perception, like you say, that beta APIs were actually stable. Because they can be removed very quickly, we often ended up in the state where we wanted to follow the policy and remove a beta API, but were unable to, because it was de facto stable, according to the community. This meant that cluster operators and users had a lot of breaking changes when doing upgrades that could have been avoided. This is really just to help stability as we go through more upgrades in the future.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I understand that only applies now to new APIs. Things that are in beta at the moment will continue to be available. So there'll be no breaking changes again?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: That's correct. There's no breaking changes in beta APIs other than the ones we've documented this release. It's only new things.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now in this release, &lt;a href="https://github.com/kubernetes/enhancements/issues/3031">the artifacts are signed&lt;/a> using Cosign signatures, and there is &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/">experimental support for verification of those signatures&lt;/a>. What needed to happen to make that process possible?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This was a huge process from the other half of SIG Release. SIG Release has the release team, but it also has the release engineering team that handles the mechanics of actually pushing releases out. They have spent, and one of my friends over there, Adolfo, has spent a lot of time trying to bring us in line with &lt;a href="https://slsa.dev/">SLSA&lt;/a> compliance. I believe we're &lt;a href="https://github.com/kubernetes/enhancements/issues/3027">looking now at Level 3 compliance&lt;/a>.&lt;/p>
&lt;p>SLSA is a framework that describes software supply chain security. That is, of course, a really big issue in our industry at the moment. And it's really good to see the project adopting the best practices for this.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I was looking back at &lt;a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">the conversation I had with Rey Lejano about the 1.23 release&lt;/a>, and we were basically approaching Level 2. We're now obviously stepping up to Level 3. I think I asked Rey at the time was, is it fair to say that SLSA is inspired by large projects like Kubernetes, and in theory, it should be really easy for these projects to tick the boxes to get to that level, because the SLSA framework is written with a project like Kubernetes in mind?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think so. I think it's been somewhat difficult, just because it's one thing to do it, but it's another thing to prove that you're doing it, which is the whole point around these frameworks — the assertation, that proof.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: As an end user of Kubernetes, whether I install it myself or I take it from a service like GKE, what will this provenance then let me prove? If we think back to &lt;a href="https://kubernetespodcast.com/episode/174-in-toto/">the orange juice example we talked to Santiago about recently&lt;/a>, how do I tell that my software is safe to run?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: If you're downloading and running Kubernetes yourself, you can use the verifying image signatures feature to verify the thing you've downloaded, and the thing you are running, is actually the thing that the Kubernetes project has released, and that it has been built from the actual source code in the Kubernetes GitHub repository. This can give you a lot of confidence in what you're running, especially if you're running in a highly secure or regulated environment of some kind.&lt;/p>
&lt;p>As an end user, this isn't something that will necessarily directly impact you. But it means that service providers that provide managed Kubernetes options, such as Google and GKE, can provide even greater levels of security and safety themselves about the services that they run.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of people get access to their Kubernetes server just by being granted an API endpoint, and they start running kubectl against it. They're not actually installing their own Kubernetes. They have a provider or a platform team do it for them. Do you think it's feasible to get to a world where there's something that you can run when you're deploying your workloads which queries the API server, for example, and gets access to that same provenance data?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think it's going to be very difficult to do it that way, simply because this provenance and assertation data implies that you actually have access to the underlying executables, which typically, when you're running in a managed platform, you don't. If you're having Kubernetes provided to you, I think you're still going to have to trust the platform team or the organization that's providing it to you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Just like when you go to the hotel breakfast bar, you have to trust that they've been good with their orange juice.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, I think the orange juice example is great. If you're making it yourself, then you can use assertation. If you're not, if you've just been given a glass, then you're going to have to trust who's pouring it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Continuing with our exploration of new stable features, &lt;a href="https://github.com/kubernetes/enhancements/issues/1472">storage capacity tracking&lt;/a> and &lt;a href="https://github.com/kubernetes/enhancements/issues/284">volume expansion&lt;/a> are generally available. What do those features enable me to do?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This is a really great set of stable features coming out of SIG Storage. Storage capacity tracking allows applications on Kubernetes to use the Kubernetes API to understand how much storage is available, which can drive application decisions. With volume expansion, that again allows an application to use the Kubernetes API to request additional storage, which can enable applications to make all kinds of operational decisions.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: SIG Storage are also working through &lt;a href="https://github.com/kubernetes/enhancements/issues/625">a project to migrate all of their in-tree storage plugins out to CSI plugins&lt;/a>. How are they going with that process?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: In 1.24 we have a couple of them that have been migrated out. The &lt;a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk&lt;/a> and &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder&lt;/a> plugins have both been migrated. They're maintaining the original API, but the actual implementation now happens in those CSI plugins.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do they have a long way to go, or are they just cutting off a couple every release?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: They're just doing a couple every release from what I see. There are a couple of others to go. This is really part of a larger theme within Kubernetes, which is pushing application-specific things out behind interfaces, such as the container storage interface and the container runtime interface.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That obviously sets up a situation where you have a stable interface and you can have beta implementations of that that are outside of Kubernetes and get around the problem we talked about before with not being able to run beta things.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, exactly. It also makes it easy to expand Kubernetes. You don't have to try to get code in-tree in order to implement a new storage engine, for example.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://github.com/kubernetes/enhancements/issues/2727">gRPC probes have graduated to beta in 1.24&lt;/a>. What does that functionality provide?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This is one of the changes that's going to be most visible to application developers in Kubernetes, I think. Until now, Kubernetes has had the ability to do readiness and liveness checks on containers and be able to make intelligent routing and pod restart decisions based on those. But those checks had to be HTTP REST endpoints.&lt;/p>
&lt;p>With Kubernetes 1.24, we're enabling a beta feature that allows them to use gRPC. This means that if you're building an application that is primarily gRPC-based, as many microservices applications are, you can now use that same technology in order to implement your probes without having to bundle an HTTP server as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are there any other enhancements that are particularly notable or relevant perhaps to the work you've been doing?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: There's a really interesting one from SIG Network which is about &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#avoiding-collisions-in-ip-allocation-to-services">avoiding collisions in IP allocations to services&lt;/a>. In existing versions of Kubernetes, you can allocate a service to have a particular internal cluster IP, or you can leave it blank and it will generate its own IP.&lt;/p>
&lt;p>In Kubernetes 1.24, there's an opt-in feature, which allows you to specify a pool for dynamic IPs to be generated from. This means that you can statically allocate an IP to a service and know that IP can not be accidentally dynamically allocated. This is a problem I've actually had in my local Kubernetes cluster, where I use static IP addresses for a bunch of port forwarding rules. I've always worried that during server start-up, they're going to get dynamically allocated to one of the other services. Now, with 1.24, and this feature, I won't have to worry about it more.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This is like the analog of allocating an IP in your DHCP server rather than just claiming it statically on your local machine?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Pretty much. It means that you can't accidentally double allocate something.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Why don't we all just use IPv6?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: That is a very deep question I don't think we have time for.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The margins of this podcast would be unable to contain it even if we did.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHING]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#release-theme-and-logo">The theme for Kubernetes 1.24 is Stargazer&lt;/a>. How did you pick that as the theme?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Every release lead gets to pick their theme, pretty much by themselves. When I started, I asked Rey, the previous release lead, how he picked his theme, because he picked the Next Frontier for Kubernetes 1.23. And he told me that he'd actually picked it before the release even started, which meant for the first couple of weeks and months of the release, I was really worried about it, because I hadn't picked one yet, and I wasn't sure what to pick.&lt;/p>
&lt;p>Then again, I was speaking to another former release lead, and they told me that they picked theirs like two weeks out. It seems to really vary. About halfway through the release, I had some ideas down. I thought maybe we could talk about — I live in a city called Bristol in the UK, which has a very famous bridge — and I thought, oh, we could talk about bridges and architectural and a metaphor for community bridging gaps and things like this. I kind of liked the idea, but it didn't really grab me.&lt;/p>
&lt;p>One thing about me is that I am a serious night owl. I cannot work effectively in the mornings. I've always enjoyed the night. And that got me thinking about astronomy and the stars. I think one night I was trying to get to sleep, because I couldn't sleep, and I was watching &lt;a href="https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g">PBS Space Time&lt;/a>, which is this fantastic YouTube channel talking about physics. And I'm not a physicist. I don't understand any of the maths. But I find it really interesting as a topic.&lt;/p>
&lt;p>I just thought, well, why don't I make a theme about stars. Kubernetes has often had a space theme in many releases. As I'm sure you're aware, its original name was based off of Star Trek. The previous release had a Star Trek-based theme. I thought, well, let's do that. So I came up with the idea of Stargazer.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Once you have a theme, you then need a release logo. I understand you have a household artist?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHS] I don't think she'd appreciate being called that, but, yes. My wife is an artist, and in particular, a digital artist. I had a bit of a conversation with the SIG Release folks to see if they'd be comfortable with my wife doing it, and they said they'd be completely fine with that.&lt;/p>
&lt;p>I asked if she would be willing to spend some time creating a logo for us. And thankfully for me, she was. She has produced this — well, I'm somewhat obliged to say — she produced us a beautiful logo, which you can see in our release blog and probably around social media. It is a telescope set over starry skies, and I absolutely love it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It is objectively very nice. It obviously has the seven stars or the Seven Sisters of the Pleiades. Do the colors have any particular meaning?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: The colors are based on the Kubernetes blue. If you look in the background, that haze is actually in the shape of a Kubernetes wheel from the original Kubernetes logo.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You must have to squint at it the right way. Very abstract. As is the wont of art.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: As is the wont.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned before Rey Lejano, the 1.23 release lead. We ask every interview what the person learned from the last lead and what they're going to put in the proverbial envelope for the next. At the time, Rey said that he would encourage you to use teachable moments in the release team meetings. Was that something you were able to do?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Not as much as I would have liked. I think the thing that I really took from Rey was communicate more. I've made a big effort this time to put as much communication in the open as possible. I was actually worried that I was going to be spamming the SIG Release Slack channel too much. I asked our SIG Release chairs Stephen and Sasha about it. And they said, just don't worry about it. Just spam as much as you want.&lt;/p>
&lt;p>And so I think the majority of the conversation in SIG Release Slack over the past few months has just been me. [LAUGHING] That seemed to work out pretty well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That's what it's for.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It is what it's for. But SIG Release does more than just the individual release process, of course. It's release engineering, too.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm sure they'd be interested in what's going on anyway?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It's true. It's true. It's been really nice to be able to talk to everyone that way, I think.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We talked before about your introduction to Kubernetes being at a KubeCon, and meeting people in person. How has it been running the release almost entirely virtually?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It's not been so bad. The release team has always been geographically distributed, somewhat by design. It's always been a very virtual engagement, so I don't think it's been impacted too, too much by the pandemic and travel restrictions. Of course, I'm looking forward to KubeCon Valencia and being able to see everyone again. But I think the release team has handled excellently in the current situation.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What is the advice that you will pass on to &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">the next release lead&lt;/a>, which has been announced to be Cici Huang from Google?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I would say to Cici that open communication is really important. I made a habit of posting every single week in SIG Release a summary of what's happened. I'm super-glad that I did that, and I'm going to encourage her to do the same if she wants to.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This release was originally due out two weeks earlier, but &lt;a href="https://groups.google.com/a/kubernetes.io/g/dev/c/9IZaUGVMnmo">it was delayed&lt;/a>. What happened?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: That delay was the result of a release-blocking bug — an absolute showstopper. This was in the underlying Go implementation of TLS certificate verification. It meant that a lot of clients simply would not be able to connect to clusters or anything else. So we took the decision that we can't release with a bug this big. Thus the term release-blocking.&lt;/p>
&lt;p>The fix had to be merged upstream in Go 1.18.1, and then we had to, of course, rebuild and release release candidates. Given the time we like to have things to sit and stabilize after we make a lot of changes like that, we felt it was more prudent to push out the release by a couple of weeks than risk shipping a broken point-zero.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Go 1.18 is itself quite new. How does the project decide how quickly to upgrade its underlying programming language?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: A lot of it is driven by support requirements. We support each release for three releases. So Kubernetes 1.24 will be most likely in support until this time next year, in 2023, as we do three releases per year. That means that right up until May, 2023, we're probably going to be shipping updates for Kubernetes 1.24, which means that the version of Go we're using, and other dependencies, have to be supported as well. My understanding is that the older version of Go, Go 1.17, just wouldn't be supported long enough.&lt;/p>
&lt;p>Any underlying critical bug fixes that were coming in, they wouldn't have been back ported to Go 1.17, and therefore we might not be able to adequately support Kubernetes 1.24.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A side effect of the unfortunate delay was an unfortunate holiday situation, where you were booked to take the week after the release off and instead you ended up taking the week before the release off. Were you able to actually have any holiday and relax in that situation?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Well, I didn't go anywhere, if that's what you're asking.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: No one ever does. This is what the pandemic's been, staycations.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, staycations. It's been interesting. On the one hand, I've done a lot of Kubernetes work in that time. So you could argue it's not really been a holiday. On the other hand, my highly annoying friends have gotten me into playing an MMO, so I've been spending a lot of time playing that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I hear also you have a new vacuum cleaner?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHS] You've been following my Twitter. Yes, I couldn't find the charging cord for my old vacuum cleaner. And so I decided just to buy a new one. I decided, at long last, just to buy one of the nice brand-name ones. And it is just better.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This isn't the BBC. You're allowed to name it if you want.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yes, we went and bought one of these nice Dyson vacuum cleaners, and the first time I've gotten one so expensive. On the one hand, I feel a little bit bad spending a lot of money on a vacuum cleaner. On the other hand, it's so much easier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Is it one of those handheld ones, like a giant Dust-Buster with a long leg?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: No, I got one of the corded floor ones, because the problem was, of course, I lost the charger for the last one, so I didn't want that to happen again. So I got a wall plug-in one.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I must say, going from a standard &lt;a href="https://www.myhenry.com/">Henry Hoover&lt;/a> to — the place we're staying at the moment has what I'll call a knock-off Dyson portable vacuum cleaner — having something that you can just pick up and carry around with you, and not have to worry about the cord, actually does encourage me to keep the place tidier.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Really? I think our last one was corded, but it didn't encourage us to use it anymore, just because it was so useless.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/jameslaverack">James Laverack&lt;/a> is a Staff Solutions Engineer at Jetstack, and was the release team lead for Kubernetes 1.24.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Meet Our Contributors - APAC (China region)</title><link>https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/</guid><description>
&lt;p>&lt;strong>Authors &amp;amp; Interviewers:&lt;/strong> &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>, &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>, &lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>, &lt;a href="https://github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>, &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>, &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Hello, everyone 👋&lt;/p>
&lt;p>Welcome back to the third edition of the &amp;quot;Meet Our Contributors&amp;quot; blog post series for APAC.&lt;/p>
&lt;p>This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project.&lt;/p>
&lt;p>So, without further ado, let's get straight to the article.&lt;/p>
&lt;h2 id="andy-zhang-https-github-com-andyzhangx">&lt;a href="https://github.com/andyzhangx">Andy Zhang&lt;/a>&lt;/h2>
&lt;p>Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago.&lt;/p>
&lt;p>He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code.&lt;/p>
&lt;p>His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects.&lt;/p>
&lt;p>Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.&lt;/p>
&lt;h2 id="shiming-zhang-https-github-com-wzshiming">&lt;a href="https://github.com/wzshiming">Shiming Zhang&lt;/a>&lt;/h2>
&lt;p>Shiming Zhang is a Software Engineer working on Kubernetes for DaoCloud in Shanghai, China.&lt;/p>
&lt;p>He has mostly been involved with SIG Node as a reviewer. His major contributions have mainly been bug fixes and feature improvements in an ongoing &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown">KEP&lt;/a>, all revolving around SIG Node.&lt;/p>
&lt;p>Some of his major PRs are &lt;a href="https://github.com/kubernetes/kubernetes/pull/100326">fixing watchForLockfileContention memory leak&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/101093">fixing startupProbe behaviour&lt;/a>, &lt;a href="https://github.com/kubernetes/enhancements/pull/2661">adding Field status.hostIPs for Pod&lt;/a>.&lt;/p>
&lt;h2 id="paco-xu-https-github-com-pacoxu">&lt;a href="https://github.com/pacoxu">Paco Xu&lt;/a>&lt;/h2>
&lt;p>Paco Xu works at DaoCloud, a Shanghai-based cloud-native firm. He works with the infra and the open source team, focusing on enterprise cloud native platforms based on Kubernetes.&lt;/p>
&lt;p>He started with Kubernetes in early 2017 and his first contribution was in March 2018. He started with a bug that he found, but his solution was not that graceful, hence wasn't accepted. He then started with some good first issues, which helped him to a great extent. In addition to this, from 2016 to 2017, he made some minor contributions to Docker.&lt;/p>
&lt;p>Currently, Paco is a reviewer for &lt;code>kubeadm&lt;/code> (a SIG Cluster Lifecycle product), and for SIG Node.&lt;/p>
&lt;p>Paco says that you should contribute to open source projects you use. For him, an open source project is like a book to learn, getting inspired through discussions with the project maintainers.&lt;/p>
&lt;blockquote>
&lt;p>In my opinion, the best way for me is learning how owners work on the project.&lt;/p>
&lt;/blockquote>
&lt;h2 id="jintao-zhang-https-github-com-tao12345666333">&lt;a href="https://github.com/tao12345666333">Jintao Zhang&lt;/a>&lt;/h2>
&lt;p>Jintao Zhang is presently employed at API7, where he focuses on ingress and service mesh.&lt;/p>
&lt;p>In 2017, he encountered an issue which led to a community discussion and his contributions to Kubernetes started. Before contributing to Kubernetes, Jintao was a long-time contributor to Docker-related open source projects.&lt;/p>
&lt;p>Currently Jintao is a maintainer for the &lt;a href="https://kubernetes.github.io/ingress-nginx/">ingress-nginx&lt;/a> project.&lt;/p>
&lt;p>He suggests keeping track of job opportunities at open source companies so that you can find one that allows you to contribute full time. For new contributors Jintao says that if anyone wants to make a significant contribution to an open source project, then they should choose the project based on their interests and should generously invest time.&lt;/p>
&lt;hr>
&lt;p>If you have any recommendations/suggestions for who we should interview next, please let us know in the &lt;a href="https://kubernetes.slack.com/archives/C1TU9EB9S">#sig-contribex channel&lt;/a> channel on the Kubernetes Slack. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.&lt;/p>
&lt;p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋&lt;/p></description></item></channel></rss>