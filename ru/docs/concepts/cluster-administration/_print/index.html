<!doctype html><html lang=ru class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/cluster-administration/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/cluster-administration/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/cluster-administration/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/cluster-administration/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/cluster-administration/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/cluster-administration/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/cluster-administration/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/cluster-administration/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/cluster-administration/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.102.3"><link rel=canonical type=text/html href=https://kubernetes.io/ru/docs/concepts/cluster-administration/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Администрирование кластера | Kubernetes</title><meta property="og:title" content="Администрирование кластера"><meta property="og:description" content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/ru/docs/concepts/cluster-administration/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Администрирование кластера"><meta itemprop=description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Администрирование кластера"><meta name=twitter:description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta property="og:description" content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta name=twitter:description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta property="og:url" content="https://kubernetes.io/ru/docs/concepts/cluster-administration/"><meta property="og:title" content="Администрирование кластера"><meta name=twitter:title content="Администрирование кластера"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class="navbar-brand img-fluid" href=/ru/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/ru/docs/>Документация</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ru/community/>Сообщество</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ru/case-studies/>Примеры использования</a></li><li class="nav-item mr-n3 mr-lg-0 dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Версии</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/ru/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/ru/docs/concepts/cluster-administration/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/ru/docs/concepts/cluster-administration/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/ru/docs/concepts/cluster-administration/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/ru/docs/concepts/cluster-administration/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/ru/docs/concepts/cluster-administration/>v1.22</a></div></li><li class="nav-item mr-n4 mr-lg-0 dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Русский (Russian)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/cluster-administration/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/cluster-administration/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/cluster-administration/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/cluster-administration/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/cluster-administration/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/cluster-administration/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/cluster-administration/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/cluster-administration/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/cluster-administration/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/cluster-administration/>Bahasa Indonesia</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Это многостраничный печатный вид этого раздела.
<a href=# onclick="return print(),!1">Нажмите что бы печатать</a>.</p><p><a href=/ru/docs/concepts/cluster-administration/>Вернуться к обычному просмотру страницы</a>.</p></div><h1 class=title>Администрирование кластера</h1><div class=lead>Lower-level detail relevant to creating or administering a Kubernetes cluster.</div><ul><li>1: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>Сертификаты</a></li><li>2: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>Архитектура для сбора логов</a></li><li>3: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Типы прокси-серверов в Kubernetes</a></li><li>4: <a href=#pg-31c9327d2332c585341b64ddafa19cdd>Равноправный доступ к API</a></li><li>5: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>Установка дополнений</a></li></ul><div class=content><p>Обзор администрирования кластера предназначен для всех, кто создает или администрирует кластер Kubernetes. Это предполагает некоторое знакомство с основными <a href=/docs/concepts/>концепциями</a> Kubernetes.</p><h2 id=планирование-кластера>Планирование кластера</h2><p>См. Руководства в разделе <a href=/docs/setup/>настройка</a> для получения примеров того, как планировать, устанавливать и настраивать кластеры Kubernetes. Решения, перечисленные в этой статье, называются <em>distros</em>.</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> не все дистрибутивы активно поддерживаются. Выбирайте дистрибутивы, протестированные с последней версией Kubernetes.</div><p>Прежде чем выбрать руководство, вот некоторые соображения:</p><ul><li>Вы хотите опробовать Kubernetes на вашем компьютере или собрать много узловой кластер высокой доступности? Выбирайте дистрибутивы, наиболее подходящие для ваших нужд.</li><li>Будете ли вы использовать <strong>размещенный кластер Kubernetes</strong>, такой, как <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a> или <strong>разместите собственный кластер</strong>?</li><li>Будет ли ваш кластер <strong>в помещении</strong> или <strong>в облаке (IaaS)</strong>? Kubernetes не поддерживает напрямую гибридные кластеры. Вместо этого вы можете настроить несколько кластеров.</li><li><strong>Если вы будете настраивать Kubernetes в помещении (локально)</strong>, подумайте, какая <a href=/docs/concepts/cluster-administration/networking/>сетевая модель</a> подходит лучше всего.</li><li>Будете ли вы запускать Kubernetes на <strong>оборудований "bare metal"</strong> или на <strong>виртуальных машинах (VMs)</strong>?</li><li>Вы хотите <strong>запустить кластер</strong> или планируете <strong>активно разворачивать код проекта Kubernetes</strong>? В последнем случае выберите активно разрабатываемый дистрибутив. Некоторые дистрибутивы используют только двоичные выпуски, но предлагают более широкий выбор.</li><li>Ознакомьтесь с <a href=/docs/concepts/overview/components/>компонентами</a> необходимые для запуска кластера.</li></ul><h2 id=управление-кластером>Управление кластером</h2><ul><li><p>Узнайте как <a href=/docs/concepts/architecture/nodes/>управлять узлами</a>.</p></li><li><p>Узнайте как настроить и управлять <a href=/docs/concepts/policy/resource-quotas/>квотами ресурсов</a> для общих кластеров.</p></li></ul><h2 id=обеспечение-безопасности-кластера>Обеспечение безопасности кластера</h2><ul><li><p><a href=/docs/tasks/administer-cluster/certificates/>Сгенерировать сертификаты</a> описывает шаги по созданию сертификатов с использованием различных цепочек инструментов.</p></li><li><p><a href=/docs/concepts/containers/container-environment/>Kubernetes Container Environment</a> описывает среду для управляемых контейнеров Kubelet на узле Kubernetes.</p></li><li><p><a href=/docs/concepts/security/controlling-access>Управление доступом к Kubernetes API</a> описывает как Kubernetes реализует контроль доступа для своего собственного API.</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>Аутентификация</a> объясняет аутентификацию в Kubernetes, включая различные варианты аутентификации.</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Авторизация</a> отделена от аутентификации и контролирует обработку HTTP-вызовов.</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Использование контроллеров допуска</a> explains plug-ins which intercepts requests to the Kubernetes API server after authentication and authorization.</p></li><li><p><a href=/docs/tasks/administer-cluster/sysctl-cluster/>Использование Sysctls в кластере Kubernetes</a> описывает администратору, как использовать sysctl инструмент командной строки для установки параметров ядра.</p></li><li><p><a href=/docs/tasks/debug-application-cluster/audit/>Аудит</a> описывает, как взаимодействовать с журналами аудита Kubernetes.</p></li></ul><h3 id=обеспечение-безопасности-kubelet>Обеспечение безопасности kubelet</h3><ul><li><a href=/docs/concepts/architecture/control-plane-node-communication/>Связь между плоскостью управления и узлом</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>Загрузка TLS</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/>Аутентификация/авторизация Kubelet</a></li></ul><h2 id=дополнительные-кластерные-услуги>Дополнительные кластерные услуги</h2><ul><li><p><a href=/docs/concepts/services-networking/dns-pod-service/>Интеграция DNS</a> описывает как разрешить DNS имя непосредственно службе Kubernetes.</p></li><li><p><a href=/docs/concepts/cluster-administration/logging/>Ведение журнала и мониторинг активности кластера</a> объясняет, как работает ведение журнала в Kubernetes и как его реализовать.</p></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>1 - Сертификаты</h1><p>Чтобы узнать, как генерировать сертификаты для кластера, см. раздел <a href=/docs/tasks/administer-cluster/certificates/>Сертификаты</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>2 - Архитектура для сбора логов</h1><p>Логи помогают понять, что происходит внутри приложения. Они особенно полезны для отладки проблем и мониторинга деятельности кластера. У большинства современных приложений имеется тот или иной механизм сбора логов. Контейнерные движки в этом смысле не исключение. Самый простой и наиболее распространенный метод сбора логов для контейнерных приложений задействует потоки <code>stdout</code> и <code>stderr</code>.</p><p>Однако встроенной функциональности контейнерного движка или среды исполнения обычно недостаточно для организации полноценного решения по сбору логов.</p><p>Например, может возникнуть необходимость просмотреть логи приложения при аварийном завершении работы Pod'а, его вытеснении (eviction) или "падении" узла.</p><p>В кластере у логов должно быть отдельное хранилище и жизненный цикл, не зависящий от узлов, Pod'ов или контейнеров. Эта концепция называется <em>сбор логов на уровне кластера</em>.</p><p>Архитектуры для сбора логов на уровне кластера требуют отдельного бэкенда для их хранения, анализа и выполнения запросов. Kubernetes не имеет собственного решения для хранения такого типа данных. Вместо этого существует множество продуктов для сбора логов, которые прекрасно с ним интегрируются. В последующих разделах описано, как обрабатывать логи и хранить их на узлах.</p><h2 id=основы-сбора-логов-в-kubernetes>Основы сбора логов в Kubernetes</h2><p>В примере ниже используется спецификация <code>Pod</code> с контейнером для отправки текста в стандартный поток вывода раз в секунду.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/ru/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("debug-counter-pod-yaml")' title="Copy debug/counter-pod.yaml to clipboard"></img></div><div class=includecode id=debug-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Запустить его можно с помощью следующей команды:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>Результат будет таким:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>pod/counter created
</span></span></span></code></pre></div><p>Получить логи можно с помощью команды <code>kubectl logs</code>, как показано ниже:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter
</span></span></code></pre></div><p>Результат будет таким:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>0: Mon Jan  1 00:00:00 UTC 2001
</span></span></span><span style=display:flex><span><span style=color:#888>1: Mon Jan  1 00:00:01 UTC 2001
</span></span></span><span style=display:flex><span><span style=color:#888>2: Mon Jan  1 00:00:02 UTC 2001
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><p>Команда <code>kubectl logs --previous</code> позволяет извлечь логи из предыдущего воплощения контейнера. Если в Pod'е несколько контейнеров, выбрать нужный для извлечения логов можно с помощью флага <code>-c</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>kubectl logs counter -c count
</span></span></span></code></pre></div><p>Для получения дополнительной информации см. <a href=/docs/reference/generated/kubectl/kubectl-commands#logs>документацию по <code>kubectl logs</code></a>.</p><h2 id=сбор-логов-на-уровне-узла>Сбор логов на уровне узла</h2><p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Сбор логов на уровне узла"></p><p>Среда исполнения для контейнера обрабатывает и перенаправляет любой вывод в потоки <code>stdout</code> и <code>stderr</code> приложения. Docker Engine, например, перенаправляет эти потоки <a href=https://docs.docker.com/engine/admin/logging/overview>драйверу журналирования</a>, который в Kubernetes настроен на запись в файл в формате JSON.</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> JSON-драйвер Docker для сбора логов рассматривает каждую строку как отдельное сообщение. В данном случае поддержка многострочных сообщений отсутствует. Обработка многострочных сообщений должна выполняться на уровне лог-агента или выше.</div><p>По умолчанию, если контейнер перезапускается, kubelet сохраняет один завершенный контейнер с его логами. Если Pod вытесняется с узла, все соответствующие контейнеры также вытесняются вместе с их логами.</p><p>Важным моментом при сборе логов на уровне узла является их ротация, чтобы логи не занимали все доступное место на узле. Kubernetes не отвечает за ротацию логов, но способен развернуть инструмент для решения этой проблемы. Например, в кластерах Kubernetes, развертываемых с помощью скрипта <code>kube-up.sh</code>, имеется инструмент <a href=https://linux.die.net/man/8/logrotate><code>logrotate</code></a>, настроенный на ежечасный запуск. Также можно настроить среду исполнения контейнера на автоматическую ротацию логов приложения.</p><p>Подробную информацию о том, как <code>kube-up.sh</code> настраивает логирование для образа COS на GCP, можно найти в соответствующем скрипте <a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh><code>configure-helper</code></a>.</p><p>При использовании <strong>среды исполнения контейнера CRI</strong> kubelet отвечает за ротацию логов и управление структурой их директории. kubelet передает данные среде исполнения контейнера CRI, а та сохраняет логи контейнера в указанное место. С помощью параметров <a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration><code>containerLogMaxSize</code> и <code>containerLogMaxFiles</code></a> в <a href=/docs/tasks/administer-cluster/kubelet-config-file/>конфигурационном файле kubelet'а</a> можно настроить максимальный размер каждого лог-файла и максимальное число таких файлов для каждого контейнера соответственно.</p><p>При выполнении команды <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a> (как в примере из раздела про основы сбора логов) kubelet на узле обрабатывает запрос и считывает данные непосредственно из файла журнала. Затем он возвращает его содержимое.</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> Если ротацию выполнила внешняя система или используется среда исполнения контейнера CRI, команде <code>kubectl logs</code> будет доступно содержимое только последнего лог-файла. Например, имеется файл размером 10 МБ, <code>logrotate</code> выполняет ротацию, и получается два файла: первый размером 10 МБ, второй - пустой. В этом случае <code>kubectl logs</code> вернет второй лог-файл (пустой).</div><h3 id=логи-системных-компонентов>Логи системных компонентов</h3><p>Существует два типа системных компонентов: те, которые работают в контейнере, и те, которые работают за пределами контейнера. Например:</p><ul><li>планировщик Kubernetes и kube-proxy выполняются в контейнере;</li><li>kubelet и среда исполнения контейнера работают за пределами контейнера.</li></ul><p>На машинах с systemd среда исполнения и kubelet пишут в journald. Если systemd отсутствует, среда исполнения и kubelet пишут в файлы <code>.log</code> в директории <code>/var/log</code>. Системные компоненты внутри контейнеров всегда пишут в директорию <code>/var/log</code>, обходя механизм ведения логов по умолчанию. Они используют библиотеку для сбора логов <a href=https://github.com/kubernetes/klog><code>klog</code></a>. Правила сбора логов и рекомендации можно найти в соответствующей <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>документации</a>.</p><p>Как и логи контейнеров, логи системных компонентов в директории <code>/var/log</code> необходимо ротировать. В кластерах Kubernetes, созданных с помощью скрипта <code>kube-up.sh</code>, эти файлы настроены на ежедневную ротацию с помощью инструмента <code>logrotate</code> или при достижении 100 МБ.</p><h2 id=архитектуры-для-сбора-логов-на-уровне-кластера>Архитектуры для сбора логов на уровне кластера</h2><p>Kubernetes не имеет собственного решения для сбора логов на уровне кластера, но есть общие подходы, которые можно рассмотреть. Вот некоторые из них:</p><ul><li>использовать агент на уровне узлов (запускается на каждом узле);</li><li>внедрить в Pod с приложением специальный sidecar-контейнер для сбора логов;</li><li>отправлять логи из приложения непосредственно в бэкенд.</li></ul><h3 id=использование-агента-на-уровне-узлов>Использование агента на уровне узлов</h3><p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Использование агента на уровне узлов"></p><p>Сбор логов на уровне кластера можно реализовать, запустив <em>node-level-агент</em> на каждом узле. Лог-агент — это специальный инструмент, который предоставляет доступ к логам или передает их бэкенду. Как правило, лог-агент представляет собой контейнер с доступом к директории с файлами логов всех контейнеров приложений на этом узле.</p><p>Поскольку лог-агент должен работать на каждом узле, рекомендуется запускать его как <code>DaemonSet</code>.</p><p>Сбор логов на уровне узла предусматривает запуск одного агента на узел и не требует изменений в приложениях, работающих на узле.</p><p>Контейнеры пишут в <code>stdout</code> и <code>stderr</code>, но без согласованного формата. Агент на уровне узла собирает эти логи и направляет их на агрегацию.</p><h3 id=sidecar-container-with-logging-agent>Сбор логов с помощью sidecar-контейнера с лог-агентом</h3><p>Sidecar-контейнер можно использовать одним из следующих способов:</p><ul><li>sidecar-контейнер транслирует логи приложений на свой собственный <code>stdout</code>;</li><li>в sidecar-контейнере работает агент, настроенный на сбор логов из контейнера приложения.</li></ul><h4 id=транслирующий-sidecar-контейнер>Транслирующий sidecar-контейнер</h4><p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Sidecar-контейнер с транслирующим контейнером"></p><p>Настроив sidecar-контейнеры на вывод в их собственные потоки <code>stdout</code> и <code>stderr</code>, можно воспользоваться преимуществами kubelet и лог-агента, которые уже работают на каждом узле. Sidecar-контейнеры считывают логи из файла, сокета или journald. Затем каждый из них пишет логи в собственный поток <code>stdout</code> или <code>stderr</code>.</p><p>Такой подход позволяет разграничить потоки логов от разных частей приложения, некоторые из которых могут не поддерживать запись в <code>stdout</code> или <code>stderr</code>. Логика, управляющая перенаправлением логов, проста и не требует значительных ресурсов. Кроме того, поскольку <code>stdout</code> и <code>stderr</code> обрабатываются kubelet'ом, можно использовать встроенные инструменты вроде <code>kubectl logs</code>.</p><p>Предположим, к примеру, что в Pod'е работает один контейнер, который пишет логи в два разных файла в двух разных форматах. Вот пример конфигурации такого Pod'а:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/ru/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-yaml")' title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Не рекомендуется писать логи разных форматов в один и тот же поток, даже если удалось перенаправить оба компонента в <code>stdout</code> контейнера. Вместо этого можно создать два sidecar-контейнера. Каждый из них будет забирать определенный лог-файл с общего тома и перенаправлять логи в свой <code>stdout</code>.</p><p>Вот пример конфигурации Pod'а с двумя sidecar-контейнерами:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/ru/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-streaming-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/1.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/2.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Доступ к каждому потоку логов такого Pod'а можно получить отдельно, выполнив следующие команды:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-1
</span></span></code></pre></div><p>Результат будет таким:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>0: Mon Jan  1 00:00:00 UTC 2001
</span></span></span><span style=display:flex><span><span style=color:#888>1: Mon Jan  1 00:00:01 UTC 2001
</span></span></span><span style=display:flex><span><span style=color:#888>2: Mon Jan  1 00:00:02 UTC 2001
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-2
</span></span></code></pre></div><p>Результат будет таким:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>Mon Jan  1 00:00:00 UTC 2001 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>Mon Jan  1 00:00:01 UTC 2001 INFO 1
</span></span></span><span style=display:flex><span><span style=color:#888>Mon Jan  1 00:00:02 UTC 2001 INFO 2
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><p>Агент на уровне узла, установленный в кластере, подхватывает эти потоки логов автоматически без дополнительной настройки. При желании можно настроить агент на парсинг логов в зависимости от контейнера-источника.</p><p>Обратите внимание: несмотря на низкое использование процессора и памяти (порядка нескольких milliCPU для процессора и пары мегабайт памяти), запись логов в файл и их последующая потоковая передача в <code>stdout</code> может вдвое увеличить нагрузку на диск. Если приложение пишет в один файл, рекомендуется установить <code>/dev/stdout</code> в качестве адресата, нежели использовать подход с транслирующим контейнером.</p><p>Sidecar-контейнеры также можно использовать для ротации файлов логов, которые не могут быть ротированы самим приложением. В качестве примера можно привести небольшой контейнер, периодически запускающий <code>logrotate</code>. Однако рекомендуется использовать <code>stdout</code> и <code>stderr</code> напрямую, а управление политиками ротации и хранения оставить kubelet'у.</p><h4 id=sidecar-контейнер-с-лог-агентом>Sidecar-контейнер с лог-агентом</h4><p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Sidecar-контейнер с лог-агентом"></p><p>Если лог-агент на уровне узла недостаточно гибок для ваших потребностей, можно создать sidecar-контейнер с отдельным лог-агентом, специально настроенным на работу с приложением.</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> Работа лог-агента в sidecar-контейнере может привести к значительному потреблению ресурсов. Более того, доступ к этим журналам с помощью <code>kubectl logs</code> будет невозможен, поскольку они не контролируются kubelet'ом.</div><p>Ниже приведены два файла конфигурации sidecar-контейнера с лог-агентом. Первый содержит <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/><code>ConfigMap</code></a> для настройки fluentd.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/ru/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-fluentd-sidecar-config-yaml")' title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"></img></div><div class=includecode id=admin-logging-fluentd-sidecar-config-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format2
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type google_cloud
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> За информацией о настройке fluentd обратитесь к его <a href=https://docs.fluentd.org/>документации</a>.</div><p>Второй файл описывает Pod с sidecar-контейнером, в котором работает fluentd. Pod монтирует том с конфигурацией fluentd.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/ru/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-agent-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>В приведенных выше примерах fluentd можно заменить на другой лог-агент, считывающий данные из любого источника в контейнере приложения.</p><h3 id=прямой-доступ-к-логам-из-приложения>Прямой доступ к логам из приложения</h3><p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Прямой доступ к логам из приложения"></p><p>Сбор логов приложения на уровне кластера, при котором доступ к ним осуществляется напрямую, выходит за рамки Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>3 - Типы прокси-серверов в Kubernetes</h1><p>На этой странице рассказывается о различных типах прокси-серверов, которые используются в Kubernetes.</p><h2 id=прокси-серверы>Прокси-серверы</h2><p>При работе с Kubernetes можно столкнуться со следующими типами прокси-серверов:</p><ol><li><p><a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectl</a>:</p><ul><li>работает на локальной машине или в Pod'е;</li><li>поднимает канал связи от локальной машины к интерфейсу API-сервера Kubernetes;</li><li>данные от клиента к прокси-серверу передаются по HTTP;</li><li>данные от прокси к серверу API передаются по HTTPS;</li><li>отвечает за обнаружение сервера API;</li><li>добавляет заголовки аутентификации.</li></ul></li><li><p><a href=/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services>Прокси-сервер API</a>:</p><ul><li>бастион, встроенный в API-сервер;</li><li>подключает пользователя за пределами кластера к IP-адресам кластера, которые в ином случае могут оказаться недоступными;</li><li>входит в процессы сервера API;</li><li>данные от клиента к прокси-серверу передаются по HTTPS (или по HTTP, если сервер API настроен соответствующим образом);</li><li>данные от прокси-сервера к цели передаются по HTTP или HTTPS в зависимости от настроек прокси;</li><li>используется для доступа к узлам, Pod'ам или сервисам;</li><li>при подключении к сервису выступает балансировщиком нагрузки.</li></ul></li><li><p><a href=/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p><ul><li>работает на каждом узле;</li><li>обрабатывает трафик UDP, TCP и SCTP;</li><li>"не понимает" HTTP;</li><li>выполняет функции балансировщика нагрузки;</li><li>используется только для доступа к сервисам.</li></ul></li><li><p>Прокси-сервер/балансировщик нагрузки перед API-сервером(-ами):</p><ul><li>наличие и тип (например, nginx) определяется конфигурацией кластера;</li><li>располагается между клиентами и одним или несколькими серверами API;</li><li>балансирует запросы при наличии нескольких серверов API.</li></ul></li><li><p>Облачные балансировщики нагрузки на внешних сервисах:</p><ul><li>предоставляются некоторыми облачными провайдерами (например, AWS ELB, Google Cloud Load Balancer);</li><li>создаются автоматически для сервисов Kubernetes с типом <code>LoadBalancer</code>;</li><li>как правило, поддерживают только UDP/TCP;</li><li>наличие поддержки SCTP зависит от реализации балансировщика нагрузки облачного провайдера;</li><li>реализация варьируется в зависимости от поставщика облачных услуг.</li></ul></li></ol><p>Пользователи Kubernetes, как правило, в своей работе сталкиваются только с прокси-серверами первых двух типов. За настройку остальных типов обычно отвечает администратор кластера.</p><h2 id=запросы-на-перенаправления>Запросы на перенаправления</h2><p>На смену функциям перенаправления (редиректам) пришли прокси-серверы. Перенаправления устарели.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-31c9327d2332c585341b64ddafa19cdd>4 - Равноправный доступ к API</h1><div style=margin-top:10px;margin-bottom:10px><b>СТАТУС ФИЧИ:</b> <code>Kubernetes v1.20 [beta]</code></div><p>Контроль за поведением API-сервера Kubernetes в условиях высокой нагрузки — ключевая задача для администраторов кластера. В <a class=glossary-tooltip title='Компонент панели управления, обслуживающий API Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label=kube-apiserver>kube-apiserver</a> имеются некоторые механизмы управления (например, флаги командной строки <code>--max-requests-inflight</code> и <code>--max-mutating-requests-inflight</code>) для ограничения нагрузки на сервер. Они предотвращают наплыв входящих запросов и потенциальный отказ сервера API, но не позволяют гарантировать прохождение наиболее важных запросов в периоды высокой нагрузки.</p><p>Функция регулирования приоритета и обеспечения равноправного доступа к API (API Priority and Fairness), или РДА, — отличная альтернатива флагам. Она оптимизирует вышеупомянутые ограничения на максимальное количество запросов. РДА тщательнее классифицирует запросы и изолирует их. Также она поддерживает механизм очередей, который помогает обрабатывать запросы при краткосрочных всплесках нагрузки. Отправка запросов из очередей осуществляется на основе метода организации равноправных очередей, поэтому плохо работающий <a class=glossary-tooltip title='Управляющий цикл который отслеживает общее состояние кластера через API-сервер и вносит изменения пытаясь приветси текушее состояние к желаемому состоянию.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=контроллер>контроллер</a> не будет мешать работе других (даже с аналогичным уровнем приоритета).</p><p>Эта функция предназначена для корректной работы со стандартными контроллерами, которые используют информеры и реагируют на неудачные API-запросы, экспоненциально увеличивая выдержку (back-off) между ними, а также клиентами, устроенными аналогичным образом.</p><div class="alert alert-warning caution callout" role=alert><strong>Внимание:</strong> Запросы, отнесенные к категории "long-running" — в первую очередь следящие, — не подпадают под действие фильтра функции равноправного доступа к API. Это также верно для флага <code>--max-requests-inflight</code> без включенной функции РДА.</div><h2 id=включение-отключение-равноправного-доступа-к-api>Включение/отключение равноправного доступа к API</h2><p>Управление РДА осуществляется с помощью переключателя функционала (feature gate); по умолчанию функция включена. В разделе <a href=/docs/reference/command-line-tools-reference/feature-gates/>Переключатели функционала</a> приведено их общее описание и способы включения/отключения. В случае РДА соответствующий переключатель называется "APIPriorityAndFairness". Данная функция также включает <a class=glossary-tooltip title='A set of related paths in the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning target=_blank aria-label='группу API'>группу API</a>, при этом: (a) версия <code>v1alpha1</code> по умолчанию отключена, (b) версии <code>v1beta1</code> и <code>v1beta2</code> по умолчанию включены. Чтобы отключить РДА и бета-версии групп API, добавьте следующие флаги командной строки в вызов <code>kube-apiserver</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kube-apiserver <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--feature-gates<span style=color:#666>=</span><span style=color:#b8860b>APIPriorityAndFairness</span><span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--runtime-config<span style=color:#666>=</span>flowcontrol.apiserver.k8s.io/v1beta1<span style=color:#666>=</span>false,flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span> <span style=color:#080;font-style:italic># …и остальные флаги, как обычно</span>
</span></span></code></pre></div><p>Кроме того, версию v1alpha1 группы API можно включить с помощью <code>--runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true</code>.</p><p>Флаг командной строки <code>--enable-priority-and-fairness=false</code> отключит функцию равноправного доступа к API, даже если другие флаги ее активировали.</p><h2 id=основные-понятия>Основные понятия</h2><p>Функция равноправного доступа к API в своей работе использует несколько базовых понятий/механизмов. Входящие запросы классифицируются по атрибутам с помощью т.н. <em>FlowSchemas</em>, после чего им присваиваются уровни приоритета. Уровни приоритета обеспечивают некоторую степень изоляции, обеспечивая различные пределы параллелизма, предотвращая влияние запросов с разными уровнями приоритета друг на друга. В пределах одного приоритета алгоритм равнодоступного формирования очереди предотвращает взаимное влияние запросов из разных <em>потоков</em> и формирует очередь запросов, снижая число неудачных запросов во время всплесков трафика при приемлемо низкой средней нагрузке.</p><h3 id=уровни-приоритета>Уровни приоритета</h3><p>Без включенного равноправного доступа к API управление общим параллелизмом в API-сервере осуществляется флагами <code>--max-requests-inflight</code> и <code>--max-mutating-requests-inflight</code> для <code>kube-apiserver</code>. При включенном равноправном доступе к API пределы параллелизма, заданные этими флагами, суммируются, а затем сумма распределяется по настраиваемому набору <em>уровней приоритета</em>. Каждому входящему запросу присваивается определенный уровень приоритета, причем каждый уровень приоритета может отправлять только такое количество параллельных запросов, которое прописано в его конфигурации.</p><p>Конфигурация по умолчанию, например, предусматривает отдельные уровни приоритета для запросов на выборы лидера, запросов от встроенных контроллеров и запросов от Pod'ов. Это означает, что Pod, ведущий себя некорректно и переполняющий API-сервер запросами, не сможет помешать выборам лидера или оказать влияние на действия встроенных контроллеров.</p><h3 id=очереди>Очереди</h3><p>Каждый уровень приоритета может включать большое количество различных источников трафика. Во время перегрузки важно предотвратить негативное влияние одного потока запросов на остальные (например, в идеале один сбойный клиент, переполняющий kube-apiserver своими запросами, не должен оказывать заметного влияния на других клиентов). Для этого при обработке запросов с одинаковым уровнем приоритета используется алгоритм равнодоступной очереди. Каждый запрос приписывается к <em>потоку</em>, который идентифицируется по имени соответствующей FlowSchema и <em>дифференциатору потока</em>: пользователю-источнику запроса, пространству имен целевого ресурса или пустым значением. Система старается придать примерно равный вес запросам в разных потоках с одинаковым уровнем приоритета.
Для раздельной обработки различных инстансов контроллеры с большим их числом должны аутентифицироваться под разными именами пользователей.</p><p>Распределив запрос в некоторый поток, РДА приписывает его к очереди. Этот процесс базируется на методе, известном как <a class=glossary-tooltip title='A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.' data-toggle=tooltip data-placement=top href='/ru/docs/reference/glossary/?all=true#term-shuffle-sharding' target=_blank aria-label='shuffle sharding'>shuffle sharding</a> (тасование между шардами), который относительно эффективно изолирует потоки низкой интенсивности от потоков высокой интенсивности с помощью очередей.</p><p>Параметры алгоритма постановки в очередь можно настраивать для каждого уровня приоритетов. В результате администратор может выбирать между использованием памяти, равнодоступностью (свойством, которое обеспечивает продвижение независимых потоков, когда совокупный трафик превышает пропускную способность), толерантностью к всплескам трафика и дополнительной задержкой, вызванной постановкой в очередь.</p><h3 id=запросы-исключения>Запросы-исключения</h3><p>Некоторые запросы считаются настолько важными, что на них не распространяется ни одно из ограничений, налагаемых этой функцией. Механизм исключений не позволяет ошибочно настроенной конфигурации управления потоком полностью вывести сервер API из строя.</p><h2 id=ресурсы>Ресурсы</h2><p>API управления потоками включает в себя два вида ресурсов. <a href=/docs/reference/generated/kubernetes-api/v1.26/#prioritylevelconfiguration-v1beta2-flowcontrol-apiserver-k8s-io>PriorityLevelConfigurations</a> определяет доступные классы изоляции, долю доступного бюджета параллелизма, которая выделяется для каждого класса, и позволяет выполнять тонкую настройку работы с очередями. <a href=/docs/reference/generated/kubernetes-api/v1.26/#flowschema-v1beta2-flowcontrol-apiserver-k8s-io>FlowSchema</a> используется для классификации отдельных входящих запросов, сопоставляя каждый из них с одной из конфигураций PriorityLevelConfiguration. Кроме того, существует версия <code>v1alpha1</code> данной группы API, с аналогичными Kinds с теми же синтаксисом и семантикой.</p><h3 id=prioritylevelconfiguration>PriorityLevelConfiguration</h3><p>PriorityLevelConfiguration представляет отдельный класс изоляции. У каждой конфигурации PriorityLevelConfiguration имеется независимый предел на количество активных запросов и ограничения на число запросов в очереди.</p><p>Пределы параллелизма для PriorityLevelConfigurations указываются не в виде абсолютного количества запросов, а в виде "долей параллелизма" (concurrency shares). Совокупный объем ресурсов API-сервера, доступных для параллелизма, распределяется между существующими PriorityLevelConfigurations пропорционально этим долям. Администратор кластера может увеличить или уменьшить совокупный объем трафика на сервер, просто перезапустив kube-apiserver с другим значением <code>--max-requests-inflight</code> (или <code>--max-mutating-requests-inflight</code>). В результате пропускная способность каждой PriorityLevelConfigurations возрастет (или снизится) соразмерно ее доле.</p><div class="alert alert-warning caution callout" role=alert><strong>Внимание:</strong> При включенной функции Priority and Fairness суммарный предел параллелизма для сервера равен сумме <code>--max-requests-inflight</code> и <code>--max-mutating-requests-inflight</code>. При этом мутирующие и не мутирующие запросы рассматриваются вместе; чтобы обрабатывать их независимо для некоторого ресурса, создайте отдельные FlowSchemas для мутирующих и не мутирующих действий (verbs).</div><p>Поле <code>type</code> спецификации PriorityLevelConfiguration определяет судьбу избыточных запросов, когда их объем, отнесенный к одной PriorityLevelConfiguration, превышает ее допустимый уровень параллелизма. Тип <code>Reject</code> означает, что избыточный трафик будет немедленно отклонен с ошибкой HTTP 429 (Too Many Requests). Тип <code>Queue</code> означает, что запросы, превышающие пороговое значение, будут поставлены в очередь, при этом для балансировки прогресса между потоками запросов будут использоваться методы тасования между шардами и равноправных очередей.</p><p>Конфигурация очередей позволяет настроить алгоритм равноправных очередей для каждого уровня приоритета. Подробности об алгоритме можно узнать из <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>предложения по улучшению</a>; если вкратце:</p><ul><li><p>Увеличение <code>queues</code> снижает количество конфликтов между различными потоками за счет повышенного использования памяти. При единице логика равнодоступной очереди отключается, но запросы все равно могут быть поставлены в очередь.</p></li><li><p>Увеличение длины очереди (<code>queueLengthLimit</code>) позволяет выдерживать большие всплески трафика без потери запросов за счет увеличения задержек и повышенного потребления памяти.</p></li><li><p>Изменение <code>handSize</code> позволяет регулировать вероятность конфликтов между различными потоками и общий параллелизм, доступный для одного потока в условиях чрезмерной нагрузки.</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> Больший <code>handSize</code> снижает вероятность конфликта двух отдельных потоков (и, следовательно, вероятность того, что один из них подавит другой), но повышает вероятность того, что малое число потоков загрузят API-сервер. Больший <code>handSize</code> также потенциально увеличивает задержку, которую может вызвать один поток с высоким трафиком. Максимальное возможное количество запросов в очереди от одного потока равно <code>handSize * queueLengthLimit</code>.</div></li></ul><p>Ниже приведена таблица с различными конфигурациями, показывающая вероятность того, что "мышь" (поток низкой интенсивности) будет раздавлена "слонами" (потоками высокой интенсивности) в зависимости от числа "слонов" при тасовании потоков между шардами. Скрипт для расчета таблицы доступен по <a href=https://play.golang.org/p/Gi0PLgVHiUg>ссылке</a>.</p><table><caption style=display:none>Конфигурации shuffle sharding</caption><thead><tr><th><code>handSize</code></th><th>Число очередей</th><th>1 слон</th><th>4 слона</th><th>16 слонов</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id=flowschema>FlowSchema</h3><p>FlowSchema сопоставляется со входящими запросами; по результатам данного действия тем приписывается определенный уровень приоритета. Каждый входящий запрос по очереди проверяется на соответствие каждой FlowSchema, начиная с тех, у которых наименьшее численное значение <code>matchingPrecedence</code> (т.е., логически наивысший приоритет). Проверка ведется до первого совпадения.</p><div class="alert alert-warning caution callout" role=alert><strong>Внимание:</strong> Учитывается только первая подходящая FlowSchema для данного запроса. Если одному входящему запросу соответствует несколько FlowSchemas, он попадет в ту, у которой наивысший <code>matchingPrecedence</code>. Если несколько FlowSchema с одинаковым <code>matchingPrecedence</code> соответствуют одному запросу, предпочтение будет отдано той, у которой лексикографически меньшее имя (<code>name</code>). Впрочем, лучше не полагаться на это, а убедиться, что <code>matchingPrecedence</code> уникален для всех FlowSchema.</div><p>Схема FlowSchema подходит определенному запросу, если хотя бы одно из ее правил (<code>rules</code>) подходит ему. В свою очередь, правило соответствует запросу, если ему соответствует хотя бы один из его субъектов (<code>subjects</code>) <em>и</em> хотя бы одно из его правил <code>resourceRules</code> или <code>nonResourceRules</code> (в зависимости от того, является ли входящий запрос ресурсным или нересурсным URL).</p><p>Для поля <code>name</code> в субъектах (subjects) и полей <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>, <code>namespaces</code> и <code>nonResourceURLs</code> в ресурсных и нересурсных правилах может быть указан универсальный символ <code>*</code>, который будет соответствовать всем значениям для данного поля, фактически исключая его из рассмотрения.</p><p>Параметр <code>distinguisherMethod.type</code> схемы FlowSchema определяет, как запросы, соответствующие этой схеме, будут разделяться на потоки. Он может быть либо <code>ByUser</code> (в этом случае один запрашивающий пользователь не сможет лишить других пользователей ресурсов), либо <code>ByNamespace</code> (в этом случае запросы на ресурсы в одном пространстве имен не смогут помешать запросам на ресурсы в других пространствах имен), либо он может быть пустым (или <code>distinguisherMethod</code> может быть опущен) (в этом случае все запросы, соответствующие данной FlowSchema, будут считаться частью одного потока). Правильный выбор для определенной FlowSchema зависит от ресурса и конкретной среды.</p><h2 id=значения-по-умолчанию>Значения по умолчанию</h2><p>kube-apiserver поддерживает два вида объектов конфигурации РДА: обязательные и рекомендуемые.</p><h3 id=обязательные-объекты-конфигурации>Обязательные объекты конфигурации</h3><p>Четыре обязательных объекта конфигурации отражают защитное поведение, встроенное в серверы. Оно реализуется независимо от этих объектов; параметры последних просто его отражают.</p><ul><li><p>Обязательный уровень приоритета <code>exempt</code> используется для запросов, которые вообще не подчиняются контролю потока: они всегда будут доставляться немедленно. Обязательная FlowSchema <code>exempt</code> относит к этому уровню приоритета все запросы из группы <code>system:masters</code>. При необходимости можно задать другие FlowSchemas, которые будут наделять другие запросы данным уровнем приоритета.</p></li><li><p>Обязательный уровень приоритета <code>catch-all</code> используется в сочетании с обязательной FlowSchema <code>catch-all</code>, гарантируя, что каждый запрос получит какую-либо классификацию. Как правило, полагаться на эту универсальную конфигурацию не следует. Рекомендуется создать свои собственные универсальные FlowSchema и PriorityLevelConfiguration (или использовать опциональный уровень приоритета <code>global-default</code>, доступный по умолчанию). Поскольку предполагается, что обязательный уровень приоритета <code>catch-all</code> будет использоваться редко, его доля параллелизма невысока, кроме того, он не ставит запросы в очередь.</p></li></ul><h3 id=опциональные-объекты-конфигурации>Опциональные объекты конфигурации</h3><p>Опциональные объекты FlowSchemas и PriorityLevelConfigurations образуют оптимальную конфигурацию по умолчанию. При желании их можно изменить и/или создать дополнительные объекты конфигурации. Если велика вероятность высокой нагрузки на кластер, следует решить, какая конфигурация будет работать лучше всего.</p><p>Опциональная конфигурация группирует запросы по шести уровням приоритета:</p><ul><li><p>Уровень приоритета <code>node-high</code> предназначен для проверки здоровья узлов.</p></li><li><p>Уровень приоритета <code>system</code> предназначен для запросов от группы <code>system:nodes</code>, не связанных с состоянием узлов, а именно: от kubelet'ов, которые должны иметь возможность связываться с сервером API для планирования рабочих нагрузок.</p></li><li><p>Уровень приоритета <code>leader-election</code> предназначен для запросов на выборы лидера от встроенных контроллеров (в частности, запросы на объекты типа <code>Endpoint</code>, <code>ConfigMap</code> или <code>Lease</code>, поступающие от пользователей <code>system:kube-controller-manager</code> или <code>system:kube-scheduler</code> и служебных учетных записей в пространстве имен <code>kube-system</code>). Их важно изолировать от другого трафика, поскольку сбои при выборе лидеров приводят к перезагрузкам контроллеров. Соответственно, новые контроллеры потребляют трафик, синхронизируя свои информеры.</p></li><li><p>Уровень приоритета <code>workload-high</code> предназначен для прочих запросов от встроенных контроллеров.</p></li><li><p>Уровень приоритета <code>workload-low</code> предназначен для запросов от остальных учетных записей служб, которые обычно включают все запросы от контроллеров, работающих в Pod'ах.</p></li><li><p>Уровень приоритета <code>global-default</code> обрабатывает весь остальной трафик, например, интерактивные команды <code>kubectl</code>, выполняемые непривилегированными пользователями.</p></li></ul><p>Опциональные FlowSchemas служат для направления запросов на вышеуказанные уровни приоритета и здесь не перечисляются.</p><h3 id=обслуживание-обязательных-и-опциональных-объектов-конфигурации>Обслуживание обязательных и опциональных объектов конфигурации</h3><p>Каждый <code>kube-apiserver</code> самостоятельно обслуживает обязательные и опциональные объекты конфигурации, используя стратегию начальных/периодических проходов. Таким образом, в ситуации с серверами разных версий может возникнуть пробуксовка (thrashing) из-за разного представления серверов о правильном содержании этих объектов.</p><p>Каждый <code>kube-apiserver</code> выполняет начальный проход по обязательным и опциональным объектам конфигурации, а затем периодически (раз в минуту) обходит их.</p><p>Для обязательных объектов обслуживание заключается в проверке того, что объект существует и имеет надлежащую спецификацию (spec). Сервер не разрешает создавать или обновлять объекты со spec, которая не соответствует его защитному поведению.</p><p>Обслуживание опциональных объектов конфигурации предусматривает возможность переопределения их спецификации (spec). Кроме того, удаление носит непостоянный характер: объект будет восстановлен в процессе обслуживания. Если опциональный объект конфигурации не нужен, его не нужно удалять, но достаточно настроить spec'и так, чтобы последствия были минимальными. Обслуживание опциональных объектов также рассчитано на поддержку автоматической миграции при выходе новой версии <code>kube-apiserver</code>, при этом вероятны конфликты (thrashing), пока группировка серверов остается смешанной.</p><p>Обслуживание опционального объекта конфигурации предусматривает его создание — с рекомендуемой спецификацией сервера — если тот не существует. В то же время, если объект уже существует, поведение при обслуживании зависит от того, кто им управляет — <code>kube-apiserver</code>'ы или пользователи. В первом случае сервер гарантирует, что спецификация объекта соответствует рекомендуемой; во втором случае спецификация не анализируется.</p><p>Чтобы узнать, кто управляет объектом, необходимо найти аннотацию с ключом <code>apf.kubernetes.io/autoupdate-spec</code>. Если такая аннотация существует и ее значение равно <code>true</code>, то объект контролируется kube-apiserver'ами. Если аннотация существует и ее значение равно <code>false</code>, объект контролируется пользователями. Если ни одно из этих условий не выполняется, выполняется обращение к <code>metadata.generation</code> объекта. Если этот параметр равен 1, объект контролируется kube-apiserver'ами. В противном случае объект контролируют пользователи. Эти правила были введены в версии 1.22, и использование <code>metadata.generation</code> обусловлено переходом от более простого предыдущего поведения. Пользователи, желающие контролировать опциональный объект конфигурации, должны убедиться, что его аннотация <code>apf.kubernetes.io/autoupdate-spec</code> имеет значение <code>false</code>.</p><p>Обслуживание обязательного или опционального объекта конфигурации также предусматривает проверку наличия у него аннотации <code>apf.kubernetes.io/autoupdate-spec</code>, которая позволяет понять, контролируют ли его kube-apiserver'ы.</p><p>Обслуживание также предусматривает удаление объектов, которые не являются ни обязательными, ни опциональными, но имеют аннотацию <code>apf.kubernetes.io/autoupdate-spec=true</code>.</p><h2 id=освобождение-проверок-работоспособности-от-параллелизма>Освобождение проверок работоспособности от параллелизма</h2><p>Опциональная конфигурация не предусматривает особого отношения к health check-запросам на kube-apiserver'ы от их локальных kubelet'ов. В данном случае обычно используется защищенный порт, но учетные данные не передаются. В опциональной конфигурации такие запросы относятся к FlowSchema <code>global-default</code> и соответствующему уровню приоритета <code>global-default</code>, где другой трафик может мешать их прохождению.</p><p>Чтобы освободить такие запросы от частотных ограничений, можно добавить FlowSchema, приведенную ниже.</p><div class="alert alert-warning caution callout" role=alert><strong>Внимание:</strong> Добавление данной FlowSchema позволит злоумышленникам отправлять удовлетворяющие ей health-check-запросы в любом количестве. При наличии фильтра веб-трафика или аналогичного внешнего механизма безопасности для защиты API-сервера кластера от интернет-трафика можно настроить правила для блокировки любых health-check-запросов, поступающих из-за пределов кластера.</div><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/ru/examples/priority-and-fairness/health-for-strangers.yaml download=priority-and-fairness/health-for-strangers.yaml><code>priority-and-fairness/health-for-strangers.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("priority-and-fairness-health-for-strangers-yaml")' title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard"></img></div><div class=includecode id=priority-and-fairness-health-for-strangers-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>FlowSchema<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>health-for-strangers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchingPrecedence</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityLevelConfiguration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>exempt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceRules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/healthz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/livez&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/readyz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>group</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:unauthenticated<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=диагностика>Диагностика</h2><p>Каждый HTTP-ответ от сервера API с включенной функцией РДА содержит два дополнительных заголовка: <code>X-Kubernetes-PF-FlowSchema-UID</code> и <code>X-Kubernetes-PF-PriorityLevel-UID</code>. В них указываются схема потока и уровень приоритета соответственно. Имена объектов API не включаются в эти заголовки на случай, если запрашивающий пользователь не обладает правами на их просмотр, поэтому при отладке можно использовать команду типа:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get flowschemas -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span><span style=display:flex><span>kubectl get prioritylevelconfigurations -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span></code></pre></div><p>чтобы привязать UID к именам для FlowSchemas и PriorityLevelConfigurations.</p><h2 id=наблюдаемость>Наблюдаемость</h2><h3 id=метрики>Метрики</h3><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> В Kubernetes до версии v1.20 лейблы <code>flow_schema</code> и <code>priority_level</code> также могли называться <code>flowSchema</code> и <code>priorityLevel</code>, соответственно. При использовании Kubernetes v1.19 и более ранних версий обратитесь к документации для соответствующей версии.</div><p>При включении функции равноправного доступа к API kube-apiserver начинает экспортировать дополнительные метрики. Их мониторинг помогает выявить негативное влияние (throttling) текущей конфигурации на важный трафик или найти неэффективные рабочие нагрузки, которые вредят здоровью системы.</p><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> — вектор-счетчик (кумулятивный с момента запуска сервера) запросов, которые были отклонены, с разбивкой по лейблам <code>flow_schema</code> (указывает на FlowSchema у запросов, попавших под соответствие), <code>priority_level</code> (уровень приоритета, который был присвоен этим запросам) и <code>reason</code>. Лейбл <code>reason</code> будет иметь одно из следующих значений:</p><ul><li><code>queue-full</code> — в очереди уже слишком много запросов;</li><li><code>concurrency-limit</code> — PriorityLevelConfiguration настроена на отклонение, а не на постановку в очередь избыточных запросов;</li><li><code>time-out</code> — запрос все еще находился в очереди, когда истек его лимит ожидания.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> — вектор-счетчик (кумулятивный с момента запуска сервера) запросов, которые начали выполняться, сгруппированный по лейблам <code>flow_schema</code> (указывает на FlowSchema у запросов, попавших под соответствие) и <code>priority_level</code> (уровень приоритета, который был присвоен этим запросам).</p></li><li><p><code>apiserver_current_inqueue_requests</code> — вектор предыдущего максимума числа запросов в очереди, сгруппированных по лейблу <code>request_kind</code>, значение которого <code>mutating</code> или <code>readOnly</code>. Эти максимумы описывают наибольшее число, наблюдавшееся в последнем завершенном односекундном окне. Они дополняют более старый вектор <code>apiserver_current_inflight_requests</code>, который показывает максимум активно обслуживаемых запросов в последнем окне.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_samples</code> — вектор-гистограмма наблюдений за тогда-текущим количеством запросов с разбивкой по лейблам <code>phase</code> (принимает значения <code>waiting</code> и <code>executing</code>) и <code>request_kind</code> (принимает значения <code>mutating</code> и <code>readOnly</code>). Наблюдения проводятся периодически с высокой частотой. Каждое наблюдаемое значение представляет собой число в диапазоне от 0 до 1, равное отношению числа запросов к соответствующему ограничению на их количество (ограничение длины очереди в случае ожидания и лимит параллелизма в случае выполнения).</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_watermarks</code> — вектор-гистограмма максимумов или минимумов количества запросов (число запросов, деленное на соответствующее ограничение) с разбивкой по лейблам <code>phase</code> (принимает значения <code>waiting</code> и <code>executing</code>) и <code>request_kind</code> (принимает значения <code>mutating</code> и <code>readOnly</code>); лейбл <code>mark</code> принимает значения <code>high</code> и <code>low</code>. Минимумы и максимумы собираются в окнах, ограниченных временем, когда наблюдение было добавлено в <code>apiserver_flowcontrol_read_vs_write_request_count_samples</code>. Эти экстремумы помогают определить разброс диапазона значений, наблюдавшийся в разных сэмплах.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> — gauge-вектор, содержащий количество стоящих в очереди (не выполняющихся) запросов в каждый момент с разбивкой по лейблам <code>priority_level</code> и <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> — gauge-вектор, содержащий количество исполняемых (не ожидающих в очереди) запросов в каждый момент с разбивкой по лейблам <code>priority_level</code> и <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_request_concurrency_in_use</code> — gauge-вектор, содержащий количество занятых мест в каждый момент с разбивкой по лейблам <code>priority_level</code> и <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_samples</code> — вектор-гистограмма наблюдений за текущим-на-тот-момент количеством запросов с разбивкой по лейблам <code>phase</code> (принимает значения <code>waiting</code> и <code>executing</code>) и <code>priority_level</code>. Каждая гистограмма получает наблюдения, сделанные периодически, вплоть до последней активности соответствующего рода. Наблюдения проводятся с высокой частотой. Каждое наблюдаемое значение представляет собой число в диапазоне от 0 до 1, равное отношению числа запросов к соответствующему ограничению на их количество (ограничение длины очереди в случае ожидания и лимит параллелизма в случае выполнения).</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_watermarks</code> — вектор-гистограмма максимумов или минимумов количества запросов с разбивкой по лейблам <code>phase</code> (принимает значения <code>waiting</code> и <code>executing</code>) и <code>priority_level</code>; лейбл <code>mark</code> принимает значения <code>high</code> и <code>low</code>. Минимумы и максимумы собираются в окнах, ограниченных временем, когда наблюдение было добавлено в <code>apiserver_flowcontrol_priority_level_request_count_samples</code>. Эти экстремумы показывают диапазон значений, наблюдавшийся в разных сэмплах.</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_count_samples</code> — вектор-гистограмма наблюдений за использованием лимита параллелизма для уровня приоритета с разбивкой по <code>priority_level</code>. Использование — отношение (количество занятых мест) / (предел параллелизма). Метрика учитывает все стадии выполнения (как обычную, так и дополнительную задержку в конце записи для покрытия соответствующей работы по уведомлению) всех запросов, кроме WATCHes; для этих запросов учитывается только начальная стадия по доставке уведомлений о ранее существующих объектах. Каждая гистограмма в векторе также помечена лейблом <code>phase: executing</code> (количество мест для фазы ожидания не ограничено). Каждая гистограмма получает наблюдения, сделанные периодически, вплоть до последней активности соответствующего рода. Наблюдения производятся с высокой частотой.</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_count_watermarks</code> — вектор-гистограмма минимумов и максимумов использования предела параллелизма для уровня приоритета с разбивкой по <code>priority_leve</code> и <code>mark</code> (принимает значения <code>high</code> и <code>low</code>). Каждая гистограмма в векторе также помечена лейблом <code>phase: executing</code> (для фазы ожидания предел на места отсутствует). Максимумы и минимумы собираются в окнах, ограниченных временем, когда наблюдение было добавлено в <code>apiserver_flowcontrol_priority_level_seat_count_samples</code>. Эти экстремумы помогают определить разброс диапазона значений, наблюдавшийся в разных сэмплах.</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> — вектор-гистограмма длины очереди для очередей с разбивкой по лейблам <code>priority_level</code> и <code>flow_schema</code> как выборки по поставленным в очередь запросам. Каждый запрос при постановке в очередь вносит один сэмпл в гистограмму, сообщая о длине очереди сразу после добавления запроса. Обратите внимание, что это дает иную статистику, чем при объективном исследовании.</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> В данном случае выброс в гистограмме означает, что, скорее всего, один поток (т.е. запросы от одного пользователя или для одного пространства имен, в зависимости от конфигурации) переполняет сервер API и "срезается" (throttled). И наоборот, если гистограмма одного уровня приоритета показывает, что все очереди для этого уровня приоритета длиннее, чем для других уровней приоритета, возможно, следует увеличить долю параллелизма для этого уровня приоритета в PriorityLevelConfiguration.</div></li><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> — gauge-вектор, содержащий вычисленный лимит параллелизма (основанный на общем лимите параллелизма сервера API и долях параллелизма PriorityLevelConfigurations), с разбивкой по лейблу <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> — вектор-гистограмма времени ожидания запросов в очереди с разбивкой по лейблам <code>flow_schema</code> (указывает, какая схема соответствует запросу), <code>priority_level</code> (указывает, к какому уровню был отнесен запрос) и <code>execute</code> (указывает, начал ли запрос выполняться).</p><div class="alert alert-info note callout" role=alert><strong>Примечание:</strong> Поскольку каждая FlowSchema всегда относит запросы к одному PriorityLevelConfiguration, можно сложить гистограммы для всех FlowSchema для одного уровня приоритета, чтобы получить эффективную гистограмму для запросов, отнесенных к этому уровню приоритета.</div></li><li><p><code>apiserver_flowcontrol_request_execution_seconds</code> — вектор-гистограмма времени, затраченного на выполнение запросов, с разбивкой по по лейблам <code>flow_schema</code> (указывает, какая схема соответствует запросу) и <code>priority_level</code> (указывает, к какому уровню был отнесен запрос).</p></li><li><p><code>apiserver_flowcontrol_watch_count_samples</code> — вектор-гистограмма количества активных запросов WATCH, относящихся к данной записи, с разбивкой по <code>flow_schema</code> и <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_work_estimated_seats</code> — вектор-гистограмма количества предполагаемых мест (максимум начального и конечного этапа выполнения), связанных с запросами, с разбивкой по <code>flow_schema</code> и <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_dispatch_no_accommodation_total</code> — вектор-счетчик количества событий, которые в принципе могли бы привести к отправке запроса, но не привели из-за отсутствия доступного параллелизма, с разбивкой по <code>flow_schema</code> и <code>priority_level</code>. Соответствующими событиями являются поступление запроса и завершение запроса.</p></li></ul><h3 id=отладочные-endpoint-ы>Отладочные endpoint'ы</h3><p>При включении функции равноправного доступа к API <code>kube-apiserver</code> предоставляет следующие дополнительные пути на своих HTTP[S]-портах.</p><ul><li><p><code>/debug/api_priority_and_fairness/dump_priority_levels</code> — список всех уровней приоритета и текущее состояние каждого из них. Получить его можно следующим образом:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels
</span></span></code></pre></div><p>Вывод выглядит примерно так:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests,
workload-low,      0,            true,   false,       0,               0,
global-default,    0,            true,   false,       0,               0,
exempt,            &lt;none&gt;,       &lt;none&gt;, &lt;none&gt;,      &lt;none&gt;,          &lt;none&gt;,
catch-all,         0,            true,   false,       0,               0,
system,            0,            true,   false,       0,               0,
leader-election,   0,            true,   false,       0,               0,
workload-high,     0,            true,   false,       0,               0,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_queues</code> — список всех очередей и их текущее состояние. Получить его можно следующим образом:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_queues
</span></span></code></pre></div><p>Вывод выглядит примерно так:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, Index,  PendingRequests, ExecutingRequests, VirtualStart,
workload-high,     0,      0,               0,                 0.0000,
workload-high,     1,      0,               0,                 0.0000,
workload-high,     2,      0,               0,                 0.0000,
...
leader-election,   14,     0,               0,                 0.0000,
leader-election,   15,     0,               0,                 0.0000,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_requests</code> — список всех запросов, которые в настоящее время ожидают в очереди. Получить его можно следующим образом:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_requests
</span></span></code></pre></div><p>Вывод выглядит примерно так:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,
exempt,            &lt;none&gt;,         &lt;none&gt;,     &lt;none&gt;,              &lt;none&gt;,                &lt;none&gt;,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:26:57.179170694Z,
</code></pre><p>В дополнение к запросам, стоящим в очереди, вывод включает одну фантомную строку для каждого уровня приоритета, на которую не распространяется ограничение.</p><p>Более подробный список можно получить с помощью следующей команды:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw <span style=color:#b44>&#39;/debug/api_priority_and_fairness/dump_requests?includeRequestDetails=1&#39;</span>
</span></span></code></pre></div><p>Вывод выглядит примерно так:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,                     UserName,              Verb,   APIPath,                                                     Namespace, Name,   APIVersion, Resource, SubResource,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:31:03.583823404Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
system,            system-nodes,   12,         1,                   system:node:127.0.0.1, 2020-07-23T15:31:03.594555947Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
</code></pre></li></ul><h2 id=что-дальше>Что дальше</h2><p>Для получения подробной информации о функции равноправного доступа к API см. <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>предложение по улучшению (KEP)</a>. Предложения и запросы функционала принимаются через <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API Machinery</a> или в специализированном <a href=https://kubernetes.slack.com/messages/api-priority-and-fairness>канале Slack</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>5 - Установка дополнений</h1><div class="alert alert-secondary callout third-party-content" role=alert><strong>Примечание:</strong>
Этот раздел ссылается на сторонние проекты, реализующие функциональность, которая требуется Kubernetes. Авторы Kubernetes не несут ответственность за проекты, представленные здесь в алфавитном порядке. Чтобы добавить проект к этому списку, ознакомьтесь с <a href=/docs/contribute/style/content-guide/#third-party-content>руководством по контенту</a> перед публикацией изменений. <a href=#third-party-content-disclaimer>Подробнее</a>.</div><p>Надстройки расширяют функциональность Kubernetes.</p><p>На этой странице перечислены некоторые из доступных надстроек и ссылки на соответствующие инструкции по установке.</p><h2 id=сеть-и-сетевая-политика>Сеть и сетевая политика</h2><ul><li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a> обеспечивает интегрированную сеть контейнеров и сетевую безопасность с помощью Cisco ACI.</li><li><a href=https://antrea.io/>Antrea</a> работает на уровне 3, обеспечивая сетевые службы и службы безопасности для Kubernetes, используя Open vSwitch в качестве уровня сетевых данных.</li><li><a href=https://docs.projectcalico.org/latest/introduction/>Calico</a> Calico поддерживает гибкий набор сетевых опций, поэтому вы можете выбрать наиболее эффективный вариант для вашей ситуации, включая сети без оверлея и оверлейные сети, с или без BGP. Calico использует тот же механизм для обеспечения соблюдения сетевой политики для хостов, модулей и (при использовании Istio и Envoy) приложений на уровне сервисной сети (mesh layer).</li><li><a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel>Canal</a> объединяет Flannel и Calico, обеспечивая сеть и сетевую политику.</li><li><a href=https://github.com/cilium/cilium>Cilium</a> - это плагин сети L3 и сетевой политики, который может прозрачно применять политики HTTP/API/L7. Поддерживаются как режим маршрутизации, так и режим наложения/инкапсуляции, и он может работать поверх других подключаемых модулей CNI.</li><li><a href=https://github.com/cni-genie/CNI-Genie>CNI-Genie</a> позволяет Kubernetes легко подключаться к выбору плагинов CNI, таких как Calico, Canal, Flannel, Romana или Weave.</li><li><a href=https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, основан на <a href=https://tungsten.io>Tungsten Fabric</a>, представляет собой платформу для виртуализации мультиоблачных сетей с открытым исходным кодом и управления политиками. Contrail и Tungsten Fabric интегрированы с системами оркестрации, такими как Kubernetes, OpenShift, OpenStack и Mesos, и обеспечивают режимы изоляции для виртуальных машин, контейнеров/подов и рабочих нагрузок без операционной системы.</li><li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a> - это поставщик оверлейной сети, который можно использовать с Kubernetes.</li><li><a href=https://github.com/ZTE/Knitter/>Knitter</a> - это плагин для поддержки нескольких сетевых интерфейсов Kubernetes подов.</li><li><a href=https://github.com/k8snetworkplumbingwg/multus-cni>Multus</a> - это плагин Multi для работы с несколькими сетями в Kubernetes, который поддерживает большинство самых популярных <a href=https://github.com/containernetworking/cni>CNI</a> (например: Calico, Cilium, Contiv, Flannel), в дополнение к рабочим нагрузкам основанных на SRIOV, DPDK, OVS-DPDK и VPP в Kubernetes.</li><li><a href=https://github.com/ovn-org/ovn-kubernetes/>OVN-Kubernetes</a> - это сетевой провайдер для Kubernetes основанный на <a href=https://github.com/ovn-org/ovn/>OVN (Open Virtual Network)</a>, реализация виртуальной сети, появившийся в результате проекта Open vSwitch (OVS). OVN-Kubernetes обеспечивает сетевую реализацию на основе наложения для Kubernetes, включая реализацию балансировки нагрузки и сетевой политики на основе OVS.</li><li><a href=https://github.com/opnfv/ovn4nfv-k8s-plugin>OVN4NFV-K8S-Plugin</a> - это подключаемый модуль контроллера CNI на основе OVN для обеспечения облачной цепочки сервисных функций (SFC), несколько наложенных сетей OVN, динамического создания подсети, динамического создания виртуальных сетей, сети поставщика VLAN, сети прямого поставщика и подключаемого к другим Multi Сетевые плагины, идеально подходящие для облачных рабочих нагрузок на периферии в сети с несколькими кластерами.</li><li><a href=https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html>NSX-T</a> плагин для контейнера (NCP) обеспечивающий интеграцию между VMware NSX-T и контейнерами оркестраторов, таких как Kubernetes, а так же интеграцию между NSX-T и контейнеров на основе платформы CaaS/PaaS, таких как Pivotal Container Service (PKS) и OpenShift.</li><li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a> - эта платформа SDN, которая обеспечивает сетевое взаимодействие на основе политик между Kubernetes подами и не Kubernetes окружением, с отображением и мониторингом безопасности.</li><li><a href=https://github.com/romana/romana>Romana</a> - это сетевое решение уровня 3 для сетей подов, которое также поддерживает <a href=/docs/concepts/services-networking/network-policies/>NetworkPolicy API</a>. Подробности установки Kubeadm доступны <a href=https://github.com/romana/romana/tree/master/containerize>здесь</a>.</li><li><a href=https://www.weave.works/docs/net/latest/kubernetes/kube-addon/>Weave Net</a> предоставляет сеть и обеспечивает сетевую политику, будет работать на обеих сторонах сетевого раздела и не требует внешней базы данных.</li></ul><h2 id=обнаружение-служб>Обнаружение служб</h2><ul><li><a href=https://coredns.io>CoreDNS</a> - это гибкий, расширяемый DNS-сервер, который может быть <a href=https://github.com/coredns/deployment/tree/master/kubernetes>установлен</a> в качестве внутрикластерного DNS для подов.</li></ul><h2 id=визуализация-и-контроль>Визуализация и контроль</h2><ul><li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a> - это веб-интерфейс панели инструментов для Kubernetes.</li><li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a> - это инструмент для графической визуализации ваших контейнеров, подов, сервисов и т.д. Используйте его вместе с <a href=https://cloud.weave.works/>учетной записью Weave Cloud</a> или разместите пользовательский интерфейс самостоятельно.</li></ul><h2 id=инфраструктура>Инфраструктура</h2><ul><li><a href=https://kubevirt.io/user-guide/#/installation/installation>KubeVirt</a> - это дополнение для запуска виртуальных машин в Kubernetes. Обычно работает на bare-metal кластерах.</li></ul><h2 id=legacy-add-ons>Legacy Add-ons</h2><p>В устаревшем каталоге <a href=https://git.k8s.io/kubernetes/cluster/addons>cluster/addons</a> задокументировано несколько других дополнений.</p><p>Ссылки на те, в хорошем состоянии, должны быть здесь. Пул реквесты приветствуются!</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/ru/docs/home/>Главная</a>
<a class=text-white href=/ru/community/>Сообщество</a>
<a class=text-white href=/ru/case-studies/>Примеры использования</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Авторы Kubernetes | Документация распространяется под лицензией <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. Все права защищены. The Linux Foundation является зарегистрированной торговой маркой. Список торговых марок The Linux Foundation приведён на странице <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>использования торговых марок</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>